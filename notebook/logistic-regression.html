

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>4. 邏輯斯迴歸 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Lab: 數值微分與優化" href="lab-torch-diff-opt.html" />
    <link rel="prev" title="3. Lab: 張量與線性代數" href="lab-torch-tensor.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-tensor.html">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-diff-opt.html">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="maximum-likelihood.html">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-mle.html">
   8. Lab: 最大概似估計
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="latent-variable-modeling.html">
   9. 潛在變項建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="factor-analysis.html">
   10. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   11. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixture-modeling.html">
   12. 混合建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-pyro.html">
   13. Lab:
   <code class="docutils literal notranslate">
    <span class="pre">
     pyro
    </span>
   </code>
   簡介
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/logistic-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/logistic-regression.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   4.1. 二元分類與邏輯斯迴歸
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     4.1.1. 二元線性分類器
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     4.1.2. 邏輯斯迴歸
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   4.2. 最大概似估計法
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     4.2.1. 交叉熵與最大概似估計準則
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     4.2.2. 最大概似估計準則之梯度
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   4.3. 數值優化技術與求解
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     4.3.1. 線搜尋與梯度下降法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bfgs">
     4.3.2. 牛頓法與BFGS法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     4.3.3. 更新步伐之選取
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   4.4. 模型適配度
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   4.5. 實作範例與練習
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     4.5.1. 虛無模型之截距估計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     4.5.2. 練習
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id15">
   4.6. 延伸閱讀
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1><span class="section-number">4. </span>邏輯斯迴歸<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>邏輯斯迴歸（logistic regression）與線性迴歸相似，都是透過一線性函數 <span class="math notranslate nohighlight">\(f(x)\)</span> 以描述兩變項 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(y\)</span> 之間的關係，但不同之處在於邏輯斯迴歸考慮的 <span class="math notranslate nohighlight">\(y\)</span> 為類別變項，其類別數為2，此外，<span class="math notranslate nohighlight">\(f(x)\)</span> 與 <span class="math notranslate nohighlight">\(y\)</span> 之間的關係，需再透過一邏輯斯（logistic）函數進行轉換。邏輯斯迴歸可說是統計領域最基本之二元分類（binary classification）方法，其可視為線性迴歸於分類問題上之拓展。</p>
<p>在此主題中，我們將會學習以下的重點：</p>
<ol class="simple">
<li><p>使用邏輯斯轉換（logistic transformation）來刻畫類別變數之隨機行為。</p></li>
<li><p>使用最大概似法（maximum likelihood method，簡稱ML法），對邏輯斯迴歸模型參數進行估計。</p></li>
<li><p>利用數值優化（numerical optimization）的技術，對函數逐步地進行優化以求解。</p></li>
<li><p>利用適配度指標以評估模型與資料之適配度。</p></li>
</ol>
<div class="section" id="id2">
<h2><span class="section-number">4.1. </span>二元分類與邏輯斯迴歸<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">4.1.1. </span>二元線性分類器<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>廣義來說，二元分類之問題關注的是如何使用一 <span class="math notranslate nohighlight">\(P\)</span> 維之向量 <span class="math notranslate nohighlight">\(x\)</span>，對於二元變項 <span class="math notranslate nohighlight">\(y\)</span> 進行預測，這裡，<span class="math notranslate nohighlight">\(y\)</span>的數值只能為0或1，即<span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span>，<span class="math notranslate nohighlight">\(y=1\)</span> 表示某關注的事件發生，而 <span class="math notranslate nohighlight">\(y=0\)</span>則表示該事件並未發生。在二元分類的問題下，研究者常試圖刻畫在給定 <span class="math notranslate nohighlight">\(x\)</span> 之下，<span class="math notranslate nohighlight">\(y\)</span> 的條件機率（conditional probability），即：</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(y|x)=\frac{\mathbb{P}(y,x)}{\mathbb{P}(x)},\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\mathbb{P}(y,x)\)</span> 表示同時考慮 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(y\)</span> 的聯合機率（joint probability），而 <span class="math notranslate nohighlight">\(\mathbb{P}(x)\)</span> 則為僅考慮 <span class="math notranslate nohighlight">\(x\)</span> 之邊際機率（marginal probability）。在此講義中，我們將簡單的使用 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 來表示在給定 <span class="math notranslate nohighlight">\(x\)</span> 之下，<span class="math notranslate nohighlight">\(y=1\)</span> 之條件機率，即</p>
<div class="math notranslate nohighlight">
\[\pi(x) =\mathbb{P}(y=1|x).\]</div>
<p>由於 <span class="math notranslate nohighlight">\(y=1\)</span> 與 <span class="math notranslate nohighlight">\(y=0\)</span> 為互補之事件（complement events），兩者對應機率之加總需為 1，因此，<span class="math notranslate nohighlight">\(1-\pi(x)\)</span> 可用於表示給定 <span class="math notranslate nohighlight">\(x\)</span> 之下 <span class="math notranslate nohighlight">\(y=0\)</span> 的機率，即</p>
<div class="math notranslate nohighlight">
\[1-\pi(x) =\mathbb{P}(y=0|x).\]</div>
<p>令 <span class="math notranslate nohighlight">\(f(x)=w_0 + \sum_{p=1}^P w_p x_p\)</span>表示一線性函數，<span class="math notranslate nohighlight">\(w_0\)</span>與<span class="math notranslate nohighlight">\(w_p\)</span>分別表示截距（偏誤）與迴歸係數（權重）。若我們將 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(w\)</span> 重新定義為 <span class="math notranslate nohighlight">\(x = (1, x_1,...,x_P)\)</span> 與 <span class="math notranslate nohighlight">\(w = (w_0, w_1,...,w_P)\)</span>，則此線性函數可簡單寫為 <span class="math notranslate nohighlight">\(f(x)=x^Tw\)</span>（見線性迴歸之章節）。</p>
<p>線性分類器（classifier）可視為對二元變項最簡單的分類模型，其基本想法為當 <span class="math notranslate nohighlight">\(f(x)\)</span> 越大時，模型所對應之 <span class="math notranslate nohighlight">\(\mathbb{P}(y=1|x)\)</span> 應越大，反之，當 <span class="math notranslate nohighlight">\(f(x)\)</span> 越小時，<span class="math notranslate nohighlight">\(\mathbb{P}(y=1|x)\)</span> 應越小。然而，由於 <span class="math notranslate nohighlight">\(f(x)\)</span> 的數值未必介於 0 到 1 之間，因此，從建模合理性的角度來看，<span class="math notranslate nohighlight">\(f(x)\)</span> 不宜直接作為 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 此條件機率使用。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">4.1.2. </span>邏輯斯迴歸<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>邏輯斯迴歸試圖使用一邏輯斯轉換（logistic transformation）來刻畫 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 與 <span class="math notranslate nohighlight">\(f(x)=x^Tw\)</span> 之關聯性，其將 <span class="math notranslate nohighlight">\(\pi(x)=\mathbb{P}(y=1|x)\)</span> 刻畫為</p>
<div class="math notranslate nohighlight">
\[\pi(x) = \frac{\exp{ \left( x^Tw \right) }}{1+\exp{ \left( x^Tw \right) }},\]</div>
<p>另一方面，<span class="math notranslate nohighlight">\(1-\pi(x) =\mathbb{P}(y=0|x)\)</span> 即為</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
1-\pi(x) = \frac{1}{1+\exp{ \left( x^Tw \right) }}.
\end{aligned}
\]</div>
<p>透過邏輯斯迴歸模型的結構，我們可以觀察到以下兩件事情：</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi(x)\)</span> 與 <span class="math notranslate nohighlight">\(1 - \pi(x)\)</span> 兩者之數值皆介於0到1之間，符合機率的公理（axiom）。</p></li>
<li><p>當 <span class="math notranslate nohighlight">\(x^Tw\)</span> 數值大時，<span class="math notranslate nohighlight">\(\pi(x)\)</span> 的數值將很靠近1，意味著獲得 <span class="math notranslate nohighlight">\(y=1\)</span> 的可能性很大，反之，<span class="math notranslate nohighlight">\(1 - \pi(x)\)</span> 的數值則較大，獲得 <span class="math notranslate nohighlight">\(y=0\)</span> 的可能性較高。</p></li>
</ol>
<p>在迴歸係數的解讀方面，<span class="math notranslate nohighlight">\(w_p\)</span> 越大，表示 <span class="math notranslate nohighlight">\(x_p\)</span> 對於觀察到 <span class="math notranslate nohighlight">\(y=1\)</span> 此事件有較正向之影響，反之，則對觀察到 <span class="math notranslate nohighlight">\(y=0\)</span> 此事件有較正面之影響。然而，<span class="math notranslate nohighlight">\(w_p\)</span> 對於 <span class="math notranslate nohighlight">\(y\)</span> 之具體效果不容易解讀，一般來說，需透過比較給定 <span class="math notranslate nohighlight">\(x\)</span> 下的對數勝率（log-odds），才能夠進行解讀：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\log \left[ \frac{\pi(x)}{1-\pi(x)} \right]
=&amp; \log \left[ \frac{\frac{\exp{ \left( x^Tw \right) }}{1+\exp{ \left( x^Tw \right) }}}{\frac{1}{1+\exp{ \left( x^Tw \right) }}} \right] \\
=&amp; \log \left[ \exp \left( x^Tw \right) \right] \\
=&amp; x^Tw 
\end{aligned}\end{split}\]</div>
<p>因此，<span class="math notranslate nohighlight">\(w_p\)</span> 可解讀為當 <span class="math notranslate nohighlight">\(x_p\)</span> 每變動一個單位時，預期對數勝率跟著變動的單位。不過，實務上對數勝率之數值大小應如何理解仍不是一簡單的工作。</p>
<p>邏輯斯迴歸的目的在於透過一樣本資料，獲得對迴歸係數 <span class="math notranslate nohighlight">\(w\)</span> 之估計 <span class="math notranslate nohighlight">\(\widehat{w}\)</span>，一方面對於 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(y\)</span> 間的關係進行推論與解釋，二方面則是利用 <span class="math notranslate nohighlight">\(\widehat{y} =\widehat{\pi}(x) = \frac{\exp{ \left( x^T\widehat{w} \right) }}{1+\exp{ \left( x^T \widehat{w} \right) }}\)</span> 此機率預測值對 <span class="math notranslate nohighlight">\(y\)</span> 進行預測。</p>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">4.2. </span>最大概似估計法<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3><span class="section-number">4.2.1. </span>交叉熵與最大概似估計準則<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>在邏輯斯回歸下，我們使用 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 此條件機率值，對於 <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span> 進行預測。為了度量 <span class="math notranslate nohighlight">\(y\)</span> 與 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 之差異，邏輯斯廻歸了使用交叉熵（cross-entropy）此損失函數（loss function）</p>
<div class="math notranslate nohighlight">
\[
L\left [ y, \pi(x) \right] = -  
y \log \pi(x)  - (1-y) \log\left[ 1- \pi(x) \right].
\]</div>
<p>為了瞭解交叉熵的數值如何反應 <span class="math notranslate nohighlight">\(y\)</span> 與 <span class="math notranslate nohighlight">\(\pi(x)\)</span>之間的差異，我們可以參考以下的表格：</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>預測值\實際值</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y = 0\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(y=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\pi(x) \approx 0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(-\log\left[ 1- \pi(x) \right] \approx 0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(- \log \pi(x) \approx \infty\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\pi(x) \approx 1\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(-\log\left[ 1- \pi(x) \right] \approx \infty\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(- \log \pi(x) \approx 0\)</span></p></td>
</tr>
</tbody>
</table>
<p>透過此表格可觀察到：</p>
<ul class="simple">
<li><p>當 <span class="math notranslate nohighlight">\(y = 1\)</span> 時，我們僅需考慮 <span class="math notranslate nohighlight">\(- \log \pi(x)\)</span>
之數值，此時，若 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 相當靠近 1 時，表示模型進行的分類是與資料匹配的，則 <span class="math notranslate nohighlight">\(- \log \pi(x)\)</span> 的數值會相當靠近零，反之，若 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 靠近 0，則 <span class="math notranslate nohighlight">\(- \log \pi(x)\)</span> 會趨近於一相當大的數值。</p></li>
<li><p>當 <span class="math notranslate nohighlight">\(y=0\)</span> 時，我們僅需考慮 <span class="math notranslate nohighlight">\(-\log\left[ 1- \pi(x) \right]\)</span>之數值，當 <span class="math notranslate nohighlight">\(\pi(x)\)</span> 靠近 0 時，交叉熵的數值會靠近 0，反之，則會靠近一很大之數值。</p></li>
</ul>
<p>給定一組隨機樣本 <span class="math notranslate nohighlight">\(\{(y_n, x_n)\}_{n=1}^N\)</span>，邏輯斯迴歸之最大概似（maximum likelihood，簡稱 ML）之適配函數（fitting function），被定義為每筆資料點所對應之交叉熵之平均：</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathcal{D}(w) 
= \frac{1}{N} \sum_{n=1}^N L\left [ y_n, \pi(x_n) \right].
\end{aligned}
\]</div>
<p>透過尋找此 ML 估計準則之極小元 <span class="math notranslate nohighlight">\(\widehat{w}\)</span>，我們可獲得迴歸係數之 ML 估計值 。</p>
</div>
<div class="section" id="id7">
<h3><span class="section-number">4.2.2. </span>最大概似估計準則之梯度<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>為了求得 ML 估計準則之極小元 <span class="math notranslate nohighlight">\(\widehat{w}\)</span>，我們須計算 <span class="math notranslate nohighlight">\(\mathcal{D}(w)\)</span> 之梯度。再開始計算梯度前，我們可以先做一些觀察以簡化眼前的問題。首先，由於 ML 估計準則，是個別交叉熵的平均，則根據微分的線性性質，我們僅需考慮對個別交叉熵微分之結果，之後再將其進行平均即可。再來，對於個別的交叉熵，我們可對其表達式簡化為以下之形式：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L\left[ y, \pi(x) \right]=&amp;-y \log \pi(x)  - (1-y) \log\left[ 1- \pi(x) \right] \\
=&amp; -y \log \left[ \frac{\exp{ \left( x^Tw \right) }}{1+\exp{ \left( x^Tw \right) }} \right]  - (1-y) \log\left[ \frac{1}{1+\exp{ \left( x^Tw \right) }}\right] \\
=&amp; -y x^T w +  \log\left[ 1+\exp{ \left( x^Tw \right) }\right].
\end{aligned}
\end{split}\]</div>
<p>這邊主要是使用到了 <span class="math notranslate nohighlight">\(\log(a/b) = \log(a)-\log(b)\)</span> 此性質。最後，對於個別交叉熵的一階偏微分結果可寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial L\left[ y, \pi(x) \right]}{\partial w_j}
=&amp;  -y \frac{\partial}{\partial w_j}  x^T w +  \frac{\partial}{\partial w_j} \log\left[ 1+\exp{ \left( x^Tw \right) }\right] \\
=&amp;  -y x_j + x_j\frac{\exp{ \left( x^Tw \right) }}{ 1+\exp{ \left( x^Tw \right) }}.
\end{aligned}
\end{split}\]</div>
<p>根據前面三點觀察，我們可得對 ML 估計準則之一階導數</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\frac{\partial \mathcal{D}(w)}{\partial w_j}
=&amp; \frac{1}{N} \sum_{n=1}^N
\left \{
-y_n x_{nj} + x_{nj}\frac{\exp{ \left( x_n^Tw \right) }}{ 1+\exp{ \left( x_n^Tw \right) }}
\right \}.
\end{aligned}
\]</div>
<p>值得注意的是，前述的一階導數可以被寫為</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\frac{\partial \mathcal{D}(w)}{\partial w_j}
= \frac{1}{N} \sum_{n=1}^N
-x_{nj} \left[ y_n - \pi(x_n) \right].
\end{aligned}
\]</div>
<p>前式與線性迴歸 LS 估計準則之一階導數有其在結構上的相似性。</p>
<p>令 <span class="math notranslate nohighlight">\(y\)</span>、<span class="math notranslate nohighlight">\(X\)</span>、以及 <span class="math notranslate nohighlight">\(\pi\)</span> 分別表示以下之向量矩陣：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y =
\underbrace{\begin{pmatrix}
y_1\\
y_2 \\
\vdots \\
y_N
\end{pmatrix}}_{N \times 1},
X =
\underbrace{\begin{pmatrix}
x_1^T\\
x_2^T \\
\vdots \\
x_N^T
\end{pmatrix}}_{N \times (P+1)},
\pi = \pi(X) =
\underbrace{\begin{pmatrix}
\pi(x_1)\\
\pi(x_2) \\
\vdots \\
\pi(x_N)
\end{pmatrix}}_{N \times 1}.
\end{split}\]</div>
<p>前述估計準則之一階導數可以寫為矩陣之形式</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\frac{\partial \mathcal{D}(w)}{\partial w} 
= - X^T y + X^T \pi.
\end{aligned}
\]</div>
<p>而根據一階必要條件，ML 估計式 <span class="math notranslate nohighlight">\(\widehat{w}\)</span> 須符合以下之等式</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
 X^T y = X^T \widehat{\pi}.
\end{aligned}
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\widehat{\pi} = \widehat{\pi}(X) = (\widehat{\pi}(x_1), \widehat{\pi}(x_2),..., \widehat{\pi}(x_N))\)</span> 表示在 <span class="math notranslate nohighlight">\(w = \widehat{w}\)</span> 之下，每筆觀測值對 <span class="math notranslate nohighlight">\(y\)</span> 機率預測所組成之向量。儘管前述等式的結構相當簡單，然而，由於其並非 <span class="math notranslate nohighlight">\(\widehat{w}\)</span> 之線性函數，故我們無法僅使用線性代數的技術來求解，取而代之的是，我們須使用數值優化的技巧。</p>
</div>
</div>
<div class="section" id="id8">
<h2><span class="section-number">4.3. </span>數值優化技術與求解<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id9">
<h3><span class="section-number">4.3.1. </span>線搜尋與梯度下降法<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p><strong>數值優化</strong>（numerical optimization）乃一系列用於計算目標函數 <span class="math notranslate nohighlight">\(\mathcal{D}(w)\)</span> 極小元 <span class="math notranslate nohighlight">\(\widehat{w}\)</span> 之技術。在此章節中，我們主要關注<strong>線搜尋</strong>（line search）這一類之技術。</p>
<p>令 <span class="math notranslate nohighlight">\(\widehat{w}^{(t)}\)</span> 表示在第 <span class="math notranslate nohighlight">\(t\)</span> 步驟下所得之參數估計，則線搜尋試圖使用以下之形式進行更新，以獲得第 <span class="math notranslate nohighlight">\(t+1\)</span> 步驟下之參數估計：</p>
<div class="math notranslate nohighlight">
\[
\widehat{w}^{(t+1)} = \widehat{w}^{(t)} + s \times \underbrace{d}_{(P+1) \times 1},
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(d\)</span> 為一向量，其表示所欲更新的方向，而 <span class="math notranslate nohighlight">\(s\)</span> 則唯一純量，表示更新步伐的大小。不同的線搜尋方法，使用不同的方向 <span class="math notranslate nohighlight">\(d\)</span> 與步伐 <span class="math notranslate nohighlight">\(s\)</span> 進行更新。然而，一般來說，更新方向 <span class="math notranslate nohighlight">\(d\)</span> 多具有以下之形式</p>
<div class="math notranslate nohighlight">
\[
d = - \underbrace{B^{-1}}_{(P+1) \times (P+1)} \nabla D(\widehat{w}^{(t)} ),
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(B\)</span> 表示一對稱且可逆之矩陣。然而，為何線搜尋的方向會與目標函數的梯度取負號有關呢？考慮 <span class="math notranslate nohighlight">\(w=\widehat{w}^{(t)}\)</span> 此位置，沿著一標準化方向 <span class="math notranslate nohighlight">\(d\)</span> （即 <span class="math notranslate nohighlight">\(||d||=1\)</span>）的方向導數為：</p>
<div class="math notranslate nohighlight">
\[
\langle \nabla D(\widehat{w}^{(t)} ), d \rangle = d^T  \nabla D(\widehat{w}^{(t)} ).
\]</div>
<p>我們希望可以找到一方向，其所對應之方向導數斜率為最陡的。根據 Cauchy–Schwarz 不等式，我們知道</p>
<div class="math notranslate nohighlight">
\[
|d^T  \nabla D(\widehat{w}^{(t)} )|  \leq ||d|| ||\nabla D(\widehat{w}^{(t)} )||
\]</div>
<p>而等號成立之條件為 <span class="math notranslate nohighlight">\(d = \pm \tfrac{1}{||\nabla D(\widehat{w}^{(t)} )||} \nabla D(\widehat{w}^{(t)} )\)</span>。因此，當我將 <span class="math notranslate nohighlight">\(d\)</span> 設為梯度取負號時，其所對應的斜率為最陡的。</p>
<p>根據「梯度取負號為最陡之方向」此特性，最簡單的線搜尋方法為<strong>梯度下降法</strong>（gradient descent method），亦稱做<strong>最陡下降法</strong>（steepest descent method），該算則可摘要為：</p>
<div class="note admonition">
<p class="admonition-title"><strong>算則：梯度下降法</strong></p>
<ol class="simple">
<li><p>設定參數之起始值 <span class="math notranslate nohighlight">\(\widehat{w}^{(0)}\)</span> 與一更新步伐 <span class="math notranslate nohighlight">\(s\)</span>。</p></li>
<li><p>當未滿足收斂標準時，計算最陡方向 <span class="math notranslate nohighlight">\(d = -\nabla D(\widehat{w}^{(t)} )\)</span>，並更新參數估計 <span class="math notranslate nohighlight">\(\widehat{w}^{(t + 1)} = \widehat{w}^{(t + 1)} + s \times d\)</span>。</p></li>
</ol>
</div>
<p>一般來說，數值優化常使用的收斂標準包括：</p>
<ul class="simple">
<li><p>直接判定梯度是否小於某很小之數值 <span class="math notranslate nohighlight">\(\epsilon\)</span>，即 <span class="math notranslate nohighlight">\(||\nabla D(\widehat{w}^{(t+1)} )|| &lt; \epsilon\)</span>。</p></li>
<li><p>判定參數估計值之改變是否小於某很小之數值 <span class="math notranslate nohighlight">\(\epsilon\)</span>，即<span class="math notranslate nohighlight">\(||\widehat{w}^{(t+1)} - \widehat{w}^{(t)}|| &lt; \epsilon\)</span>。此標準較為間接，主要用於無法直接評估梯度數值的情境。</p></li>
<li><p>判斷 <span class="math notranslate nohighlight">\(t\)</span> 是否已大於等於可接受之最大迭代次數 <span class="math notranslate nohighlight">\(T\)</span>，即 <span class="math notranslate nohighlight">\(t \geq T\)</span>。不過要注意的是，滿足此標準不代表已找到適切的解，此標準只是用來避免過多的迭代次數。</p></li>
</ul>
</div>
<div class="section" id="bfgs">
<h3><span class="section-number">4.3.2. </span>牛頓法與BFGS法<a class="headerlink" href="#bfgs" title="Permalink to this headline">¶</a></h3>
<p>若進一步考慮 <span class="math notranslate nohighlight">\(d = - B^{-1}\nabla D(\widehat{w}^{(t)} )\)</span> 此一形式，則是否能夠找到一方向，其能夠更有效率地找到目標函數的極小元呢？根據泰勒之定理（Taylor’s theorem），<span class="math notranslate nohighlight">\(\mathcal{D}(w)\)</span> 於 <span class="math notranslate nohighlight">\(w\)</span> 之附近，可以被以下之二次函數逼近</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}(w + d) \approx \mathcal{D}(w) + d^T \nabla \mathcal{D}(w) + \frac{1}{2} d^T \nabla^2 \mathcal{D}(w) d.
\]</div>
<p>令 <span class="math notranslate nohighlight">\(f(d) = \mathcal{D}(w) + d^T \nabla \mathcal{D}(w) + \frac{1}{2} d^T \nabla^2 \mathcal{D}(w) d\)</span>，若我們試圖找到一可以最小化 <span class="math notranslate nohighlight">\(f(d)\)</span> 之 <span class="math notranslate nohighlight">\(d\)</span>，則該 <span class="math notranslate nohighlight">\(d\)</span> 需滿足的條件</p>
<div class="math notranslate nohighlight">
\[
\nabla f(d) =  \nabla \mathcal{D}(w) + \nabla^2 \mathcal{D}(w) d = 0.
\]</div>
<p>因此，更新方向應為</p>
<div class="math notranslate nohighlight">
\[
d = -  \nabla^2 \mathcal{D}(w)^{-1} \nabla \mathcal{D}(w).
\]</div>
<p>意即，將 <span class="math notranslate nohighlight">\(B\)</span> 設為估計準則之黑塞矩陣 <span class="math notranslate nohighlight">\(\nabla^2 \mathcal{D}(w)\)</span>。</p>
<p>進一步考慮到黑塞矩陣之訊息，我們可以得到一較為進階的線搜尋法，其稱作<strong>牛頓法</strong>（Newton’s method），該算則可摘要為：</p>
<div class="note admonition">
<p class="admonition-title"><strong>算則：牛頓法</strong></p>
<ol class="simple">
<li><p>設定參數之起始值 <span class="math notranslate nohighlight">\(\widehat{w}^{(0)}\)</span> 與一更新步伐 <span class="math notranslate nohighlight">\(s\)</span>。</p></li>
<li><p>當未滿足收斂標準時，計算 <span class="math notranslate nohighlight">\(d = - \nabla^2 \mathcal{D}(\widehat{w}^{(t)}) \nabla D(\widehat{w}^{(t)} )\)</span>，並更新參數估計 <span class="math notranslate nohighlight">\(\widehat{w}^{(t + 1)} = \widehat{w}^{(t)} + s \times d\)</span>。</p></li>
</ol>
</div>
<p>牛頓法比梯度下降法有較佳的收斂性，意即，可使用較少之迭代次數就找到極小元。然而，其缺點在於黑塞矩陣的計算，並非對所有的估計準則來說都是容易的，此外，當參數個數較多時，計算黑塞矩陣的成本提高，甚至在大型模型下有可能會引發記憶體不足的問題。</p>
<p>為了解決前述牛頓法之缺點，在實務上，大多會採用所謂的<strong>準牛頓法</strong>（quasi-Newton’s method），其使用某種方式獲得對於黑色矩陣 <span class="math notranslate nohighlight">\(\nabla^2 \mathcal{D}(\widehat{w}^{(t)})\)</span> 之逼近。其中，最為有名的是<strong>BFGS法</strong>，此名稱乃根據其提出者Broyden、Fletcher、Goldfarb、Shanno命名。</p>
<p>令 <span class="math notranslate nohighlight">\(u^{(t)} = \widehat{w}^{(t)} - \widehat{w}^{(t-1)}\)</span> 與 <span class="math notranslate nohighlight">\(v^{(t)}=\nabla D(\widehat{w}^{(t)} ) - \nabla D(\widehat{w}^{(t-1)} )\)</span>，根據泰勒之定理，我們有</p>
<div class="math notranslate nohighlight">
\[
v^{(t)} \approx \nabla^2 \mathcal{D}(\widehat{w}^{(t)}) u^{(t)}.
\]</div>
<p>前述關係式亦描述了以下之關係</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 \mathcal{D}(\widehat{w}^{(t)}) ^{-1} v^{(t)} \approx  u^{(t)}.
\]</div>
<p>BFGS 法利用此式之特性，試圖獲得對於黑塞矩陣反矩陣之逼近。令 <span class="math notranslate nohighlight">\(H^{(t-1)}\)</span> 表示在 <span class="math notranslate nohighlight">\(t-1\)</span> 步驟時對於黑塞矩陣反矩陣之逼近，BFGS 法考慮的問題為：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;\text{minimize}_{H} ||H - H^{(t-1)}|| \\
&amp; \text{subject to } H = H^T, Bv^{(t)} = u^{(t)} .
\end{aligned}
\end{split}\]</div>
<p>透過解此優化問題，可得到 <span class="math notranslate nohighlight">\(H^{(t)}\)</span> 的更新公式為：</p>
<div class="math notranslate nohighlight">
\[
H^{(t)} = (I - \rho^{(t)} u^{(t)} {v^{(t)}}^T )H^{(t-1)} (I - \rho^{(t)} v^{(t)} {u^{(t)}}^T ) + \rho^{(t)} u^{(t)} {u^{(t)}}^T ,
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\rho^{(t)} = \frac{1}{ {v^{(t)}}^T u^{(t)}}\)</span>。在此，我們可觀察到在BFGS法之下，我們直接利用了一簡單的公式，獲得了對於黑塞矩陣的反矩陣，並且該簡單的公式僅利用到了參數估計與梯度在迭代過程中的差值。BFGS 可以摘要為</p>
<div class="note admonition">
<p class="admonition-title"><strong>算則：BFGS法</strong></p>
<ol class="simple">
<li><p>設定參數之起始值 <span class="math notranslate nohighlight">\(\widehat{w}^{(0)}\)</span> 、一更新步伐 <span class="math notranslate nohighlight">\(s\)</span>、以及<span class="math notranslate nohighlight">\(H^{(0)}\)</span>（實務上多採用<span class="math notranslate nohighlight">\(H^{(0)}=I\)</span>）。</p></li>
<li><p>當未滿足收斂標準時，計算 <span class="math notranslate nohighlight">\(d = - H^{(t)} \nabla D(\widehat{w}^{(t)} )\)</span>，並更新參數估計 <span class="math notranslate nohighlight">\(\widehat{w}^{(t + 1)} = \widehat{w}^{(t)} + s \times d\)</span>。</p></li>
</ol>
</div>
</div>
<div class="section" id="id10">
<h3><span class="section-number">4.3.3. </span>更新步伐之選取<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>除了更新方向 <span class="math notranslate nohighlight">\(d\)</span>，更新步伐 <span class="math notranslate nohighlight">\(s\)</span> 的挑選，亦在線搜尋方法中扮演重要的角色。當 <span class="math notranslate nohighlight">\(s\)</span> 過大時，可能會無法找到局部極小元，而當 <span class="math notranslate nohighlight">\(s\)</span> 過小時，則可能會導致收斂過慢的問題。因此，要如何找到一適切的更新步伐大小，不是一件容易的工作。</p>
<p>在實務上，<span class="math notranslate nohighlight">\(s\)</span> 常在優化過程中，以動態的方式進行調整。理想上，在給定更新方向 <span class="math notranslate nohighlight">\(d\)</span> 之後，我們可透過解決以下的問題，獲得一最佳的更新步伐：</p>
<div class="math notranslate nohighlight">
\[
\text{minimize}_{s} \mathcal{D}(\widehat{w} + s \times d).
\]</div>
<p>然而，這種找最適更新步伐的方法，等同於要去解一個優化問題，除非目標函數的結構很單純，否則，前述的作法是相當不經濟的。因此，在實務上，更新步伐的調整多採用以下兩種策略：</p>
<ul class="simple">
<li><p>根據某定好之規則，令<span class="math notranslate nohighlight">\(s\)</span> 之數值隨迭代次數 <span class="math notranslate nohighlight">\(t\)</span> 遞減。舉例來說，我們可增加一滿足 <span class="math notranslate nohighlight">\(0&lt;\gamma&lt;1\)</span> 之參數 <span class="math notranslate nohighlight">\(\gamma\)</span>，接著，每當 <span class="math notranslate nohighlight">\(t\)</span> 滿足某些條件時（如 <span class="math notranslate nohighlight">\(t\)</span> 為10的倍數時），重新定義新的更新步伐為 <span class="math notranslate nohighlight">\(s \leftarrow \gamma s\)</span>。</p></li>
<li><p>利用<strong>回朔線搜尋</strong>（backtracking line search）尋找 <span class="math notranslate nohighlight">\(s\)</span>。此方法之執行如下：</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title"><strong>算則：回朔線搜尋</strong></p>
<ol class="simple">
<li><p>給定 <span class="math notranslate nohighlight">\(s_0&gt;0\)</span>、<span class="math notranslate nohighlight">\(\gamma \in (0, 1)\)</span>、以及 <span class="math notranslate nohighlight">\(c \in (0, 1)\)</span>。</p></li>
<li><p>在每次迭代 <span class="math notranslate nohighlight">\(t\)</span> 開始時，將 <span class="math notranslate nohighlight">\(s\)</span> 設為 <span class="math notranslate nohighlight">\(s_0\)</span>，接著，當 <span class="math notranslate nohighlight">\(\mathcal{D}(\widehat{w}^{(t)} + s \times d) &lt; \mathcal{D}(\widehat{w}^{(t)}) + s \times c \times d^T \nabla \mathcal{D}(\widehat{w}^{(t)})\)</span> 此條件未被滿足時，根據 <span class="math notranslate nohighlight">\(s \leftarrow \gamma \times s\)</span> 此公式持續調整 <span class="math notranslate nohighlight">\(s\)</span> 之數值。</p></li>
</ol>
</div>
</div>
</div>
<div class="section" id="id11">
<h2><span class="section-number">4.4. </span>模型適配度<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>邏輯斯迴歸利用 <span class="math notranslate nohighlight">\(\widehat{\pi}(x) = \frac{\exp{ \left( x^T\widehat{w} \right) }}{1+\exp{ \left( x^T \widehat{w} \right) }}\)</span> 對 <span class="math notranslate nohighlight">\(y\)</span> 進行預測，然而，<span class="math notranslate nohighlight">\(\widehat{\pi}(x)\)</span> 究竟是否能夠良好地刻畫 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(y\)</span> 間的關係，則需透過適配度指標來評估。</p>
<p>首先，ML 適配函數 <span class="math notranslate nohighlight">\(\mathcal{D}(\widehat{w})\)</span> 即可用於評估模型之表現，但其數值大小不易解讀，故實務上難以使用。</p>
<p>有鑒於 <span class="math notranslate nohighlight">\(R^2\)</span> 於線性迴歸問題上的受歡迎，在邏輯斯迴歸的脈絡下，亦有不少所謂的虛擬 <span class="math notranslate nohighlight">\(R^2\)</span> （pseudo <span class="math notranslate nohighlight">\(R^2\)</span>）被提出。如 McFadden（1973）將虛擬 <span class="math notranslate nohighlight">\(R^2\)</span> 定義為</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
R_{McFadden}^2 &amp;= \frac{\mathcal{D}(\widetilde{w}) - \mathcal{D}(\widehat{w})}{\mathcal{D}(\widetilde{w})},
\end{aligned}
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\widetilde{w}\)</span> 指的是在虛無模型下之估計量，即限制所有變項迴歸係數皆為 0，僅對截距項進行估計。另外，Efron（1978）則是將虛擬 <span class="math notranslate nohighlight">\(R^2\)</span> 定義為</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
R_{Efron}^2 &amp;=  \frac{\sum_{n=1}^N(y_n - m_Y)^2 - \sum_{n=1}^N(y_n - \widehat{\pi}(x_n))^2}{\sum_{n=1}^N(y_n - m_Y)^2}.
\end{aligned}
\]</div>
<p>從 <span class="math notranslate nohighlight">\(R_{McFadden}^2\)</span> 與 <span class="math notranslate nohighlight">\(R_{Efron}^2\)</span> 兩者之公式來看，前者乃透過 ML 估計準則之數值來定義 <span class="math notranslate nohighlight">\(R^2\)</span>，後者則是根據 LS 估計準則搭配機率預測值 <span class="math notranslate nohighlight">\(\widehat{\pi}(x)\)</span> 來定義。但不管採用何種定義，兩者皆透過與一虛無模型比較建立，數值越靠近 0 表示模型與虛無模型表現類似，而數值越靠近 1 則表示模型與資料之適配程度越好。</p>
<p>除了前述的指標外，若我們將 <span class="math notranslate nohighlight">\(\widehat{\pi}(x)\)</span> 此機率預測值改為類別預測，則可使用分類正確率來評估模型適配度。常使用的類別預測為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\widehat{y}^{c}=
\begin{cases}
1 &amp;\text{if } \widehat{\pi}(x) \geq 0.5, \\
0 &amp;\text{if } \widehat{\pi}(x) &lt; 0.5,
\end{cases}
\end{split}\]</div>
<p>意即，當 <span class="math notranslate nohighlight">\(\widehat{\pi}(x) \geq 0.5\)</span> 時，我們傾向認為其所對應之 <span class="math notranslate nohighlight">\(y\)</span> 應為 1，反之，則認為其所對應之 <span class="math notranslate nohighlight">\(y\)</span> 應為0。因此，我們可以將分類正確率定義為樣本資料中，<span class="math notranslate nohighlight">\(y_n\)</span> 與 <span class="math notranslate nohighlight">\(\widehat{y}_n^c\)</span> 相等之比例，即</p>
<div class="math notranslate nohighlight">
\[
Accuracy = \frac{1}{N} \sum_{n=1}^N 1\{y_n = \widehat{y}_n^c \},
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(1\{\cdot \}\)</span> 表示一指示函數（indicator function），其用於「指示」大括號內之事件是否為真。當 <span class="math notranslate nohighlight">\(E\)</span> 事件為真時，<span class="math notranslate nohighlight">\(1\{E \} = 1\)</span>，反之，<span class="math notranslate nohighlight">\(1\{E \} = 0\)</span>。</p>
</div>
<div class="section" id="id12">
<h2><span class="section-number">4.5. </span>實作範例與練習<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id13">
<h3><span class="section-number">4.5.1. </span>虛無模型之截距估計<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>我們在此展示如何使用梯度下降法，對虛無模型下之截距項進行估計。由於此模型中並未加入任何的共變量，因此，其 ML 適配函數與一階導數為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D}(w_0) &amp; = \frac{1}{N} \sum_{n=1}^N \left \{ -y_n w_0 +  \log\left[ 1+\exp{ \left( w_0 \right) }\right] \right\} \\
&amp; =  - w_0 m_Y +  \log\left[ 1+\exp{ \left( w_0 \right) }\right],
\end{aligned}
\end{split}\]</div>
<p>與</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{D}(w_0)}{\partial w_0} &amp;=
\frac{1}{N} \sum_{n=1}^N
\left \{
-y_n + \frac{\exp{ \left( w_0 \right) }}{ 1+\exp{ \left( w_0 \right) }}
\right \}\\
&amp; = -m_Y+ \frac{\exp{ \left( w_0 \right) }}{ 1+\exp{ \left( w_0 \right) }},
\end{aligned}
\end{split}\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(m_Y\)</span> 表示 <span class="math notranslate nohighlight">\(Y\)</span> 此變項之樣本平均數，在二元變項下，<span class="math notranslate nohighlight">\(m_Y\)</span> 等同於樣本資料中等於 1 的比例。倘若手邊的樣本資料中，<span class="math notranslate nohighlight">\(y_n=1\)</span> 的比例為80%，則 <span class="math notranslate nohighlight">\(m_Y=.8\)</span>，我們可以使用 <code class="docutils literal notranslate"><span class="pre">python</span></code> 撰寫函數，來計算在不同的 <span class="math notranslate nohighlight">\(w_0\)</span>之下，所對應到的 ML 適配函數與其一階導數之數值：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="k">def</span> <span class="nf">cal_loss</span><span class="p">(</span><span class="n">bias</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">bias</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">cal_grad</span><span class="p">(</span><span class="n">bias</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.8</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p>接下來，我們撰寫一執行梯度下降法之函數，以尋找一可最小化 ML 適配函數之 <span class="math notranslate nohighlight">\(\widehat{w}_0\)</span>。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">iter_max</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iter_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="o">-</span> <span class="n">cal_grad</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">bias</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">direction</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cal_loss</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter </span><span class="si">{:2.0f}</span><span class="s2">, loss = </span><span class="si">{:2.3f}</span><span class="s2">, grad = </span><span class="si">{:2.3f}</span><span class="s2">, bias = </span><span class="si">{:2.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="o">-</span><span class="n">direction</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p>我們將參數起始值 <span class="math notranslate nohighlight">\(\widehat{w}_0^{(0)}\)</span>設為0，步伐大小 <span class="math notranslate nohighlight">\(s\)</span> 設為 2，觀察迭代 10 次後，<span class="math notranslate nohighlight">\(\widehat{w}_0^{(10)}\)</span> 之數值為何</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">iter_max</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">gradient_descent</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">iter_max</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>iter  1, loss = 0.557, grad = -0.300, bias = 0.600
iter  2, loss = 0.520, grad = -0.154, bias = 0.909
iter  3, loss = 0.508, grad = -0.087, bias = 1.083
iter  4, loss = 0.504, grad = -0.053, bias = 1.189
iter  5, loss = 0.502, grad = -0.033, bias = 1.256
iter  6, loss = 0.501, grad = -0.022, bias = 1.299
iter  7, loss = 0.501, grad = -0.014, bias = 1.328
iter  8, loss = 0.501, grad = -0.010, bias = 1.347
iter  9, loss = 0.500, grad = -0.006, bias = 1.360
iter 10, loss = 0.500, grad = -0.004, bias = 1.368
</pre></div>
</div>
</div>
</div>
<p>我們可以看到梯度下降法，逐步地更新截距項，使得 ML 適配函數數值下降，且一階導數越來越靠近 0。事實上，此優化問題的極小元存在閉合形式解（closed-form solution），根據一階最適條件，<span class="math notranslate nohighlight">\(\frac{\exp{ \left( \widehat{w}_0 \right) }}{ 1+\exp{ \left( \widehat{w}_0 \right) }} = m_Y\)</span>，因此，<span class="math notranslate nohighlight">\(\widehat{w}_0 = \log \left[\frac{m_Y}{1-m_Y} \right] = \log \left[\frac{.8}{1-.8} \right] \approx  1.386\)</span>，我們可以看到梯度下降找到的解，與此閉合形式解幾無差異。</p>
<p>接下來，請仔細觀察梯度下降在不同更新步伐大小與起始值下之行為表現：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">bais = </span><span class="si">{:2.3f}</span><span class="s2">, step size = </span><span class="si">{:2.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.</span><span class="p">))</span>
<span class="n">gradient_descent</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">bais = </span><span class="si">{:2.3f}</span><span class="s2">, step size = </span><span class="si">{:2.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">))</span>
<span class="n">gradient_descent</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">bais = </span><span class="si">{:2.3f}</span><span class="s2">, step size = </span><span class="si">{:2.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
<span class="n">gradient_descent</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>
bais = 0.000, step size = 20.00
iter  1, loss = 1.202, grad = -0.300, bias = 6.000
iter  2, loss = 0.531, grad = 0.198, bias = 2.049
iter  3, loss = 0.607, grad = 0.086, bias = 0.332
iter  4, loss = 0.947, grad = -0.218, bias = 4.689
iter  5, loss = 0.524, grad = 0.191, bias = 0.871
iter  6, loss = 0.615, grad = -0.095, bias = 2.772
iter  7, loss = 0.709, grad = 0.141, bias = -0.051
iter  8, loss = 1.243, grad = -0.313, bias = 6.204
iter  9, loss = 0.550, grad = 0.198, bias = 2.244
iter 10, loss = 0.648, grad = 0.104, bias = 0.161

bais = 0.000, step size = 0.20
iter  1, loss = 0.676, grad = -0.300, bias = 0.060
iter  2, loss = 0.660, grad = -0.285, bias = 0.117
iter  3, loss = 0.645, grad = -0.271, bias = 0.171
iter  4, loss = 0.633, grad = -0.257, bias = 0.223
iter  5, loss = 0.621, grad = -0.245, bias = 0.272
iter  6, loss = 0.610, grad = -0.233, bias = 0.318
iter  7, loss = 0.601, grad = -0.221, bias = 0.362
iter  8, loss = 0.592, grad = -0.210, bias = 0.404
iter  9, loss = 0.584, grad = -0.200, bias = 0.444
iter 10, loss = 0.577, grad = -0.191, bias = 0.483

bais = 1.500, step size = 2.00
iter  1, loss = 0.501, grad = 0.018, bias = 1.465
iter  2, loss = 0.501, grad = 0.012, bias = 1.440
iter  3, loss = 0.501, grad = 0.009, bias = 1.423
iter  4, loss = 0.500, grad = 0.006, bias = 1.412
iter  5, loss = 0.500, grad = 0.004, bias = 1.404
iter  6, loss = 0.500, grad = 0.003, bias = 1.398
iter  7, loss = 0.500, grad = 0.002, bias = 1.394
iter  8, loss = 0.500, grad = 0.001, bias = 1.392
iter  9, loss = 0.500, grad = 0.001, bias = 1.390
iter 10, loss = 0.500, grad = 0.001, bias = 1.389
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id14">
<h3><span class="section-number">4.5.2. </span>練習<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>請利用以下之程式碼，產生邏輯斯迴歸之資料，並撰寫一函數執行梯度下降法，計算在此資料下之模型參數估計，並觀察其在不同起始值與更新步伐下之行為表現。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set seed</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">246437</span><span class="p">)</span>

<span class="c1"># write a function to generate data</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Bernoulli</span>
<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_sample</span><span class="p">,</span>
                  <span class="n">weight</span><span class="p">,</span>
                  <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="n">mean_feature</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="n">std_feature</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">n_feature</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean_feature</span><span class="p">,</span>
                     <span class="n">std</span> <span class="o">=</span> <span class="n">std_feature</span><span class="p">,</span>
                     <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">n_feature</span><span class="p">),</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">logit</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">+</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weight</span>
    <span class="n">bernoulli</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">logits</span> <span class="o">=</span> <span class="n">logit</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># run generate_data</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="n">bias</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                     <span class="n">mean_feature</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="n">std_feature</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id15">
<h2><span class="section-number">4.6. </span>延伸閱讀<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Agresti, A. (2012). <em>Categorical data analysis (3rd edition)</em>. New York: Wiley.</p></li>
<li><p>Efron, B. (1978). Regression and ANOVA with zero-one data: Measures of residual variation. <em>Journal of the American Statistical Association, 73</em>(361), 113–121.</p></li>
<li><p>McFadden, D. (1973). Conditional logit analysis of qualitative choice behavior. In <em>Frontiers in Econometrics</em> (Edited by P. Zarembka), 105-42. Academic Press, New York.</p></li>
<li><p>Nocedal, J., &amp; Wright, S. J. (1999). <em>Numerical optimization</em>. New York: Springer.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="lab-torch-tensor.html" title="previous page"><span class="section-number">3. </span>Lab: 張量與線性代數</a>
    <a class='right-next' id="next-link" href="lab-torch-diff-opt.html" title="next page"><span class="section-number">5. </span>Lab: 數值微分與優化</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>