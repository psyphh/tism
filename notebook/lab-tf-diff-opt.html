

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>5. Lab: 數值微分與優化 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. 機率分佈" href="probability-distribution.html" />
    <link rel="prev" title="4. 邏輯斯迴歸" href="logistic-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-tf-tensor.html">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic-regression.html">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="maximum-likelihood.html">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-tf-mle.html">
   8. Lab III: 最大概似估計法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="latent-variable-modeling.html">
   9. 潛在變項建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="factor-analysis.html">
   10. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   11. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-tf-example.html">
   12. Lab:
   <code class="docutils literal notranslate">
    <span class="pre">
     tensoflow
    </span>
   </code>
   範例
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/lab-tf-diff-opt.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/lab-tf-diff-opt.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   5.1. 數值微分
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     5.1.1. 變量
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     5.1.2. 數值微分之基礎
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     5.1.3. 二階微分矩陣
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   5.2. 數值優化
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     5.2.1. 手動撰寫優化算則
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-optimizers-optimizer">
     5.2.2. 使用
     <code class="docutils literal notranslate">
      <span class="pre">
       tf.optimizers.Optimizer
      </span>
     </code>
     進行優化
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="lab">
<h1><span class="section-number">5. </span>Lab: 數值微分與優化<a class="headerlink" href="#lab" title="Permalink to this headline">¶</a></h1>
<p>在此 lab 中，我們將介紹</p>
<ol class="simple">
<li><p>如何使用 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 進行數值微分。</p></li>
<li><p>如何使用 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 進行數值優化。</p></li>
<li><p>利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">5.1. </span>數值微分<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first-order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。</p>
<div class="section" id="id2">
<h3><span class="section-number">5.1.1. </span>變量<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>在 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 中，我們所欲進行微分之變量（variable）乃透過 <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> 此類型來表徵，其可透過 <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> 此建構式來建立：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=float64, numpy=array([1., 1., 1.])&gt;
</pre></div>
</div>
</div>
</div>
<p>這裏，我們建立了一尺寸為 <span class="math notranslate nohighlight">\((3,)\)</span> 的變量。乍看之下，變量與張量非常類似，兩者皆牽涉到資料、形狀、以及類型等面向，事實上，變量背後的資料結構的確是一張量，當我們進行運算時，使用的都是該張量資料，如</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise addition:&quot;</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise multiplication:&quot;</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>element-wise addition: tf.Tensor([2. 2. 2.], shape=(3,), dtype=float64)
element-wise multiplication: tf.Tensor([1. 1. 1.], shape=(3,), dtype=float64)
</pre></div>
</div>
</div>
</div>
<p>然而，變量容許我們在程式執行的過程中，不斷地對其狀態進行更新。比如說，我們可以使用 <code class="docutils literal notranslate"><span class="pre">.assign</span></code> 此方法對一變量的張量資料進行更新，其會重新使用儲存該張量的記憶體：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">assign</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=float64, numpy=array([1., 2., 3.])&gt;
</pre></div>
</div>
</div>
</div>
<p>在 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 中，變量最重要的功能就是用來表徵模型的參數，其可透過不同的優化策略來更新其數值，因此，變量通常是可以被訓練的（trainable），我們可透過其 <code class="docutils literal notranslate"><span class="pre">.trainbale</span></code> 屬性來了解</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">trainable</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3><span class="section-number">5.1.2. </span>數值微分之基礎<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>在 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 中，自動微分乃透過 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> 來進行。首先，我們利用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape()</span></code> 所建立的環境，紀錄該變量的計算過程</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor y: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor z: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor y: 
 tf.Tensor([-2.  0.  2.], shape=(3,), dtype=float64)
tensor z: 
 tf.Tensor(8.0, shape=(), dtype=float64)
</pre></div>
</div>
</div>
</div>
<p>在此例子中，整個計算過程可寫為 <span class="math notranslate nohighlight">\(y_i = 2(x_{i} - 2)\)</span>，<span class="math notranslate nohighlight">\(z = \sum_{i=1}^3 y_i^2\)</span>。針對已紀錄之運算過程，想要獲得與該運算有關的梯度時，可以使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape.gradient</span></code>此函數：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)
</pre></div>
</div>
</div>
</div>
<p>這裡， <code class="docutils literal notranslate"><span class="pre">grad_x</span></code> 即為 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial x}\)</span> 之計算結果，其張量尺寸與 <code class="docutils literal notranslate"><span class="pre">x</span></code> 相同，即 <code class="docutils literal notranslate"><span class="pre">(3,1)</span></code>。</p>
<p>需要特別注意的是，在執行完一次 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape.gradient</span></code> 後，<code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 即會將該記錄器所使用的資源給釋放出來，故無法再次使用 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape.gradient</span></code> 此指令。若想要多次執行 <code class="docutils literal notranslate"><span class="pre">tf.GradientTape.gradient</span></code>，可以在建立 <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> 時，使用 <code class="docutils literal notranslate"><span class="pre">persistent=True</span></code> 此指令：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>透過該指令，我們就能夠計算多個梯度的結果，如計算 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial x}\)</span> 與 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial y}\)</span> 兩者。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">grad_y</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dy: &quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)
dz/dy:  tf.Tensor([-4.  0.  4.], shape=(3,), dtype=float64)
</pre></div>
</div>
</div>
</div>
<p>我們也可以在單一的 <code class="docutils literal notranslate"><span class="pre">gradient</span></code> 指令下同時計算 <code class="docutils literal notranslate"><span class="pre">x</span></code> 與 <code class="docutils literal notranslate"><span class="pre">y</span></code> 的梯度</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">y</span><span class="p">})</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">grad</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/d&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)
dz/dy:  tf.Tensor([-4.  0.  4.], shape=(3,), dtype=float64)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">5.1.3. </span>二階微分矩陣<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 也可以用於計算二次微分矩陣，即黑塞矩陣。其過程需使用到兩組 <code class="docutils literal notranslate"><span class="pre">GradeientTape</span></code>，其中一組用來計算一階導數，另外一組用來紀錄計算一階導數的過程，並計算二階導數，如以下範例</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape2</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape1</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape1</span><span class="o">.</span><span class="n">jacobian</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hess</span> <span class="o">=</span> <span class="n">tape2</span><span class="o">.</span><span class="n">jacobian</span><span class="p">(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d^2z/dx^2: &quot;</span><span class="p">,</span> <span class="n">hess</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>d^2z/dx^2:  [[8. 0. 0.]
 [0. 8. 0.]
 [0. 0. 8.]]
</pre></div>
</div>
</div>
</div>
<p>然而，在此需要特別小心注意張量的尺寸會如何影響計算之過程。</p>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">5.2. </span>數值優化<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3><span class="section-number">5.2.1. </span>手動撰寫優化算則<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>前一小節所使用的範例，其計算過程可以寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z &amp;= f(x)\\
 &amp;= \sum_{i=1}^3\left[2(x_i-2)\right]^2 \\
 &amp;= \sum_{i=1}^3 y_i^2
\end{aligned}
\end{split}\]</div>
<p>若想要找到一 <span class="math notranslate nohighlight">\(\widehat{x}\)</span>，其使得 <span class="math notranslate nohighlight">\(f(\widehat{x})\)</span> 達到最小值的話，由於 <span class="math notranslate nohighlight">\(z\)</span> 為 <span class="math notranslate nohighlight">\(y_1, y_2, y_3\)</span> 的平方和，因此，其會在 <span class="math notranslate nohighlight">\(\widehat{y} = (0, 0, 0)\)</span>的地方達到最小值，也意味著 <span class="math notranslate nohighlight">\(\widehat{x} = (2,2,2)\)</span>。</p>
<p>那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 <span class="math notranslate nohighlight">\(\theta\)</span> 表示模型參數（其扮演範例中<span class="math notranslate nohighlight">\(x\)</span>的角色），<span class="math notranslate nohighlight">\(\mathcal{D}(\theta)\)</span> 表示度量模型好壞的目標函數（其扮演<span class="math notranslate nohighlight">\(f(x)\)</span>的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 的更新規則為</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta} \leftarrow \widehat{\theta} - s \times \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta}
\]</div>
<p>這裏，<span class="math notranslate nohighlight">\(s\)</span> 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 足夠圓滑（smooth），且 <span class="math notranslate nohighlight">\(s\)</span> 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 最小值的發生位置。</p>
<p>在開始進行梯度下降前，我們先定義一函數 <code class="docutils literal notranslate"><span class="pre">f</span></code> 使得 <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">f(x)</span></code>，並了解起始狀態時，<code class="docutils literal notranslate"><span class="pre">f(x)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">x</span></code> 的數值為何：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = [1. 2. 3.]
</pre></div>
</div>
</div>
</div>
<p>使用 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 進行梯度下降，需先計算在當下 <code class="docutils literal notranslate"><span class="pre">x</span></code> 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 <code class="docutils literal notranslate"><span class="pre">x</span></code> 進行更新，即</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = [1.8 2.  2.2]
</pre></div>
</div>
</div>
</div>
<p>這裡，我們將學習速率 <code class="docutils literal notranslate"><span class="pre">lr</span></code> 設為0.1，而張量的 <code class="docutils literal notranslate"><span class="pre">.assign_sub()</span></code> 方法則是就地減去括號內的數值直接更新。透過 <code class="docutils literal notranslate"><span class="pre">f(x)</span></code> 的數值，可觀察到梯度下降的確導致 <code class="docutils literal notranslate"><span class="pre">z</span></code> 數值的下降，而 <code class="docutils literal notranslate"><span class="pre">x</span></code> 也與 <span class="math notranslate nohighlight">\(\widehat{x}=(2,2,2)\)</span> 更加地靠近。</p>
<p>梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 <code class="docutils literal notranslate"><span class="pre">for</span></code> 迴圈，重複更新<span class="math notranslate nohighlight">\(I\)</span>次：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad_x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = [1.8 2.  2.2]
f(x) = 0.320, x = [1.96 2.   2.04]
f(x) = 0.013, x = [1.992 2.    2.008]
f(x) = 0.001, x = [1.9984 2.     2.0016]
f(x) = 0.000, x = [1.99968 2.      2.00032]
f(x) = 0.000, x = [1.999936 2.       2.000064]
f(x) = 0.000, x = [1.9999872 2.        2.0000128]
f(x) = 0.000, x = [1.99999744 2.         2.00000256]
f(x) = 0.000, x = [1.99999949 2.         2.00000051]
f(x) = 0.000, x = [1.9999999 2.        2.0000001]
f(x) = 0.000, x = [1.99999998 2.         2.00000002]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
f(x) = 0.000, x = [2. 2. 2.]
</pre></div>
</div>
</div>
</div>
<p>然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 <code class="docutils literal notranslate"><span class="pre">tol</span></code> 時，算則就停止，其所對應之程式碼為：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad_x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">grad_x</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = [1.8 2.  2.2]
f(x) = 0.320, x = [1.96 2.   2.04]
f(x) = 0.013, x = [1.992 2.    2.008]
f(x) = 0.001, x = [1.9984 2.     2.0016]
f(x) = 0.000, x = [1.99968 2.      2.00032]
f(x) = 0.000, x = [1.999936 2.       2.000064]
f(x) = 0.000, x = [1.9999872 2.        2.0000128]
f(x) = 0.000, x = [1.99999744 2.         2.00000256]
f(x) = 0.000, x = [1.99999949 2.         2.00000051]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tf-optimizers-optimizer">
<h3><span class="section-number">5.2.2. </span>使用<code class="docutils literal notranslate"><span class="pre">tf.optimizers.Optimizer</span></code>進行優化<a class="headerlink" href="#tf-optimizers-optimizer" title="Permalink to this headline">¶</a></h3>
<p>由於 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 <code class="docutils literal notranslate"><span class="pre">tf.optimizers.Optimizer</span></code> 此類型來求得函數的最小值。</p>
<p><code class="docutils literal notranslate"><span class="pre">tf.optimizers.SGD</span></code> 為進行梯度下降法之物件，由於 <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini-batch gradient descent）來強化優化的效能，因此，<code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> 使用 <code class="docutils literal notranslate"><span class="pre">SGD</span></code> 一詞。事實上，除了計算一階導數時資料量的差異外，<code class="docutils literal notranslate"><span class="pre">SGD</span></code> 與傳統的梯度下降並無差異。</p>
<p><code class="docutils literal notranslate"><span class="pre">tf.optimizers.SGD</span></code> 可透過以下的程式碼來使用：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">grad_x</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">z</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">grad_x</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = [1.80000001 2.         2.19999999]
f(x) = 0.320, x = [1.96 2.   2.04]
f(x) = 0.013, x = [1.992 2.    2.008]
f(x) = 0.001, x = [1.9984 2.     2.0016]
f(x) = 0.000, x = [1.99968 2.      2.00032]
f(x) = 0.000, x = [1.999936 2.       2.000064]
f(x) = 0.000, x = [1.9999872 2.        2.0000128]
f(x) = 0.000, x = [1.99999744 2.         2.00000256]
f(x) = 0.000, x = [1.99999949 2.         2.00000051]
</pre></div>
</div>
</div>
</div>
<p>這裡，我們使用 <code class="docutils literal notranslate"><span class="pre">tf.optimizers.SGD</span></code> 來生成優化器（optimizer）物件 <code class="docutils literal notranslate"><span class="pre">opt</span></code>，其在生成時，需要指定學習速率 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>。使用內建優化器時，我們仍須自行計算梯度，並利用 <code class="docutils literal notranslate"><span class="pre">.apply_gradient</span></code> 此方法，給定梯度與變量的列表進行更新。乍看之下，使用優化器並為簡化原有之程式碼，不過，當所欲進行更新的變量很多時，優化器可以避免逐一對變量數值進行更新的麻煩。</p>
<p><code class="docutils literal notranslate"><span class="pre">tf.optimizers.SGD</span></code> 容許使用者加入動能（momentum）<span class="math notranslate nohighlight">\(m\)</span>（其預設為 0），此時，優化算則會改為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\delta &amp; \leftarrow m \times  \delta + \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta} \\
\widehat{\theta} &amp;\leftarrow \widehat{\theta} - s \times \delta
\end{aligned}
\end{split}\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\delta\)</span> 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">tf.optimizers</span></code> 中，有許多不同的優化器（見<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"><code class="docutils literal notranslate"><span class="pre">tf.optimizers</span></code>頁面</a>），如</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Adadelta</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adagrad</span></code>（見<a class="reference external" href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adam</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RMSprop</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a>）</p></li>
</ul>
<p>讀者可自行深入了解這些方法。</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="logistic-regression.html" title="previous page"><span class="section-number">4. </span>邏輯斯迴歸</a>
    <a class='right-next' id="next-link" href="probability-distribution.html" title="next page"><span class="section-number">6. </span>機率分佈</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>