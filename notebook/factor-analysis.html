

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>10. 因素分析 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. 試題反應理論" href="item-response-theory.html" />
    <link rel="prev" title="9. 潛在變項建模" href="latent-variable-modeling.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-tensor.html">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic-regression.html">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-diff-opt.html">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="maximum-likelihood.html">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-mle.html">
   8. Lab: 最大概似估計
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="latent-variable-modeling.html">
   9. 潛在變項建模
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   10. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   11. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixture-modeling.html">
   12. 混合建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-pyro.html">
   13. Lab:
   <code class="docutils literal notranslate">
    <span class="pre">
     pyro
    </span>
   </code>
   簡介
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/factor-analysis.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/factor-analysis.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   10.1. 因素分析模型
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     10.1.1. 模型架構
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     10.1.2. 矩陣形式之模型架構
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     10.1.3. 轉軸不定性
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   10.2. 參數估計
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     10.2.1. 最小平方法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     10.2.2. 最大概似法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     10.2.3. 期望最大化算則
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   10.3. 程式範例
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     10.3.1. 產生因素分析資料
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     10.3.2. 梯度下降法求解
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     10.3.3. 期望最大化算則求解
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     10.3.4. 練習
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1><span class="section-number">10. </span>因素分析<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2><span class="section-number">10.1. </span>因素分析模型<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">10.1.1. </span>模型架構<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math notranslate nohighlight">\(x_i\)</span> 表示個體於第 <span class="math notranslate nohighlight">\(i\)</span> 個測驗（或是試題）的觀測分數（observed score）（<span class="math notranslate nohighlight">\(i=1,2,...,I\)</span>），因素分析（factor analysis）試圖引入 <span class="math notranslate nohighlight">\(M\)</span> 個潛在因素（latent factor）<span class="math notranslate nohighlight">\(\eta_1, \eta_2,...,\eta_M\)</span>，以解釋 <span class="math notranslate nohighlight">\(x_i\)</span> 之變異</p>
<div class="math notranslate nohighlight">
\[
x_i = \nu_i + \sum_{m=1}^M \lambda_{im} \eta_m + \epsilon_i
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\eta_m\)</span> 表示第 <span class="math notranslate nohighlight">\(m\)</span> 個潛在因素，其對 <span class="math notranslate nohighlight">\(x_i\)</span> 之效果 <span class="math notranslate nohighlight">\(\lambda_{im}\)</span> 被稱作因素負荷量（factor loading），其反映 <span class="math notranslate nohighlight">\(\eta_m\)</span> 每變動一單位，預期 <span class="math notranslate nohighlight">\(x_i\)</span> 跟著變動的量，<span class="math notranslate nohighlight">\(\nu_i\)</span> 為試題 <span class="math notranslate nohighlight">\(i\)</span> 之截距，其反映當所有 <span class="math notranslate nohighlight">\(\eta_m = 0\)</span>時，<span class="math notranslate nohighlight">\(x_i\)</span> 的預期數值，而 <span class="math notranslate nohighlight">\(\epsilon_i\)</span> 則為試題 <span class="math notranslate nohighlight">\(i\)</span> 所對應之測量誤差。</p>
<p>因素分析模型假設</p>
<ol class="simple">
<li><p>潛在因素 <span class="math notranslate nohighlight">\(\eta_m\)</span> 與誤差分數 <span class="math notranslate nohighlight">\(\epsilon_i\)</span> 為統計獨立。</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta_m \sim (0, 1)\)</span>，<span class="math notranslate nohighlight">\(\mathbb{C} \text{ov}(\eta_m, \eta_{m'}) = \phi_{mm'}\)</span>。當所有 <span class="math notranslate nohighlight">\(\phi_{mm'}=0\)</span>（<span class="math notranslate nohighlight">\(m \neq m'\)</span>）時，我們稱此因素結構為正交結構（orthogonal structure）。</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i \sim (0, \psi^2_i)\)</span>，<span class="math notranslate nohighlight">\(\mathbb{C} \text{ov}(\epsilon_i, \epsilon_{i'}) =  \psi_{ii'}\)</span>。多數情況下，模型假設<span class="math notranslate nohighlight">\(\psi_{ii'} = 0\)</span>（<span class="math notranslate nohighlight">\(i \neq i'\)</span>）。</p></li>
</ol>
<p>在平均數與共變異數結構方面，當誤差分數間無相關的假設下，該結構為</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_i(\theta) = \nu_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{i}^2(\theta) = \sum_{m=1}^M \lambda_{im}^2 + \psi_{i}^2\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{ij}(\theta) = \sum_{m=1}^M \sum_{k=1}^M \lambda_{im}\lambda_{jk} \phi_{mk}\)</span>。</p></li>
</ol>
</div>
<div class="section" id="id4">
<h3><span class="section-number">10.1.2. </span>矩陣形式之模型架構<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>若我們將 <span class="math notranslate nohighlight">\(\eta_1, \eta_2,...,\eta_M\)</span> 與 <span class="math notranslate nohighlight">\(\lambda_{i1}, \lambda_{i2},...,\lambda_{iM}\)</span> 皆排成 <span class="math notranslate nohighlight">\(M\)</span> 維之向量，即 <span class="math notranslate nohighlight">\(\eta = (\eta_1, \eta_2,...,\eta_M)\)</span> 與 <span class="math notranslate nohighlight">\(\lambda_i = (\lambda_{i1}, \lambda_{i2},...,\lambda_{iM})\)</span>，則前述之方程式可以寫為</p>
<div class="math notranslate nohighlight">
\[
x_i = \nu_i + \lambda_{i}^T \eta + \epsilon_i
\]</div>
<p>進一步，令 <span class="math notranslate nohighlight">\(x = (x_1, x_2, ..., x_I)\)</span>，<span class="math notranslate nohighlight">\(\nu = (\nu_1, \nu_2, ..., \nu_I)\)</span>，以及 <span class="math notranslate nohighlight">\(\epsilon = (\epsilon_1, \epsilon_2, ..., \epsilon_I)\)</span> 皆表示一 <span class="math notranslate nohighlight">\(I \times 1\)</span> 矩陣，而</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Lambda =
\underbrace{\begin{pmatrix}
\lambda_1^T \\
\lambda_2^T \\
\vdots \\
\lambda_I^T
\end{pmatrix}}_{I \times M}
=
\underbrace{\begin{pmatrix}
\lambda_{11} &amp; \lambda_{12} &amp; \cdots &amp; \lambda_{1M} \\
\lambda_{21} &amp; \lambda_{22} &amp; \cdots &amp; \lambda_{2M} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\lambda_{I1} &amp; \lambda_{I2} &amp; \cdots &amp; \lambda_{IM} \\
\end{pmatrix}}_{I \times M}
\end{split}\]</div>
<p>在前述之符號表示下，觀察變項向量 <span class="math notranslate nohighlight">\(x\)</span> 可以被寫為</p>
<div class="math notranslate nohighlight">
\[
x = \nu + \Lambda \eta + \epsilon
\]</div>
<p>我們可以將前述因素分析模型之假設，轉為矩陣之形式：（1）<span class="math notranslate nohighlight">\(\eta\)</span> 與 <span class="math notranslate nohighlight">\(\epsilon\)</span> 兩者獨立；（2）<span class="math notranslate nohighlight">\(\eta \sim (0, \Phi)\)</span> ；（3）<span class="math notranslate nohighlight">\(\epsilon \sim (0, \Psi)\)</span>。此時，平均數與共變異數結構可以寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mu(\theta) &amp;= \nu \\
\Sigma(\theta) &amp;= \Lambda \Phi \Lambda^T + \Psi
\end{aligned}
\end{split}\]</div>
</div>
<div class="section" id="id5">
<h3><span class="section-number">10.1.3. </span>轉軸不定性<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>前述之因素分析模型因著轉軸不定性（rotational indeterminancy），並無法獲得唯一的參數解。</p>
<p>以正交模型為例，令 <span class="math notranslate nohighlight">\(Q\)</span> 表示一 <span class="math notranslate nohighlight">\(M \times M\)</span> 之正交矩陣（orthogonal matrix），即 <span class="math notranslate nohighlight">\(Q\)</span> 滿足 <span class="math notranslate nohighlight">\(Q Q^T = Q^T Q = I\)</span>（<span class="math notranslate nohighlight">\(Q^T\)</span> 為 <span class="math notranslate nohighlight">\(Q\)</span> 之反矩陣），則</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\Sigma(\theta) &amp;= \Lambda Q Q^T \Lambda^T + \Psi \\
&amp;= \Lambda^* {\Lambda^*}^T + \Psi \\
\end{aligned}
\end{split}\]</div>
<p>如果不限制 <span class="math notranslate nohighlight">\(Q\)</span> 為正交矩陣，僅假設：（1）<span class="math notranslate nohighlight">\(Q\)</span> 為對稱矩陣；（2）<span class="math notranslate nohighlight">\(Q^{-1}\)</span> 存在；（3）<span class="math notranslate nohighlight">\(Q^{-1} {Q^{-1}}^T\)</span> 為相關係數矩陣（即對角線為1），則</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\Sigma(\theta) &amp;= \Lambda Q Q^{-1} {Q^{-1}}^T Q^T \Lambda^T + \Psi \\
&amp;= \Lambda^* \Phi^* {\Lambda^*}^T + \Psi \\
\end{aligned}
\end{split}\]</div>
<p>因此，只要給予了一組參數解，我們即可透過 <span class="math notranslate nohighlight">\(Q\)</span> 獲得另一組參數解，且模型適配度與原先的解相同。</p>
<p>傳統上，有兩種取向可獲得因素分析之唯一參數解：</p>
<ol class="simple">
<li><p>探索性因素分析（exploratory factor analysis）利用轉軸以獲得一最精簡之因素負荷量矩陣以移除轉軸不確定性。</p></li>
<li><p>驗證性因素分析（confirmatory factor analysis）將部分的因素負荷量設為 0 以移除轉軸不確定性。</p></li>
</ol>
</div>
</div>
<div class="section" id="id6">
<h2><span class="section-number">10.2. </span>參數估計<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3><span class="section-number">10.2.1. </span>最小平方法<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>給定一樣本共變異數矩陣 <span class="math notranslate nohighlight">\(S\)</span>，其第 <span class="math notranslate nohighlight">\(i,j\)</span> 元素為 <span class="math notranslate nohighlight">\(s_{ij}\)</span>，則一般最小平方（ordinal least squares，簡稱OLS）法透過最小化以下準則以獲得模型參數之估計</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_{OLS}(\theta) =\sum_{i=j}^I \sum_{j=1}^I (s_{ij} - \sigma_{ij}(\theta))^2.
\]</div>
<p>一個與最小平方法有關的變形為最小殘差（minimum residual，簡稱MINRES）法，其僅考慮共變異數之非對角線元素進行估計</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_{MINRES}(\theta) =\sum_{i=j+1}^I \sum_{j=1}^{I-1} (s_{ij} - \sigma_{ij}(\theta))^2.
\]</div>
<p>當所有的 <span class="math notranslate nohighlight">\(\psi_i^2\)</span> 皆可被自由估計時，MINRES與OLS兩者為等價的。</p>
<p>前述的OLS法可以進一步引入權重，即成為加權最小平方法（weighted least squares，簡稱WLS），其估計準則改為</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_{WLS}(\theta) =\sum_{i=j}^I \sum_{j=1}^I w_{ij} (s_{ij} - \sigma_{ij}(\theta))^2.
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(w_{ij}\)</span> 表示對 <span class="math notranslate nohighlight">\((s_{ij} - \sigma_{ij}(\theta))\)</span> 此殘差給予的權重，其並非模型之參數，乃研究者於估計準則中給定的，當 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 越大，即表示研究者希望 <span class="math notranslate nohighlight">\((s_{ij} - \sigma_{ij}(\theta))\)</span> 之差異應越小越好。</p>
</div>
<div class="section" id="id8">
<h3><span class="section-number">10.2.2. </span>最大概似法<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>在因素分析的模型假設下，<span class="math notranslate nohighlight">\(x\)</span> 之平均數與共變異數為 <span class="math notranslate nohighlight">\(\mu(\theta)\)</span> 與 <span class="math notranslate nohighlight">\(\Sigma(\theta))\)</span>，若再進一步引進多元常態分配之假設，則 <span class="math notranslate nohighlight">\(x \sim \text{Normal}(\mu(\theta), \Sigma(\theta))\)</span>，此時，<span class="math notranslate nohighlight">\(x\)</span> 之對數機率密度函數為</p>
<div class="math notranslate nohighlight">
\[
\log f(x;\theta) = -\frac{I}{2} \log{2\pi} - \frac{1}{2} \log |\Sigma(\theta)| - \frac{1}{2} (x - \mu(\theta))^T \Sigma(\theta) ^{-1} (x - \mu(\theta))
\]</div>
<p>因此，給定樣本資料 <span class="math notranslate nohighlight">\(x_1, x_2,...,x_N\)</span>下，最大概似估計準則可以寫為</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta) = C  -\frac{N}{2} \log |\Sigma(\theta)| - \frac{1}{2} \sum_{n=1}^N (x_n - \mu(\theta))^T \Sigma(\theta) ^{-1} (x_n - \mu(\theta))
\]</div>
<p>前述之最大概似準則可以簡化為</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta) = C  - \frac{N}{2} \log |\Sigma(\theta)| - \frac{N}{2} tr(\Sigma(\theta) ^{-1} S) - \frac{N}{2} (m - \mu(\theta))^T \Sigma(\theta) ^{-1} (m - \mu(\theta))
\]</div>
</div>
<div class="section" id="id9">
<h3><span class="section-number">10.2.3. </span>期望最大化算則<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>期望最大化算則（expectation-maximization algorithm，簡稱EM算則）常用於處理不完整資料（incomplete data）的最大概似估計問題。在心理計量領域，潛在因素可被視為不完整資料，因此，EM算則可用於處理心理計量模型之估計問題。</p>
<p>在因素分析的問題上，若 <span class="math notranslate nohighlight">\(\eta\)</span> 可以直接被觀察，則我們可以考慮以下的完整資料之對數機率密度函數</p>
<div class="math notranslate nohighlight">
\[
\log f(x,\eta;\theta) = \log f(x|\eta; \theta) + \log f(\eta; \theta)
\]</div>
<p>若我們假設測量誤差 <span class="math notranslate nohighlight">\(\epsilon_i\)</span> 為常態分配，且 <span class="math notranslate nohighlight">\(\epsilon_i\)</span> 與 <span class="math notranslate nohighlight">\(\epsilon_j\)</span> 獨立（<span class="math notranslate nohighlight">\(i \neq j\)</span>），則在給定 <span class="math notranslate nohighlight">\(\eta\)</span> 之下，我們有</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(x_i|\eta \sim \text{Normal}(\nu_i + \lambda_i^T \eta, \psi_i^2)\)</span></p></li>
<li><p>給定 <span class="math notranslate nohighlight">\(\eta\)</span> 之下，<span class="math notranslate nohighlight">\(x_i\)</span> 與 <span class="math notranslate nohighlight">\(x_j\)</span> 為獨立。</p></li>
</ol>
<p>因此，<span class="math notranslate nohighlight">\(\log f(x|\eta; \theta)\)</span> 可以寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\log f(x|\eta; \theta) &amp;= \sum_{i=1}^I \log f(x_i|\eta; \theta)\\
&amp;= \sum_{i=1}^I \left[
C -\frac{1}{2} \log \psi_i^2
 -\frac{1}{2 \psi_i^2} (x_i - \nu_i - \lambda_i^T \eta)^2
\right]
\end{aligned}
\end{split}\]</div>
<p>這意味著，在 <span class="math notranslate nohighlight">\(\eta\)</span> 可直接觀察到的情況下，估計因素負荷量與截距，只是一線性回歸的問題。</p>
<p>在 <span class="math notranslate nohighlight">\(\eta\)</span> 服從多元常態分配的假設下，<span class="math notranslate nohighlight">\(\log f(\eta; \theta)\)</span> 可以寫為</p>
<div class="math notranslate nohighlight">
\[
\log f(\eta;\theta) = C- \frac{1}{2} \log |\Phi| - \frac{1}{2} \eta^T \Phi^{-1} \eta
\]</div>
<p>若我們進一步假設 <span class="math notranslate nohighlight">\(\eta\)</span> 為正交結構，此時，<span class="math notranslate nohighlight">\(\Phi\)</span> 為單位矩陣，我們甚至不用對 <span class="math notranslate nohighlight">\(\eta\)</span> 的分配參數進行估計。</p>
<p>給定樣本資料 <span class="math notranslate nohighlight">\((x_1, \eta_1), (x_2, \eta_2), ..., (x_N, \eta_N)\)</span>，完整資料的概似函數可以寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell_{\text{comp}}(\theta) &amp;= \sum_{n=1}^N \log f(x_n, \eta_n; \theta) \\
&amp;=
C +
\sum_{n=1}^N
\sum_{i=1}^I
\left[
-\frac{1}{2} \log \psi_i^2
 -\frac{1}{2 \psi_i^2} (x_{ni} - \nu_i - \lambda_i^T \eta_n)^2
\right]
+
\sum_{n=1}^N
\left[
- \frac{1}{2} \log |\Phi| - \frac{1}{2} \eta_n^T \Phi^{-1} \eta_n
\right]
\end{aligned}
\end{split}\]</div>
<p>EM算則牽涉到期望步驟（E-Step）與最大化步驟（M-Step）。在E-Step時，我們計算在給定觀察資料 <span class="math notranslate nohighlight">\(\mathcal{X}={x_n}_{n=1}^N\)</span>，以及當下參數估計值 <span class="math notranslate nohighlight">\(\widehat{\theta}^{(t)}\)</span>，完整資料概似函數之條件期望值，即</p>
<div class="math notranslate nohighlight">
\[
Q(\theta|\widehat{\theta}^{(t)}) = \mathbb{E}\left[ \ell_{\text{comp}}(\theta) | \mathcal{X}; \widehat{\theta}^{(t)} \right]
\]</div>
<p>而在M-Step時，我們則試圖找到一<span class="math notranslate nohighlight">\(\widehat{\theta}^{(t+1)}\)</span>，其可最大化 <span class="math notranslate nohighlight">\(Q(\theta|\widehat{\theta}^{(t)})\)</span>，即</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta}^{(t+1)} =\text{argmax}_{\theta} \ Q(\theta|\widehat{\theta}^{(t)})
\]</div>
<p>在因素分析的EM算則中，E-Step的關鍵在於，要能夠計算在給定 <span class="math notranslate nohighlight">\(x\)</span> 之下，<span class="math notranslate nohighlight">\(\eta\)</span> 的分佈特性。當 <span class="math notranslate nohighlight">\(\eta\)</span> 與 <span class="math notranslate nohighlight">\(\epsilon\)</span> 皆為常態分配時，<span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(\eta\)</span> 亦服從常態分配，其平均數與變異數為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
x \\
\eta
\end{pmatrix}
\sim
\text{Normal}
\left[
\begin{pmatrix}
\nu \\
0
\end{pmatrix}
,
\begin{pmatrix}
\Lambda \Phi \Lambda^T + \Psi &amp; \Lambda \Phi \\
 \Phi \Lambda^T &amp; \Phi
\end{pmatrix}
\right]
\end{split}\]</div>
<p>接著，利用多元常態分配條件分配之特性，我們可以得</p>
<div class="math notranslate nohighlight">
\[
\eta| x \sim \text{Normal}
\left[
\Phi \Lambda^T \Sigma(\theta)^{-1}(x - \nu),
\Phi - \Phi \Lambda^T \Sigma(\theta)^{-1} \Lambda \Phi
\right]
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\Sigma(\theta) = \Lambda \Phi \Lambda^T + \Psi\)</span> 即為因素分析之共變結構。</p>
</div>
</div>
<div class="section" id="id10">
<h2><span class="section-number">10.3. </span>程式範例<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id11">
<h3><span class="section-number">10.3.1. </span>產生因素分析資料<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="k">def</span> <span class="nf">create_fa_model</span><span class="p">(</span><span class="n">n_factor</span><span class="p">,</span> <span class="n">n_item</span><span class="p">,</span> <span class="n">ld</span><span class="p">,</span> <span class="n">psi</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rho</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n_item</span> <span class="o">%</span> <span class="n">n_factor</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n_item</span> <span class="o">=</span> <span class="n">n_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_item</span> <span class="o">//</span> <span class="n">n_factor</span><span class="p">)</span>
    <span class="n">loading</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_item</span><span class="p">,</span> <span class="n">n_factor</span><span class="p">))</span>
    <span class="n">item_per_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_item</span> <span class="o">//</span> <span class="n">n_factor</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_factor</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">item_per_factor</span><span class="p">,</span>
                       <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">item_per_factor</span><span class="p">):</span>
            <span class="n">loading</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ld</span>
    <span class="k">if</span> <span class="n">rho</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_factor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_factor</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_factor</span><span class="p">)</span>
        <span class="n">cor</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="p">(</span><span class="n">unit</span> <span class="o">@</span> <span class="n">unit</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">identity</span>
    <span class="k">if</span> <span class="n">psi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uniqueness</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">loading</span> <span class="o">@</span> <span class="n">cor</span> <span class="o">@</span> <span class="n">loading</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">uniqueness</span> <span class="o">=</span> <span class="n">psi</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_item</span><span class="p">,</span> <span class="p">))</span>
    <span class="k">return</span> <span class="n">loading</span><span class="p">,</span> <span class="n">uniqueness</span><span class="p">,</span> <span class="n">cor</span>

<span class="k">def</span> <span class="nf">generate_fa_data</span><span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">loading</span><span class="p">,</span> <span class="n">uniqueness</span><span class="p">,</span> <span class="n">cor</span><span class="p">):</span>
    <span class="n">n_item</span> <span class="o">=</span> <span class="n">loading</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_item</span><span class="p">,</span> <span class="p">))</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">loading</span> <span class="o">@</span> <span class="n">cor</span> <span class="o">@</span> <span class="n">loading</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">uniqueness</span><span class="p">)</span>
    <span class="n">mvn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">mean</span><span class="p">,</span> <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">mvn</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_sample</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">246437</span><span class="p">)</span>
<span class="n">loading_true</span><span class="p">,</span> <span class="n">uniqueness_true</span><span class="p">,</span> <span class="n">cor_true</span> <span class="o">=</span> <span class="n">create_fa_model</span><span class="p">(</span><span class="n">n_factor</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n_item</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="n">ld</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">generate_fa_data</span><span class="p">(</span><span class="n">n_sample</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
                        <span class="n">loading</span> <span class="o">=</span> <span class="n">loading_true</span><span class="p">,</span>
                        <span class="n">uniqueness</span> <span class="o">=</span> <span class="n">uniqueness_true</span><span class="p">,</span>
                        <span class="n">cor</span> <span class="o">=</span> <span class="n">cor_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id12">
<h3><span class="section-number">10.3.2. </span>梯度下降法求解<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="n">loading</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span> <span class="o">*</span> <span class="n">loading_true</span>
<span class="n">uniqueness</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span> <span class="o">*</span> <span class="n">uniqueness_true</span>
<span class="c1"># loading_mask = 1 *  (loading_true != 0)</span>
<span class="n">loading</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">uniqueness</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">lr</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">loading</span><span class="p">,</span> <span class="n">uniqueness</span><span class="p">],</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model_fa</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">loading</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="p">)),</span>
        <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span>
            <span class="n">loading</span> <span class="o">@</span> <span class="n">loading</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">uniqueness</span><span class="p">)))</span>
    <span class="n">loss_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">model_fa</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model_fa</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loading</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">uniqueness</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(-156878.2031, grad_fn=&lt;SumBackward0&gt;)
tensor([[ 0.6415, -0.1556, -0.0308, -0.2295],
        [ 0.6335, -0.1715, -0.0402, -0.2255],
        [ 0.6439, -0.1686, -0.0293, -0.2139],
        [ 0.1495,  0.6905,  0.0190, -0.0976],
        [ 0.1363,  0.6855,  0.0191, -0.0832],
        [ 0.1510,  0.6881,  0.0054, -0.0816],
        [ 0.1802,  0.0225,  0.5769,  0.3776],
        [ 0.1713,  0.0067,  0.5490,  0.3713],
        [ 0.1773, -0.0078,  0.5651,  0.3506],
        [ 0.1925,  0.0195, -0.4087,  0.5301],
        [ 0.1839,  0.0250, -0.4217,  0.5419],
        [ 0.1761,  0.0442, -0.4049,  0.5559]], requires_grad=True)
tensor([0.5050, 0.5047, 0.5129, 0.5076, 0.5058, 0.5120, 0.4718, 0.5181, 0.5199,
        0.5256, 0.4999, 0.5083], requires_grad=True)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id13">
<h3><span class="section-number">10.3.3. </span>期望最大化算則求解<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_sample</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_item</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_factor</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">beta</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span> <span class="o">*</span> <span class="n">loading_true</span>
<span class="n">tau</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">uniqueness</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">c_yy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_sample</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">e_step</span><span class="p">(</span><span class="n">c_yy</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">sigma_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">beta</span> <span class="o">@</span> <span class="n">beta</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">tau</span><span class="p">)</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span>
    <span class="n">delta_small</span> <span class="o">=</span> <span class="n">sigma_inv</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">delta_big</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_factor</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">sigma_inv</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">c_yz_hat</span> <span class="o">=</span> <span class="n">c_yy</span> <span class="o">@</span> <span class="n">delta_small</span>
    <span class="n">c_zz_hat</span> <span class="o">=</span> <span class="n">delta_small</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">c_yy</span> <span class="o">@</span> <span class="n">delta_small</span> <span class="o">+</span> <span class="n">delta_big</span>
    <span class="k">return</span> <span class="n">c_yz_hat</span><span class="p">,</span> <span class="n">c_zz_hat</span>

<span class="k">def</span> <span class="nf">m_step</span><span class="p">(</span><span class="n">c_yy</span><span class="p">,</span> <span class="n">c_yz_hat</span><span class="p">,</span> <span class="n">c_zz_hat</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">c_yz_hat</span> <span class="o">@</span> <span class="n">c_zz_hat</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">c_yy</span> <span class="o">-</span> <span class="n">c_yz_hat</span> <span class="o">@</span> <span class="n">c_zz_hat</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span> <span class="o">@</span> <span class="n">c_yz_hat</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">c_yz_hat</span><span class="p">,</span> <span class="n">c_zz_hat</span> <span class="o">=</span> <span class="n">e_step</span><span class="p">(</span><span class="n">c_yy</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">m_step</span><span class="p">(</span><span class="n">c_yy</span><span class="p">,</span> <span class="n">c_yz_hat</span><span class="p">,</span> <span class="n">c_zz_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">tau</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 6.9943e-01,  9.4414e-03,  7.0608e-03, -1.6059e-03],
        [ 6.9496e-01, -8.3238e-03, -5.2081e-04,  4.1212e-03],
        [ 6.9935e-01, -4.6706e-03,  1.7090e-02,  9.4043e-03],
        [ 4.8433e-03,  7.1317e-01,  4.2302e-03, -1.3696e-02],
        [-1.0689e-02,  7.0387e-01,  8.7874e-03, -6.3474e-03],
        [ 2.4067e-03,  7.0916e-01,  2.0881e-03,  6.8173e-03],
        [ 3.4815e-03,  1.9012e-02,  7.1275e-01,  2.1692e-03],
        [ 2.6130e-03,  2.2796e-03,  6.8453e-01,  1.0449e-02],
        [ 1.7609e-02, -7.6797e-03,  6.8791e-01, -1.4011e-02],
        [ 1.8089e-02, -1.0909e-02, -2.5496e-03,  6.9649e-01],
        [ 5.7851e-03, -9.0123e-03, -8.8994e-03,  7.1109e-01],
        [-1.1556e-02,  6.4172e-03,  1.0414e-02,  7.1110e-01]],
       grad_fn=&lt;MmBackward&gt;)
tensor([0.5050, 0.5047, 0.5128, 0.5075, 0.5058, 0.5119, 0.4718, 0.5181, 0.5197,
        0.5255, 0.4999, 0.5082], grad_fn=&lt;DiagBackward&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id14">
<h3><span class="section-number">10.3.4. </span>練習<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>問如何確定前述兩種方法得到的解都是最大概似解。</p></li>
<li><p>如何對於因素間之因素進行估計。</p></li>
<li><p>如何僅估計真實模型中不為零之因素負荷量。</p></li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="latent-variable-modeling.html" title="previous page"><span class="section-number">9. </span>潛在變項建模</a>
    <a class='right-next' id="next-link" href="item-response-theory.html" title="next page"><span class="section-number">11. </span>試題反應理論</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>