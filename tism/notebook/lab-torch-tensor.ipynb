{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "colab": {
   "name": "lab-torch-tensor.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "0vVgs3K29Aub",
    "ZZfIKPlD9Aui",
    "rhhG55d59Auk",
    "Jc5ZK-x19Aur",
    "lel6BArc9Auw",
    "_A-Y8qaR9Au-"
   ]
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UPBFUdBe9At-"
   },
   "source": [
    "Lab: 張量與線性代數\n",
    "================\n",
    "\n",
    "此 lab 中，我們將會透過 `torch` 此套件，學習以下的主題。\n",
    "\n",
    "1. 認識 `torch` 的張量（tensor）之基礎。\n",
    "\n",
    "2. 了解如何對 `torch` 張量進行操弄。\n",
    "\n",
    "3. 使用 `torch` 進行線性代數之運算。\n",
    "\n",
    "4. 應用前述之知識，建立一可進行線性迴歸分析之類型（class）。\n",
    "\n",
    "`torch`之安裝與基礎教學，可參考 [PyTorch官方網頁](https://pytorch.org/get-started/locally)，如果讀者使用的是Google的Colab服務，則不需要另外安裝 `torch`。在安裝完 `torch` 後，可透過以下的指令載入"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9zz89G869At_"
   },
   "source": [
    "import torch"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "awqR2BrI9AuB"
   },
   "source": [
    "## 張量之基礎\n",
    "\n",
    "### 張量之輸入\n",
    "`torch` 最基本的物件是張量（tensor），其與 `numpy` 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 `python` 的 `list` 或是 `numpy` 的 `ndarray`），置於`torch.tensor`函數中\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wLmt_TIE9AuC",
    "outputId": "16da78cc-500f-46cd-aa64-12c52cbe3efc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "a = torch.tensor(data = [[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12]])\n",
    "type(a)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "zrMkaYVc9AuE"
   },
   "source": [
    "透過 `type()`，可看見其屬於 `torch.Tensor` 此一類型（class），若欲了解 `a` 的樣貌，我們可使用 `print` 指令來列印其主要的內容"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E31n21Kr9AuF",
    "outputId": "976d7564-8173-49a4-8ffd-a88cc1aef5ce",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "print(a)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "OIeqlSWi9AuH"
   },
   "source": [
    "透過對 `a` 列印的結果，我們可觀察到：\n",
    "\n",
    "+ `a` 內部的資料數值（value）為 `[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]`。\n",
    "\n",
    "除此之外，`a` 還有兩個重要的屬性並未顯示在列印的結果中：\n",
    "\n",
    "+ `a` 的尺寸（size）為 `(3, 4)`，表示 `a` 為一 $3 \\times 4$ 之 2d 張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則。\n",
    "+ `a` 的資料類型（data type）為 `int64`，表示64位元的整數。在進行運算時，張量間的類型須相同。\n",
    "\n",
    "稍後，我們會討論如何獲得 `torch` 張量的尺寸與資料類型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n-FdRyM9AuI"
   },
   "source": [
    "\n",
    "### 張量之數值\n",
    "若要擷取 `torch` 張量的資料數值（value），則可透過 `.numpy()`獲得，其回傳該張量對應之 `numpy` 陣列"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VP4HV4wW9AuI",
    "outputId": "e15d3850-5338-401b-9655-882dc2b321c4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    }
   },
   "source": [
    "print(\"data of tensor is: \\n\", a.numpy())"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data of tensor is: \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtujoPzv9AuL"
   },
   "source": [
    "`numpy` 陣列是 `python` 進行科學運算時，幾乎都會仰賴的資料格式。因此，`.numpy()` 此指令主要用於不同套件間的資料交換，或是希望列印出來的結果比較簡單使用。\n",
    "\n",
    "在形成張量時，要記得張量的變數名稱，僅為其表徵資料的一個標籤，而相同的資料，可以有許多不同的標籤指稱其。舉例來說，考慮一下的程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor c is: \n",
      " [[1 1 1]\n",
      " [2 2 2]]\n",
      "tensor d is: \n",
      " [[1 1 1]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([[1, 1, 1], [2, 2, 2]])\n",
    "d = c\n",
    "print(\"tensor c is: \\n\",\n",
    "      c.numpy())\n",
    "print(\"tensor d is: \\n\",\n",
    "      d.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "沒有意外的，`c` 和 `d` 內部的數值是一樣的。然而，若我們利用 `.fill_()` 方法，將 `c` 的內部全部填入 0 的話，則我們可以看到不僅是 `c`，`d` 內部的資料亦改變了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor c is: \n",
      " [[0 0 0]\n",
      " [0 0 0]]\n",
      "tensor d is: \n",
      " [[0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "c.fill_(0)\n",
    "print(\"tensor c is: \\n\",\n",
    "      c.numpy())\n",
    "print(\"tensor d is: \\n\",\n",
    "      d.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在此，讀者需特別注意的是，`torch` 中若有方法的尾端是底線 `_` 的話，則意味著該方法會取代原有物件中的資料，如 `c` 此張量對應的資料直接被取代掉了，不需要另外寫 `c = c.fill_(0)`。\n",
    "\n",
    "前述的設計，主要是為了避免資料的拷貝，以減少記憶體的使用。如果說希望 `d` 表徵的資料為 `c` 對應資料的拷貝的話，可以使用 `.clone()`此方法："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "d = c.clone()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如此，就不會出現更動 `c` 的資料，`d` 也跟著動的狀況發生。\n",
    "\n",
    "\n",
    "`torch` 內建了多種函數，以協助產生具有特別數值結構之張量："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_1e7KRQC9AuL",
    "outputId": "178d4042-8ec1-4fcb-ebb5-b4356e78b86f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    }
   },
   "source": [
    "print(\"tensor with all elements being ones \\n\",\n",
    "      torch.ones(size = (4, 1)).numpy())\n",
    "print(\"tensor with all elements being zeros \\n\",\n",
    "      torch.zeros(size = (2, 3)).numpy())\n",
    "print(\"identity-like tensor \\n\",\n",
    "      torch.eye(n = 3, m = 5).numpy())\n",
    "print(\"diagonal matrix \\n\",\n",
    "      torch.diag(input = torch.tensor([1, 2, 3, 4])).numpy())"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with all elements being ones \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "tensor with all elements being zeros \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "identity-like tensor \n",
      " [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "diagonal matrix \n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRYflb3s9AuN"
   },
   "source": [
    "`torch` 亦可隨機地產生資料，或是直接使用尚未起始化的資料："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qSHYPt0B9AuO",
    "outputId": "934eea4a-e941-4ebd-f001-219502fe0b5c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    }
   },
   "source": [
    "print(\"tensor with random elements from uniform(0, 1) \\n\",\n",
    "      torch.rand(size = (2, 6)).numpy())\n",
    "print(\"tensor with uninitialized data \\n\",\n",
    "      torch.empty(size = (2, 6)).numpy())\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with random elements from uniform(0, 1) \n",
      " [[0.80467886 0.84691507 0.14114821 0.3856095  0.87928057 0.98100513]\n",
      " [0.94347435 0.34811568 0.4800673  0.4131937  0.38151032 0.28071737]]\n",
      "tensor with uninitialized data \n",
      " [[0.0000000e+00 8.5899346e+09 1.6103790e+06 3.6902465e+19 1.1210388e-44\n",
      "  0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 張量之形狀\n",
    "\n",
    "張量之形狀與形狀維度之數量，可透過張量物件的 `.size()`（或 `.shape`） 與 `dim` 方法來獲得"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KU6P1KU49AuR",
    "outputId": "a4ad521d-1e0c-4381-c260-325ea8653a04",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "print(\"size of tensor is\", a.size())\n",
    "print(\"size of tensor is\", a.shape)\n",
    "print(\"dim of tensor is\", a.dim())"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of tensor is torch.Size([3, 4])\n",
      "size of tensor is torch.Size([3, 4])\n",
      "dim of tensor is 2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36tAvwMJ9AuT"
   },
   "source": [
    "如果要對張量的形狀進行改變的話，可透過 `.view()` 此方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bLPQX1Kk9AuU",
    "outputId": "8d647ee2-0107-4ea1-932c-75660b0d4f9d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    }
   },
   "source": [
    "print(\"tensor with shape (4, 3): \\n\",\n",
    "      a.view(size = (4, 3)).numpy())\n",
    "print(\"tensor with shape (2, 2, 3): \\n\",\n",
    "      a.view(size = (2, 2, 3)).numpy())\n",
    "print(\"tensor with shape (12, 1): \\n\",\n",
    "      a.view(size = (12, 1)).numpy())\n",
    "print(\"tensor with shape (12, 1) by (-1, 1): \\n\",\n",
    "      a.view(size = (-1, 1)).numpy())\n",
    "print(\"tensor with shape (12,): \\n\",\n",
    "      a.view(size = (12, )).numpy())\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with shape (4, 3): \n",
      " [[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "tensor with shape (2, 2, 3): \n",
      " [[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]]\n",
      "tensor with shape (12, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12, 1) by (-1, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12,): \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "OQ3-AgiN9AuW"
   },
   "source": [
    "注意，`(12, 1)` 與 `(12,)` 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。另外，-1表示該面向對應之尺寸，由其它面向決定。\n",
    "\n",
    "`.view()` 所回傳的張量，即為原本張量之資料在不同尺寸下對應之張量，`torch` 並未對資料進行拷貝（copy）。考慮以下的程式碼"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qzTKjJGA9AuW",
    "outputId": "d0c29f45-b7e8-4842-e98b-4b1128e8bf03",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    }
   },
   "source": [
    "c = torch.zeros(4, 2)\n",
    "d = c.view((8, ))\n",
    "print(\"tensor c is: \\n\",\n",
    "      c.numpy())\n",
    "print(\"tensor d is: \\n\",\n",
    "      d.numpy())"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor c is: \n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "tensor d is: \n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "NKh83wdl9AuY"
   },
   "source": [
    "如預期地，`c` 與 `d` 內部有著相同的數值，僅差在尺寸有所不同。接著，我們將 `c` 的內部全部填入1，我們會觀察到 `d` 的數值也跟著改變了："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nkdDGNxj9AuZ",
    "outputId": "a8d73bc4-0a6b-4eae-c64e-fadf2c84c2de",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    }
   },
   "source": [
    "c.fill_(1)\n",
    "print(\"tensor c is: \\n\",\n",
    "      c.numpy())\n",
    "print(\"tensor d is: \\n\",\n",
    "      d.numpy())"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor c is: \n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "tensor d is: \n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "0vVgs3K29Aub"
   },
   "source": [
    "這顯示 `c` 與 `d` 背後有著共享的資料內容。\n",
    "\n",
    "\n",
    "在0.4版之後，`.reshape()` 此方法亦可改變張量的尺寸，但其有可能會對資料進行拷貝，不過，當資料本身的排列不具有連續性時，僅 `.reshape()` 能夠使用。因此，在 `torch` 的[官方文件](https://pytorch.org/docs/master/tensors.html#torch.Tensor.view)中，建議使用 `.reshape()`此指令。\n",
    "\n",
    "### 張量之資料類型\n",
    "張量的資料類型，可透過 `.dtype` 方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lCtbW1j_9Auc",
    "outputId": "16a3c8fa-a6b9-4f0c-db37-4b6c408662a5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"data type of tensor is\", a.dtype)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of tensor is torch.int64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "N6ECGYQu9Aue"
   },
   "source": [
    "若是要調整資料類型的話，則可透過 `.type()` 此方法："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cGfi2Z2a9Aue",
    "outputId": "57380458-cd59-46b0-926b-c32930b62113",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(a.type(torch.float64))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]], dtype=torch.float64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "IRso66O69Auh"
   },
   "source": [
    "`torch` 內建多種資料類型，包含整數類型（如 `torch.int32` 與 `torch.int64`）與浮點數類型（如 `torch.float32` 與 `torch.float64`），完整的資料類型請見 [`torch.Tensor`文件](https://pytorch.org/docs/stable/tensors.html)之 `dtype` 欄位。\n",
    "\n",
    "在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 `torch` 常用之資料類型為 `torch.float32` 與 `torch.float64`，前者所需的記憶體較小，但運算結果的數值誤差較大。\n",
    "\n",
    "\n",
    "除了利用 `torch.tensor()` 搭配 `dtype` 產生張量外，另一種產生張量的方法為，直接利用特定資料型態張量的建構式，再利用特定的方法填入數值。例如，以下的程式碼在 CPU 先產稱了一資料類型為 `torch.float64` 之張量，再利用均勻分配隨機生成資料填入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "c = torch.FloatTensor(2, 4).uniform_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "TAeKTEDl9Auh"
   },
   "source": [
    "前述之張量建構風格，在後續討論到 GPU 計算時會很有幫助。不同資料類型之建構式，可同樣參考 [torch.Tensor文件](https://pytorch.org/docs/stable/tensors.html) 之 `CPU tensor` 欄位（若要在 GPU 上建構張量，則是參考 `GPU tensor` 之欄位）。\n",
    "\n",
    "\n",
    "## 張量之操弄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZfIKPlD9Aui"
   },
   "source": [
    "### 張量之切片\n",
    "\n",
    "若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。`torch` 張量的切片方式，與 `numpy` 類似，皆使用中括號 `[]`，再搭配所欲擷取資料行列的索引（index）獲得。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "34zpU56v9Aui",
    "outputId": "45365e96-8031-4ea3-dd39-980cb54a9bbd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"extract 1st row: \\n\",\n",
    "      a[0, :].numpy())\n",
    "print(\"extract 1st and 2nd rows: \\n\",\n",
    "      a[:2, :].numpy())\n",
    "print(\"extract 2nd column: \\n\",\n",
    "      a[:, 1].numpy())\n",
    "print(\"extract 2nd and 3rd columns: \\n\",\n",
    "      a[:, 1:3].numpy())"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract 1st row: \n",
      " [1 2 3 4]\n",
      "extract 1st and 2nd rows: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "extract 2nd column: \n",
      " [ 2  6 10]\n",
      "extract 2nd and 3rd columns: \n",
      " [[ 2  3]\n",
      " [ 6  7]\n",
      " [10 11]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "VqlvOjlH9Auk"
   },
   "source": [
    "進行切片時，有幾項重點需要注意。\n",
    "\n",
    "+ 各面向之索引從0開始。\n",
    "+ 負號表示從結尾數回來，如 `-1` 表示最後一個位置。\n",
    "+ `:`表示該面向所有元素皆挑選。\n",
    "+ `start:stop` 表示從 `start` 開始挑選到 `stop-1`。\n",
    "+ `start:stop:step` 表示從 `start` 開始到 `stop-1`，間隔 `step` 挑選。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rhhG55d59Auk"
   },
   "source": [
    "### 張量之串接\n",
    "多個張量在維度可對應之前提下，可透過 `torch.cat` 串接"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-eQxRvjK9Aul",
    "outputId": "f1be9142-161e-47e5-97f9-2f525e609e3a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"vertical concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 0).numpy())\n",
    "print(\"horizontal concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 1).numpy())"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertical concatenation \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "horizontal concatenation \n",
      " [[ 1  2  3  4  1  2  3  4]\n",
      " [ 5  6  7  8  5  6  7  8]\n",
      " [ 9 10 11 12  9 10 11 12]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu5qantc9Auo"
   },
   "source": [
    "## 張量之運算\n",
    "考慮以下 `a` 與 `b` 兩張量"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "23l3g8eK9Auo",
    "outputId": "887134af-089e-4a80-c81b-278eb41c8aeb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    }
   },
   "source": [
    "a = torch.tensor(data = [[1, 2], [3, 4], [5, 6]],\n",
    "                dtype = torch.float64)\n",
    "b = torch.tensor(data = [[1, 2], [1, 2], [1, 2]],\n",
    "                dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a.numpy())\n",
    "print(\"tensor b is \\n\", b.numpy())\n"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a is \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "tensor b is \n",
      " [[1. 2.]\n",
      " [1. 2.]\n",
      " [1. 2.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "4DbB7FLK9Auq"
   },
   "source": [
    "我們將使用 `a` 與 `b` 來展示如何使用 `torch` 進行張量間的計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc5ZK-x19Aur"
   },
   "source": [
    "### 張量元素對元素之運算\n",
    "透過 `torch` 的數學函數，可進行張量元素對元素的四則運算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TZa753SL9Aur",
    "outputId": "0dac1de8-0d7d-4426-dd1d-6e4a96a6fec9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"element-wise add \\n\",\n",
    "      torch.add(a, b))\n",
    "print(\"element-wise subtract \\n\",\n",
    "      torch.sub(a, b))\n",
    "print(\"element-wise multiply \\n\",\n",
    "      torch.mul(a, b))\n",
    "print(\"element-wise divide \\n\",\n",
    "      torch.div(a, b))"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ZH5s1OBS9Aut"
   },
   "source": [
    "前述採用的函數，皆可取代為其所對應之運算子計算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rLrvLPV-9Aut",
    "outputId": "7650b084-9bfe-40fe-faec-c53604926ab2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"element-wise add \\n\", a + b)\n",
    "print(\"element-wise subtract \\n\", a - b)\n",
    "print(\"element-wise multiply \\n\", a * b)\n",
    "print(\"element-wise divide \\n\", a / b)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LHWAZ7Eb9Auv"
   },
   "source": [
    "若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 [troch官方文件](https://pytorch.org/docs/stable/torch.html#math-operations) 此模組中尋找對應的數學函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "lel6BArc9Auw"
   },
   "source": [
    "### 張量線性代數之運算\n",
    "除了簡單的四則運算外，當張量的 `dim` 為2時，`torch` 提供了進行線性代數（linear algebra）相關的函數，如\n",
    "\n",
    "+ 矩陣轉置（matrix transpose）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zb-vLX2X9Auw",
    "outputId": "c86433e9-0dde-4a27-9753-fe226b5ffaef",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "a_t = torch.transpose(input=a, dim0=0, dim1=1)\n",
    "print(\"transpose of a is \\n\",\n",
    "      a_t.numpy())\n",
    "print(\"transpose of a is \\n\",\n",
    "      a.t().numpy())"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transpose of a is \n",
      " [[1. 3. 5.]\n",
      " [2. 4. 6.]]\n",
      "transpose of a is \n",
      " [[1. 3. 5.]\n",
      " [2. 4. 6.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "e083H5jH9Auy"
   },
   "source": [
    "+ 矩陣乘法（matrix multiplication）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Hch3BB9k9Auy",
    "outputId": "eb313781-502c-4921-ebb6-913db8ab27ce",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "c = a_t @ a\n",
    "print(\"c = a_t @ a is \\n\",\n",
    "      c.numpy())"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = a_t @ a is \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB0YBjW-9Au0"
   },
   "source": [
    "+ 反矩陣（matrix inverse）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8BEn-HfR9Au0",
    "outputId": "c9bc9484-df69-45cc-f2c7-895d70f39526",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "c_inv = torch.inverse(input = c)\n",
    "print(\"inverse of c is \\n\",\n",
    "      c_inv.numpy()) # c @ c_inv should be identity matrix\n",
    "print(\"check for inverse (left) \\n\",\n",
    "      (c_inv @ c).numpy())\n",
    "print(\"check for inverse (right) \\n\",\n",
    "      (c @ c_inv).numpy())"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse of c is \n",
      " [[ 2.33333333 -1.83333333]\n",
      " [-1.83333333  1.45833333]]\n",
      "check for inverse (left) \n",
      " [[ 1.00000000e+00 -1.42108547e-14]\n",
      " [ 0.00000000e+00  1.00000000e+00]]\n",
      "check for inverse (right) \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Zrzmva0W9Au3"
   },
   "source": [
    "+ Cholesky 拆解（Cholesky decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "CAHj1B_39Au4",
    "outputId": "e57b43ab-f887-48d2-f5c7-23ac970f1c7c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "c_chol = torch.cholesky(input = c)\n",
    "print(\"Cholesky factor of c is \\n\",\n",
    "      c_chol.numpy())\n",
    "print(\"check for Cholesky decomposition \\n\",\n",
    "      (c_chol @ torch.transpose(c_chol, 0 , 1)).numpy())"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky factor of c is \n",
      " [[5.91607978 0.        ]\n",
      " [7.43735744 0.82807867]]\n",
      "check for Cholesky decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xdec3fR9Au5"
   },
   "source": [
    "+ 特徵拆解（eigen-decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NXgb-fb99Au6",
    "outputId": "761d22a8-9244-43c4-9e17-4788242f5431",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "e, v = torch.symeig(input = c, eigenvectors=True)\n",
    "print(\"eigenvalue of c is \\n\",\n",
    "      e.numpy())\n",
    "print(\"eigenvector of c is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for eigen-decomposition \\n\",\n",
    "      (v @ torch.diag(e) @\n",
    "       torch.transpose(v, 0 , 1)).numpy())\n"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue of c is \n",
      " [ 0.26450509 90.73549491]\n",
      "eigenvector of c is \n",
      " [[-0.78489445  0.61962948]\n",
      " [ 0.61962948  0.78489445]]\n",
      "check for eigen-decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75Bo_Hef9Au8"
   },
   "source": [
    "+ 奇異值拆解（singular value decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "L_Wpt4Uq9Au8",
    "outputId": "bf7244a5-735d-4ab9-be28-637cb020d973",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "u, s, v = torch.svd(input = a)\n",
    "print(\"singular value of a is \\n\",\n",
    "      s.numpy())\n",
    "print(\"left singular vector of a is \\n\",\n",
    "      u.numpy())\n",
    "print(\"right singular vector of a is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for singular value decomposition \\n\",\n",
    "      (u @ torch.diag(s) @\n",
    "       torch.transpose(v, 0, 1)).numpy())\n"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singular value of a is \n",
      " [9.52551809 0.51430058]\n",
      "left singular vector of a is \n",
      " [[-0.2298477   0.88346102]\n",
      " [-0.52474482  0.24078249]\n",
      " [-0.81964194 -0.40189603]]\n",
      "right singular vector of a is \n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "check for singular value decomposition \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_A-Y8qaR9Au-"
   },
   "source": [
    "### 對張量之數值進行摘要\n",
    "`torch` 提供了一些化約（reduction）的函數，對張量內的數值進行摘要"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "np_L8ttn9Au-",
    "outputId": "3dfaa576-b106-4749-911e-abe1237635d9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"calculate mean \\n\",\n",
    "      torch.mean(input = a).numpy())\n",
    "print(\"calculate standard deviation \\n\",\n",
    "      torch.std(input = a).numpy())\n",
    "print(\"calculate max \\n\",\n",
    "      torch.max(input = a).numpy())\n",
    "print(\"calculate min \\n\",\n",
    "      torch.min(input = a).numpy())"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean \n",
      " 3.5\n",
      "calculate standard deviation \n",
      " 1.8708286933869707\n",
      "calculate max \n",
      " 6.0\n",
      "calculate min \n",
      " 1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8G19P0vK9AvA"
   },
   "source": [
    "我們亦可對張量的各面向，進行前述的摘要。以平均數為例："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EM8PxB6p9AvA",
    "outputId": "5f48f967-ee53-489a-f0c5-6289d17ffbd6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "source": [
    "print(\"calculate mean for each column \\n\",\n",
    "      torch.mean(input = a, dim=0).numpy())\n",
    "print(\"calculate mean for each row \\n\",\n",
    "      torch.mean(input = a, dim=1).numpy())"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean for each column \n",
      " [3. 4.]\n",
      "calculate mean for each row \n",
      " [1.5 3.5 5.5]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "izBk31if9AvD"
   },
   "source": [
    "其它的化約函數，可以參考[官方文件](https://pytorch.org/docs/stable/torch.html#reduction-ops)。\n",
    "\n",
    "\n",
    "## 張量運算之進階議題\n",
    "\n",
    "### 廣播\n",
    "在先前討論到兩張量進行元素對元素的加減乘除時，有一個條件是兩張量的尺寸必須相等。然而，此條件並非必要的。考慮以下的範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b is \n",
      " tensor([[ 2.,  4.],\n",
      "        [ 6.,  8.],\n",
      "        [10., 12.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3]],\n",
    "                 dtype = torch.float64)\n",
    "b = torch.tensor([2],\n",
    "                 dtype = torch.float64)\n",
    "print(\"a * b is \\n\", a * b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我們可以看到，雖然 `a` 和 `b` 的尺寸並不相同，但 `torch` 仍然會根據某種規則，將 `b` 的數值分配給 `a` 進行元素對元素的運算，這樣的特性被稱作廣播（broadcasting）。廣播的優點在於，其一方面可以處理不同尺寸張量的運算，二方面則是提供了高效率的計算。\n",
    "\n",
    "廣播的概念承襲自 `numpy` 套件，其詳細的運作機制可以參考 [Array Broadcasting in Numpy](https://numpy.org/doc/stable/user/theory.broadcasting.html) 一文。當以下條件滿足時，則 `torch` 可進行廣播：\n",
    "\n",
    "+ 兩張量從尾端軸（trailing axes）往前算回來的尺寸相同，或是其中一張量之維度必須是 1。\n",
    "\n",
    "以下之範例為尺寸 `(2, 3)` 與 `(3, )` 兩張量之乘法，由於兩張量從尾端軸算回來的維度都是 `(3, )`，故符合前述條件。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a is \n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n",
      "tensor b is \n",
      " tensor([0., 1., 2.], dtype=torch.float64)\n",
      "tensor a * b is \n",
      " tensor([[ 0.,  2.,  6.],\n",
      "        [ 0.,  5., 12.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]],\n",
    "                 dtype = torch.float64)\n",
    "b = torch.tensor([0, 1, 2],\n",
    "                 dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a)\n",
    "print(\"tensor b is \\n\", b)\n",
    "print(\"tensor a * b is \\n\", a * b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下之範例為尺寸 `(2, 3)` 與 `(2, 1)` 兩張量之乘法，其從尾端軸算回來的維度為相等或是其中一張量等於1，亦符合廣播條件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a is \n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n",
      "tensor b is \n",
      " tensor([[0.],\n",
      "        [1.]], dtype=torch.float64)\n",
      "tensor a * b is \n",
      " tensor([[0., 0., 0.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n",
      "a * b is \n",
      " tensor([[0., 0., 0.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]],\n",
    "                 dtype = torch.float64)\n",
    "b = torch.tensor([[0], [1]],\n",
    "                 dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a)\n",
    "print(\"tensor b is \\n\", b)\n",
    "print(\"tensor a * b is \\n\", a * b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "然而，若是尺寸為 `(2, 3)` 與 `(2, )` 之張量相乘的，則不符合廣播條件，會產生錯誤訊息。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU運算\n",
    "當執行程式碼之機器支援GPU運算時，`torch` 可利用GPU獲得高效能之運算。而機器是否具有GPU 支援，可透過 `torch.cuda.is_available()` 此指令進行判定。若使用 Google 的 Colab 服務，進行 GPU 運算需要點選 `Runtime\\Change runtime type`，在 `Hardware accelerator` 處點選 `GPU`，才會轉移到具有 GPU 之機器進行計算。\n",
    "\n",
    "為了瞭解 GPU 在計算上的幫助，我們撰寫了一個函數 `math_speed_test`，其內部生成一 `(n, n)` 之方陣 `a`，接著對其連續進行 `iter_max` 次的連加，並記錄完成整個工作所需的時間。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tMIh8FFX-CF2"
   },
   "source": [
    "import time\n",
    "def math_speed_test(n, iter_max, device):\n",
    "    a = torch.zeros((n, n), dtype = torch.float64).to(device)\n",
    "    start_time = time.time()\n",
    "    for _ in range(iter_max):\n",
    "        a += a\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(device.upper(), 'time =', elapsed_time)"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch` 進行 GPU 運算的關鍵步驟在於，利用 `.to(\"cuda\")` 方法將資料送到 GPU 上，而如果將指令改為 `.to(\"cpu\")`，則意味著我們將使用一般的 CPU 進行運算。\n",
    "\n",
    "接下來的程式碼，我們將 `iter_max` 設為 100，`n` 設為 `10`、`100`、以及`1000`，觀察計算時間的變化（注意，下述程式碼得在支援 GPU 之機器上執行，才可以看見 GPU 之計算時間，否則僅會呈現 CPU 的結果）："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "la-V5Bg99WfT",
    "outputId": "bb1fdb36-85dd-401c-ec2a-4d20c99426ed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    }
   },
   "source": [
    "iter_max = 100\n",
    "for n in [10, 100, 1000]:\n",
    "    print(\"n =\", n)\n",
    "    math_speed_test(n = n, iter_max = iter_max, device = \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        math_speed_test(n = n, iter_max = iter_max, device = \"cuda\")"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10\n",
      "CPU time = 0.0013108253479003906\n",
      "n = 100\n",
      "CPU time = 0.0004150867462158203\n",
      "n = 1000\n",
      "CPU time = 0.07234787940979004\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKvw1_YJBy4f"
   },
   "source": [
    "大體上，我們可以觀察到在 `n` 數值為 `10` 或 `100` 時，GPU並沒有顯著的幫助，甚至可能表現得更差。然而，在 `n = 1000`，GPU 可大幅提升計算的速度。\n",
    "\n",
    "除了數學運算外，隨機亂數之生成亦可透過 GPU 顯著的加速，可參考以下之程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10\n",
      "CPU time = 0.0016641616821289062\n",
      "n = 100\n",
      "CPU time = 0.00910329818725586\n",
      "n = 1000\n",
      "CPU time = 0.8795418739318848\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.FloatTensor(n, n).uniform_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %timeit torch.cuda.FloatTensor(n, n).uniform_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在這裡，我們使用了 `IPython` 的魔術指令 `%timeit` ，其可用於評估該行程式碼之計算時間。如果要評估的是整個程式塊（code block）的時間的話，可以改為使用 `%%timeit` 置於程式塊的開頭。\n",
    "\n",
    "另外，線性代數亦可獲得 GPU 之幫助"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit torch.FloatTensor(n, n).uniform_() @ torch.FloatTensor(n, n).uniform_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %timeit torch.cuda.FloatTensor(n, n).uniform_() @ torch.cuda.FloatTensor(n, n).uniform_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "然而，部分線性代數的計算則未必可以獲得 GPU 的強力幫助，如計算反矩陣"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit torch.FloatTensor(n, n).uniform_().inverse()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %timeit torch.cuda.FloatTensor(n, n).uniform_().inverse()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一個數學運算是否能夠獲得 GPU 幫助的關鍵在於，該運算是否能夠被平行化。如果讀者曾經有使用過高斯消去法（Gaussian elimination）來解反矩陣的話，就知道高斯消去法是個得序列求解的方法，故 GPU 幫助不大。\n",
    "\n",
    "\n",
    "在實務上，我們可以透過以下的程式碼來檢驗機器是否有可供使用的GPU，以動態決定 `device` 應為何："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hyUHfRLsBw0U",
    "outputId": "664fc501-4223-4d96-9d8a-49ecbbe2a22a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Tnt7SwLY9AvF"
   },
   "source": [
    "## 實作範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNZXr9v_9AvF"
   },
   "source": [
    "### 產生線性迴歸資料\n",
    "\n",
    "在開始之前，我們先設定一種子，以讓後續的亂數生成都能夠獲得相同的結果（不過，這裡的 `torch.manual_seed` 僅適用於 CPU 之張量，若使用GPU，請改為 `torch.cuda.manual_seed`）。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nNs8m0DQ9AvG",
    "outputId": "89186214-e662-441e-c374-589a55db311a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "torch.manual_seed(48)"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f94b076c0b0>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "W4r2rdyO9AvH"
   },
   "source": [
    "# define a function to generate x and y\n",
    "def generate_data(n_sample, weight,\n",
    "                  bias = 0,\n",
    "                  std_residual = 1,\n",
    "                  mean_feature = 0,\n",
    "                  std_feature = 1,\n",
    "                  dtype = torch.float64):\n",
    "    weight = torch.tensor(weight, dtype = dtype)\n",
    "    n_feature = weight.shape[0]\n",
    "    x = torch.normal(mean = mean_feature,\n",
    "                     std = std_feature,\n",
    "                     size = (n_sample, n_feature),\n",
    "                     dtype = dtype)\n",
    "    e = torch.normal(mean = 0,\n",
    "                     std = std_residual,\n",
    "                     size = (n_sample, 1),\n",
    "                     dtype = dtype)\n",
    "    weight = weight.view(size = (-1, 1))\n",
    "    y = bias + x @ weight + e\n",
    "    return x, y"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JnbkE3TG9AvJ",
    "outputId": "e41d20f4-0def-450f-a8c1-e983096b727d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    }
   },
   "source": [
    "# run generate_data\n",
    "x, y = generate_data(n_sample = 10,\n",
    "                     weight = [-5, 3, 0],\n",
    "                     bias = 5,\n",
    "                     std_residual = 1,\n",
    "                     mean_feature = 10,\n",
    "                     std_feature = 3,\n",
    "                     dtype = torch.float64)\n",
    "print(\"feature matrix x is \\n\", x.numpy())\n",
    "print(\"response vector y is \\n\", y.numpy())"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix x is \n",
      " [[10.86425232  2.85930239 12.99748957]\n",
      " [ 7.98709525  9.79137811 12.98202577]\n",
      " [ 8.40585824 13.31645195  4.51125723]\n",
      " [10.52843635 10.28388618  8.43102705]\n",
      " [11.25624976 12.04005459 12.59301479]\n",
      " [12.31145327  9.15153143  7.96220487]\n",
      " [11.49807312 14.63609048  6.82995236]\n",
      " [11.7234386   7.16098399  9.8980089 ]\n",
      " [ 7.24735628  7.5570991   5.9154331 ]\n",
      " [11.05035931 10.47430089  4.86376669]]\n",
      "response vector y is \n",
      " [[-42.98404441]\n",
      " [ -4.94309144]\n",
      " [  0.31840093]\n",
      " [-16.21407503]\n",
      " [-13.77391657]\n",
      " [-31.18360077]\n",
      " [ -8.4871242 ]\n",
      " [-31.53590804]\n",
      " [ -7.8563037 ]\n",
      " [-19.27007283]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "l0pp8vaM9AvL"
   },
   "source": [
    "### 計算模型參數"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dGm_73SA9AvM"
   },
   "source": [
    "# define a function to calculate model parameter\n",
    "def calculate_parameter(x, y, dtype = torch.float64):\n",
    "    if x.dtype is not dtype:\n",
    "        x = x.type(dtype = dtype)\n",
    "    if y.dtype is not dtype:\n",
    "        y = y.type(dtype = dtype)\n",
    "    u = torch.ones(size = (x.size()[0], 1), dtype = dtype)\n",
    "    x_design = torch.cat([u, x], dim = 1)\n",
    "    parameter = torch.inverse(\n",
    "        torch.transpose(x_design, dim0=0, dim1=1) @ x_design) @ \\\n",
    "                torch.transpose(x_design, dim0=0, dim1=1) @ y\n",
    "    bias = parameter[0, 0]\n",
    "    weight = parameter[1:, 0]\n",
    "    return bias, weight"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Zvz2eyco9AvO",
    "outputId": "b4bf90fd-b56a-4f3d-f2d1-511023faef4f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    }
   },
   "source": [
    "# run calculate_parameter\n",
    "x, y = generate_data(n_sample = 1000,\n",
    "                     weight = [-5, 3, 0],\n",
    "                     bias = 5,\n",
    "                     std_residual = 1,\n",
    "                     mean_feature = 10,\n",
    "                     std_feature = 3,\n",
    "                     dtype = torch.float64)\n",
    "bias, weight = calculate_parameter(x, y)\n",
    "print(\"bias estimate is \\n\", bias.numpy())\n",
    "print(\"weight estimate is \\n\", weight.numpy())"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias estimate is \n",
      " 4.996612423766793\n",
      "weight estimate is \n",
      " [-4.98003037  2.98762589 -0.00737968]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "RWWrlUkN9AvQ"
   },
   "source": [
    "### 建立一進行迴歸分析之物件"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UMKYz0B39AvQ"
   },
   "source": [
    "# define a class to fit linear regression\n",
    "class LinearRegression():\n",
    "    def __init__(self, dtype = torch.float64):\n",
    "        self.dtype = dtype\n",
    "        self.bias = None\n",
    "        self.weight = None\n",
    "    def fit(self, x, y):\n",
    "        if x.dtype is not self.dtype:\n",
    "            x = x.type(dtype = self.dtype)\n",
    "        if y.dtype is not self.dtype:\n",
    "            y = y.type(dtype = self.dtype)\n",
    "        u = torch.ones(size = (x.size()[0], 1), dtype = self.dtype)\n",
    "        x_design = torch.cat([u, x], dim = 1)\n",
    "        parameter = torch.inverse(\n",
    "            torch.transpose(x_design, dim0=0, dim1=1) @ x_design) @ \\\n",
    "                    torch.transpose(x_design, dim0=0, dim1=1) @ y\n",
    "        self.bias = parameter[0, 0]\n",
    "        self.weight = parameter[1:, 0]\n",
    "        return self"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bIyrEE-C9AvS",
    "outputId": "bdc29e8e-fe7a-44aa-d41a-39dcc2ca6c21",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    }
   },
   "source": [
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x, y)\n",
    "print(\"bias estimate is \\n\", model_lr.bias.numpy())\n",
    "print(\"weight estimate is \\n\", model_lr.weight.numpy())"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias estimate is \n",
      " 4.996612423766793\n",
      "weight estimate is \n",
      " [-4.98003037  2.98762589 -0.00737968]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 作業\n",
    "\n",
    "1. 請根據本章節的內容，進行一系列的實驗，以了解在哪些運算與張量尺寸上，GPU 計算才具有其優勢。\n",
    "\n",
    "2. 請在 `LinearRegression` 此類型中，加入以下新的功能：\n",
    "+ 新增一個選項，以決定 `x` 是否需要進行標準化（每個變項的平均數為0，變異數為1，不准使用其它的套件）\n",
    "+ 新增一個選項，以決定是否使用 GPU 進行運算。\n",
    "+ 新增一方法 `predict()`，其輸入為一新的 `x`，輸出為在該 `x` 下，`y` 的預測值。\n",
    "+ 新增一方法 `gradient()`，其可獲得在當前係數估計下，最小平方估計準則的梯度數值。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}