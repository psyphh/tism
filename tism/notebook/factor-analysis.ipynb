{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "factor-analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MzRkehf7AMc2"
      },
      "source": [
        "因素分析\n",
        "================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvEAeNUA1bG"
      },
      "source": [
        "\n",
        "## 因素分析模型\n",
        "\n",
        "### 模型架構\n",
        "令 $x_i$ 表示個體於第 $i$ 個測驗（或是試題）的觀測分數（observed score）（$i=1,2,...,I$），因素分析（factor analysis）試圖引入 $M$ 個潛在因素（latent factor）$\\eta_1, \\eta_2,...,\\eta_M$，以解釋 $x_i$ 之變異\n",
        "\n",
        "$$\n",
        "x_i = \\nu_i + \\sum_{m=1}^M \\lambda_{im} \\eta_m + \\epsilon_i\n",
        "$$\n",
        "\n",
        "這裡，$\\eta_m$ 表示第 $m$ 個潛在因素，其對 $x_i$ 之效果 $\\lambda_{im}$ 被稱作因素負荷量（factor loading），其反映 $\\eta_m$ 每變動一單位，預期 $x_i$ 跟著變動的量，$\\nu_i$ 為試題 $i$ 之截距，其反映當所有 $\\eta_m = 0$時，$x_i$ 的預期數值，而 $\\epsilon_i$ 則為試題 $i$ 所對應之測量誤差。\n",
        "\n",
        "\n",
        "因素分析模型假設\n",
        "\n",
        "1. 潛在因素 $\\eta_m$ 與誤差分數 $\\epsilon_i$ 為統計獨立。\n",
        "2. $\\eta_m \\sim (0, 1)$，$\\mathbb{C} \\text{ov}(\\eta_m, \\eta_{m'}) = \\phi_{mm'}$。當所有 $\\phi_{mm'}=0$（$m \\neq m'$）時，我們稱此因素結構為正交結構（orthogonal structure）。\n",
        "3. $\\epsilon_i \\sim (0, \\psi^2_i)$，$\\mathbb{C} \\text{ov}(\\epsilon_i, \\epsilon_{i'}) =  \\psi_{ii'}$。多數情況下，模型假設$\\psi_{ii'} = 0$（$i \\neq i'$）。\n",
        "\n",
        "\n",
        "在平均數與共變異數結構方面，當誤差分數間無相關的假設下，該結構為\n",
        "\n",
        "1. $\\mu_i(\\theta) = \\nu_i$\n",
        "2. $\\sigma_{i}^2(\\theta) = \\sum_{m=1}^M \\lambda_{im}^2 + \\psi_{i}^2$。\n",
        "3. $\\sigma_{ij}(\\theta) = \\sum_{m=1}^M \\sum_{k=1}^M \\lambda_{im}\\lambda_{jk} \\phi_{mk}$。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 矩陣形式之模型架構\n",
        "若我們將 $\\eta_1, \\eta_2,...,\\eta_M$ 與 $\\lambda_{i1}, \\lambda_{i2},...,\\lambda_{iM}$ 皆排成 $M$ 維之向量，即 $\\eta = (\\eta_1, \\eta_2,...,\\eta_M)$ 與 $\\lambda_i = (\\lambda_{i1}, \\lambda_{i2},...,\\lambda_{iM})$，則前述之方程式可以寫為\n",
        "\n",
        "$$\n",
        "x_i = \\nu_i + \\lambda_{i}^T \\eta + \\epsilon_i\n",
        "$$\n",
        "\n",
        "進一步，令 $x = (x_1, x_2, ..., x_I)$，$\\nu = (\\nu_1, \\nu_2, ..., \\nu_I)$，以及 $\\epsilon = (\\epsilon_1, \\epsilon_2, ..., \\epsilon_I)$ 皆表示一 $I \\times 1$ 矩陣，而\n",
        "\n",
        "$$\n",
        "\\Lambda =\n",
        "\\underbrace{\\begin{pmatrix}\n",
        "\\lambda_1^T \\\\\n",
        "\\lambda_2^T \\\\\n",
        "\\vdots \\\\\n",
        "\\lambda_I^T\n",
        "\\end{pmatrix}}_{I \\times M}\n",
        "=\n",
        "\\underbrace{\\begin{pmatrix}\n",
        "\\lambda_{11} & \\lambda_{12} & \\cdots & \\lambda_{1M} \\\\\n",
        "\\lambda_{21} & \\lambda_{22} & \\cdots & \\lambda_{2M} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\lambda_{I1} & \\lambda_{I2} & \\cdots & \\lambda_{IM} \\\\\n",
        "\\end{pmatrix}}_{I \\times M}\n",
        "$$\n",
        "\n",
        "\n",
        "在前述之符號表示下，觀察變項向量 $x$ 可以被寫為\n",
        "\n",
        "$$\n",
        "x = \\nu + \\Lambda \\eta + \\epsilon\n",
        "$$\n",
        "\n",
        "我們可以將前述因素分析模型之假設，轉為矩陣之形式：（1）$\\eta$ 與 $\\epsilon$ 兩者獨立；（2）$\\eta \\sim (0, \\Phi)$ ；（3）$\\epsilon \\sim (0, \\Psi)$。此時，平均數與共變異數結構可以寫為\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu(\\theta) &= \\nu \\\\\n",
        "\\Sigma(\\theta) &= \\Lambda \\Phi \\Lambda^T + \\Psi\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "而在正交因素結構下，前述之共變異數結構可以簡化為\n",
        "\n",
        "$$\n",
        "\\Sigma(\\theta) = \\Lambda \\Lambda^T + \\Psi\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "die-ftqeA6a5"
      },
      "source": [
        "\n",
        "## 參數估計\n",
        "\n",
        "### 轉軸不定性\n",
        "前述之因素分析模型因著轉軸不定性（rotational indeterminancy），並無法獲得唯一的參數解。\n",
        "\n",
        "以正交模型為例，令 $Q$ 表示一 $M \\times M$ 之正交矩陣（orthogonal matrix），即 $Q$ 滿足 $Q Q^T = Q^T Q = I$（$Q^T$ 為 $Q$ 之反矩陣），則\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\Sigma(\\theta) &= \\Lambda Q Q^T \\Lambda^T + \\Psi \\\\\n",
        "&= \\Lambda^* {\\Lambda^*}^T + \\Psi \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "如果不限制 $Q$ 為正交矩陣，僅假設：（1）$Q$ 為對稱矩陣；（2）$Q^{-1}$ 存在；（3）$Q^{-1} {Q^{-1}}^T$ 為相關係數矩陣（即對角線為1），則\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\Sigma(\\theta) &= \\Lambda Q Q^{-1} {Q^{-1}}^T Q^T \\Lambda^T + \\Psi \\\\\n",
        "&= \\Lambda^* \\Phi^* {\\Lambda^*}^T + \\Psi \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "因此，只要給予了一組參數解，我們即可透過 $Q$ 獲得另一組參數解，且模型適配度與原先的解相同。\n",
        "\n",
        "傳統上，有兩種取向可獲得因素分析之唯一參數解：\n",
        "\n",
        "1. 探索性因素分析（exploratory factor analysis）利用轉軸以獲得一最精簡之因素負荷量矩陣以移除轉軸不確定性。 \n",
        "2. 驗證性因素分析（confirmatory factor analysis）將部分的因素負荷量設為 0 以移除轉軸不確定性。\n",
        "\n",
        "\n",
        "### 最小平方法\n",
        "給定一樣本共變異數矩陣 $S$，其第 $i,j$ 元素為 $s_{ij}$，則一般最小平方（ordinal least squares，簡稱OLS）法透過最小化以下準則以獲得模型參數之估計\n",
        "\n",
        "$$\n",
        "\\mathcal{D}_{OLS}(\\theta) =\\sum_{i=j}^I \\sum_{j=1}^I (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
        "$$\n",
        "\n",
        "一個與最小平方法有關的變形為最小殘差（minimum residual，簡稱MINRES）法，其僅考慮共變異數之非對角線元素進行估計\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathcal{D}_{MINRES}(\\theta) =\\sum_{i=j+1}^I \\sum_{j=1}^{I-1} (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
        "$$\n",
        "\n",
        "當所有的 $\\psi_i^2$ 皆可被自由估計時，MINRES與OLS兩者為等價的。\n",
        "\n",
        "前述的OLS法可以進一步引入權重，即成為加權最小平方法（weighted least squares，簡稱WLS），其估計準則改為\n",
        "\n",
        "$$\n",
        "\\mathcal{D}_{WLS}(\\theta) =\\sum_{i=j}^I \\sum_{j=1}^I w_{ij} (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
        "$$\n",
        "\n",
        "這裡，$w_{ij}$ 表示對 $(s_{ij} - \\sigma_{ij}(\\theta))$ 此殘差給予的權重，其並非模型之參數，乃研究者於估計準則中給定的，當 $w_{ij}$ 越大，即表示研究者希望 $(s_{ij} - \\sigma_{ij}(\\theta))$ 之差異應越小越好。\n",
        "\n",
        "### 最大概似法\n",
        "\n",
        "在因素分析的模型假設下，$x$ 之平均數與共變異數為 $\\mu(\\theta)$ 與 $\\Sigma(\\theta))$，若再進一步引進多元常態分配之假設，則 $x \\sim \\text{Normal}(\\mu(\\theta), \\Sigma(\\theta))$，此時，$x$ 之對數機率密度函數為\n",
        "\n",
        "$$\n",
        "\\log f(x|\\theta) = -\\frac{I}{2} \\log{2\\pi} - \\frac{1}{2} |\\Sigma(\\theta)| - \\frac{1}{2} (x - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (x - \\mu(\\theta))\n",
        "$$\n",
        "\n",
        "因此，給定樣本資料 $x_1, x_2,...,x_N$下，最大概似估計準則可以寫為\n",
        "\n",
        "$$\n",
        "\\ell(\\theta) = C  -\\frac{N}{2} |\\Sigma(\\theta)| - \\frac{1}{2} \\sum_{n=1}^N (x_n - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (x_n - \\mu(\\theta))\n",
        "$$\n",
        "\n",
        "前述之最大概似準則可以簡化為\n",
        "\n",
        "$$\n",
        "\\ell(\\theta) = C  - \\frac{N}{2} |\\Sigma(\\theta)| - \\frac{N}{2} tr(\\Sigma(\\theta) ^{-1} S) - \\frac{N}{2} (m - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (m - \\mu(\\theta))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UXhdEJgA9bE"
      },
      "source": [
        "## 期望最大化算則\n",
        "\n",
        "期望最大化算則（expectation-maximization algorithm，簡稱EM算則）常用於處理不完整資料（incomplete data）的最大概似估計問題。在心理計量領域，EM算則將潛在變項視為不完整之資料，"
      ]
    }
  ]
}