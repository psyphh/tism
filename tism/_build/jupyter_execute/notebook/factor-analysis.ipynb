{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "因素分析\n",
    "================================\n",
    "\n",
    "\n",
    "## 因素分析模型\n",
    "\n",
    "### 模型架構\n",
    "令 $x_i$ 表示個體於第 $i$ 個測驗（或是試題）的觀測分數（observed score）（$i=1,2,...,I$），因素分析（factor analysis）試圖引入 $M$ 個潛在因素（latent factor）$\\eta_1, \\eta_2,...,\\eta_M$，以解釋 $x_i$ 之變異\n",
    "\n",
    "$$\n",
    "x_i = \\nu_i + \\sum_{m=1}^M \\lambda_{im} \\eta_m + \\epsilon_i\n",
    "$$\n",
    "\n",
    "這裡，$\\eta_m$ 表示第 $m$ 個潛在因素，其對 $x_i$ 之效果 $\\lambda_{im}$ 被稱作因素負荷量（factor loading），其反映 $\\eta_m$ 每變動一單位，預期 $x_i$ 跟著變動的量，$\\nu_i$ 為試題 $i$ 之截距，其反映當所有 $\\eta_m = 0$時，$x_i$ 的預期數值，而 $\\epsilon_i$ 則為試題 $i$ 所對應之測量誤差。\n",
    "\n",
    "\n",
    "因素分析模型假設\n",
    "\n",
    "1. 潛在因素 $\\eta_m$ 與誤差分數 $\\epsilon_i$ 為統計獨立。\n",
    "2. $\\eta_m \\sim (0, 1)$，$\\mathbb{C} \\text{ov}(\\eta_m, \\eta_{m'}) = \\phi_{mm'}$。當所有 $\\phi_{mm'}=0$（$m \\neq m'$）時，我們稱此因素結構為正交結構（orthogonal structure）。\n",
    "3. $\\epsilon_i \\sim (0, \\psi^2_i)$，$\\mathbb{C} \\text{ov}(\\epsilon_i, \\epsilon_{i'}) =  \\psi_{ii'}$。多數情況下，模型假設$\\psi_{ii'} = 0$（$i \\neq i'$）。\n",
    "\n",
    "\n",
    "在平均數與共變異數結構方面，當誤差分數間無相關的假設下，該結構為\n",
    "\n",
    "1. $\\mu_i(\\theta) = \\nu_i$\n",
    "2. $\\sigma_{i}^2(\\theta) = \\sum_{m=1}^M \\lambda_{im}^2 + \\psi_{i}^2$。\n",
    "3. $\\sigma_{ij}(\\theta) = \\sum_{m=1}^M \\sum_{k=1}^M \\lambda_{im}\\lambda_{jk} \\phi_{mk}$。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 矩陣形式之模型架構\n",
    "若我們將 $\\eta_1, \\eta_2,...,\\eta_M$ 與 $\\lambda_{i1}, \\lambda_{i2},...,\\lambda_{iM}$ 皆排成 $M$ 維之向量，即 $\\eta = (\\eta_1, \\eta_2,...,\\eta_M)$ 與 $\\lambda_i = (\\lambda_{i1}, \\lambda_{i2},...,\\lambda_{iM})$，則前述之方程式可以寫為\n",
    "\n",
    "$$\n",
    "x_i = \\nu_i + \\lambda_{i}^T \\eta + \\epsilon_i\n",
    "$$\n",
    "\n",
    "進一步，令 $x = (x_1, x_2, ..., x_I)$，$\\nu = (\\nu_1, \\nu_2, ..., \\nu_I)$，以及 $\\epsilon = (\\epsilon_1, \\epsilon_2, ..., \\epsilon_I)$ 皆表示一 $I \\times 1$ 矩陣，而\n",
    "\n",
    "$$\n",
    "\\Lambda =\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\lambda_1^T \\\\\n",
    "\\lambda_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_I^T\n",
    "\\end{pmatrix}}_{I \\times M}\n",
    "=\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\lambda_{11} & \\lambda_{12} & \\cdots & \\lambda_{1M} \\\\\n",
    "\\lambda_{21} & \\lambda_{22} & \\cdots & \\lambda_{2M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\lambda_{I1} & \\lambda_{I2} & \\cdots & \\lambda_{IM} \\\\\n",
    "\\end{pmatrix}}_{I \\times M}\n",
    "$$\n",
    "\n",
    "\n",
    "在前述之符號表示下，觀察變項向量 $x$ 可以被寫為\n",
    "\n",
    "$$\n",
    "x = \\nu + \\Lambda \\eta + \\epsilon\n",
    "$$\n",
    "\n",
    "我們可以將前述因素分析模型之假設，轉為矩陣之形式：（1）$\\eta$ 與 $\\epsilon$ 兩者獨立；（2）$\\eta \\sim (0, \\Phi)$ ；（3）$\\epsilon \\sim (0, \\Psi)$。此時，平均數與共變異數結構可以寫為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu(\\theta) &= \\nu \\\\\n",
    "\\Sigma(\\theta) &= \\Lambda \\Phi \\Lambda^T + \\Psi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 轉軸不定性\n",
    "前述之因素分析模型因著轉軸不定性（rotational indeterminancy），並無法獲得唯一的參數解。\n",
    "\n",
    "以正交模型為例，令 $Q$ 表示一 $M \\times M$ 之正交矩陣（orthogonal matrix），即 $Q$ 滿足 $Q Q^T = Q^T Q = I$（$Q^T$ 為 $Q$ 之反矩陣），則\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma(\\theta) &= \\Lambda Q Q^T \\Lambda^T + \\Psi \\\\\n",
    "&= \\Lambda^* {\\Lambda^*}^T + \\Psi \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "如果不限制 $Q$ 為正交矩陣，僅假設：（1）$Q$ 為對稱矩陣；（2）$Q^{-1}$ 存在；（3）$Q^{-1} {Q^{-1}}^T$ 為相關係數矩陣（即對角線為1），則\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma(\\theta) &= \\Lambda Q Q^{-1} {Q^{-1}}^T Q^T \\Lambda^T + \\Psi \\\\\n",
    "&= \\Lambda^* \\Phi^* {\\Lambda^*}^T + \\Psi \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "因此，只要給予了一組參數解，我們即可透過 $Q$ 獲得另一組參數解，且模型適配度與原先的解相同。\n",
    "\n",
    "傳統上，有兩種取向可獲得因素分析之唯一參數解：\n",
    "\n",
    "1. 探索性因素分析（exploratory factor analysis）利用轉軸以獲得一最精簡之因素負荷量矩陣以移除轉軸不確定性。\n",
    "2. 驗證性因素分析（confirmatory factor analysis）將部分的因素負荷量設為 0 以移除轉軸不確定性。\n",
    "\n",
    "## 參數估計\n",
    "\n",
    "\n",
    "\n",
    "### 最小平方法\n",
    "給定一樣本共變異數矩陣 $S$，其第 $i,j$ 元素為 $s_{ij}$，則一般最小平方（ordinal least squares，簡稱OLS）法透過最小化以下準則以獲得模型參數之估計\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{OLS}(\\theta) =\\sum_{i=j}^I \\sum_{j=1}^I (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
    "$$\n",
    "\n",
    "一個與最小平方法有關的變形為最小殘差（minimum residual，簡稱MINRES）法，其僅考慮共變異數之非對角線元素進行估計\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{MINRES}(\\theta) =\\sum_{i=j+1}^I \\sum_{j=1}^{I-1} (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
    "$$\n",
    "\n",
    "當所有的 $\\psi_i^2$ 皆可被自由估計時，MINRES與OLS兩者為等價的。\n",
    "\n",
    "前述的OLS法可以進一步引入權重，即成為加權最小平方法（weighted least squares，簡稱WLS），其估計準則改為\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{WLS}(\\theta) =\\sum_{i=j}^I \\sum_{j=1}^I w_{ij} (s_{ij} - \\sigma_{ij}(\\theta))^2.\n",
    "$$\n",
    "\n",
    "這裡，$w_{ij}$ 表示對 $(s_{ij} - \\sigma_{ij}(\\theta))$ 此殘差給予的權重，其並非模型之參數，乃研究者於估計準則中給定的，當 $w_{ij}$ 越大，即表示研究者希望 $(s_{ij} - \\sigma_{ij}(\\theta))$ 之差異應越小越好。\n",
    "\n",
    "### 最大概似法\n",
    "\n",
    "在因素分析的模型假設下，$x$ 之平均數與共變異數為 $\\mu(\\theta)$ 與 $\\Sigma(\\theta))$，若再進一步引進多元常態分配之假設，則 $x \\sim \\text{Normal}(\\mu(\\theta), \\Sigma(\\theta))$，此時，$x$ 之對數機率密度函數為\n",
    "\n",
    "$$\n",
    "\\log f(x;\\theta) = -\\frac{I}{2} \\log{2\\pi} - \\frac{1}{2} \\log |\\Sigma(\\theta)| - \\frac{1}{2} (x - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (x - \\mu(\\theta))\n",
    "$$\n",
    "\n",
    "因此，給定樣本資料 $x_1, x_2,...,x_N$下，最大概似估計準則可以寫為\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = C  -\\frac{N}{2} \\log |\\Sigma(\\theta)| - \\frac{1}{2} \\sum_{n=1}^N (x_n - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (x_n - \\mu(\\theta))\n",
    "$$\n",
    "\n",
    "前述之最大概似準則可以簡化為\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = C  - \\frac{N}{2} \\log |\\Sigma(\\theta)| - \\frac{N}{2} tr(\\Sigma(\\theta) ^{-1} S) - \\frac{N}{2} (m - \\mu(\\theta))^T \\Sigma(\\theta) ^{-1} (m - \\mu(\\theta))\n",
    "$$\n",
    "\n",
    "### 期望最大化算則\n",
    "\n",
    "期望最大化算則（expectation-maximization algorithm，簡稱EM算則）常用於處理不完整資料（incomplete data）的最大概似估計問題。在心理計量領域，潛在因素可被視為不完整資料，因此，EM算則可用於處理心理計量模型之估計問題。\n",
    "\n",
    "在因素分析的問題上，若 $\\eta$ 可以直接被觀察，則我們可以考慮以下的完整資料之對數機率密度函數\n",
    "\n",
    "$$\n",
    "\\log f(x,\\eta;\\theta) = \\log f(x|\\eta; \\theta) + \\log f(\\eta; \\theta)\n",
    "$$\n",
    "\n",
    "若我們假設測量誤差 $\\epsilon_i$ 為常態分配，且 $\\epsilon_i$ 與 $\\epsilon_j$ 獨立（$i \\neq j$），則在給定 $\\eta$ 之下，我們有\n",
    "\n",
    "1. $x_i|\\eta \\sim \\text{Normal}(\\nu_i + \\lambda_i^T \\eta, \\psi_i^2)$\n",
    "2. 給定 $\\eta$ 之下，$x_i$ 與 $x_j$ 為獨立。\n",
    "\n",
    "因此，$\\log f(x|\\eta; \\theta)$ 可以寫為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log f(x|\\eta; \\theta) &= \\sum_{i=1}^I \\log f(x_i|\\eta; \\theta)\\\\\n",
    "&= \\sum_{i=1}^I \\left[\n",
    "C -\\frac{1}{2} \\log \\psi_i^2\n",
    " -\\frac{1}{2 \\psi_i^2} (x_i - \\nu_i - \\lambda_i^T \\eta)^2\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "這意味著，在 $\\eta$ 可直接觀察到的情況下，估計因素負荷量與截距，只是一線性回歸的問題。\n",
    "\n",
    "在 $\\eta$ 服從多元常態分配的假設下，$\\log f(\\eta; \\theta)$ 可以寫為\n",
    "\n",
    "$$\n",
    "\\log f(\\eta;\\theta) = C- \\frac{1}{2} \\log |\\Phi| - \\frac{1}{2} \\eta^T \\Phi^{-1} \\eta\n",
    "$$\n",
    "\n",
    "若我們進一步假設 $\\eta$ 為正交結構，此時，$\\Phi$ 為單位矩陣，我們甚至不用對 $\\eta$ 的分配參數進行估計。\n",
    "\n",
    "\n",
    "給定樣本資料 $(x_1, \\eta_1), (x_2, \\eta_2), ..., (x_N, \\eta_N)$，完整資料的概似函數可以寫為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell_{\\text{comp}}(\\theta) &= \\sum_{n=1}^N \\log f(x_n, \\eta_n; \\theta) \\\\\n",
    "&=\n",
    "C +\n",
    "\\sum_{n=1}^N\n",
    "\\sum_{i=1}^I\n",
    "\\left[\n",
    "-\\frac{1}{2} \\log \\psi_i^2\n",
    " -\\frac{1}{2 \\psi_i^2} (x_{ni} - \\nu_i - \\lambda_i^T \\eta_n)^2\n",
    "\\right]\n",
    "+\n",
    "\\sum_{n=1}^N\n",
    "\\left[\n",
    "- \\frac{1}{2} \\log |\\Phi| - \\frac{1}{2} \\eta_n^T \\Phi^{-1} \\eta_n\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "EM算則牽涉到期望步驟（E-Step）與最大化步驟（M-Step）。在E-Step時，我們計算在給定觀察資料 $\\mathcal{X}={x_n}_{n=1}^N$，以及當下參數估計值 $\\widehat{\\theta}^{(t)}$，完整資料概似函數之條件期望值，即\n",
    "\n",
    "$$\n",
    "Q(\\theta|\\widehat{\\theta}^{(t)}) = \\mathbb{E}\\left[ \\ell_{\\text{comp}}(\\theta) | \\mathcal{X}; \\widehat{\\theta}^{(t)} \\right]\n",
    "$$\n",
    "\n",
    "而在M-Step時，我們則試圖找到一$\\widehat{\\theta}^{(t+1)}$，其可最大化 $Q(\\theta|\\widehat{\\theta}^{(t)})$，即\n",
    "\n",
    "$$\n",
    "\\widehat{\\theta}^{(t+1)} =\\text{argmax}_{\\theta} \\ Q(\\theta|\\widehat{\\theta}^{(t)})\n",
    "$$\n",
    "\n",
    "\n",
    "在因素分析的EM算則中，E-Step的關鍵在於，要能夠計算在給定 $x$ 之下，$\\eta$ 的分佈特性。當 $\\eta$ 與 $\\epsilon$ 皆為常態分配時，$x$ 與 $\\eta$ 亦服從常態分配，其平均數與變異數為\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "\\eta\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\text{Normal}\n",
    "\\left[\n",
    "\\begin{pmatrix}\n",
    "\\nu \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\begin{pmatrix}\n",
    "\\Lambda \\Phi \\Lambda^T + \\Psi & \\Lambda \\Phi \\\\\n",
    " \\Phi \\Lambda^T & \\Phi\n",
    "\\end{pmatrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "接著，利用多元常態分配條件分配之特性，我們可以得\n",
    "\n",
    "$$\n",
    "\\eta| x \\sim \\text{Normal}\n",
    "\\left[\n",
    "\\Phi \\Lambda^T \\Sigma(\\theta)^{-1}(x - \\nu),\n",
    "\\Phi - \\Phi \\Lambda^T \\Sigma(\\theta)^{-1} \\Lambda \\Phi\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "這裡，$\\Sigma(\\theta) = \\Lambda \\Phi \\Lambda^T + \\Psi$ 即為因素分析之共變結構。\n",
    "\n",
    "\n",
    "## 程式範例\n",
    "\n",
    "### 產生因素分析資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:02.800357Z",
     "iopub.status.busy": "2020-12-15T07:16:02.799285Z",
     "iopub.status.idle": "2020-12-15T07:16:03.154088Z",
     "shell.execute_reply": "2020-12-15T07:16:03.154699Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_fa_model(n_factor, n_item, ld, psi = None, rho = None):\n",
    "    if (n_item % n_factor) != 0:\n",
    "        n_item = n_factor * (n_item // n_factor)\n",
    "    loading = torch.zeros((n_item, n_factor))\n",
    "    item_per_factor = (n_item // n_factor)\n",
    "    for i in range(n_factor):\n",
    "        for j in range(i * item_per_factor,\n",
    "                       (i + 1) * item_per_factor):\n",
    "            loading[j, i] = ld\n",
    "    if rho is None:\n",
    "        cor = torch.eye(n_factor)\n",
    "    else:\n",
    "        unit = torch.ones((n_factor, 1))\n",
    "        identity = torch.eye(n_factor)\n",
    "        cor = rho * (unit @ unit.T) + (1 - rho) * identity\n",
    "    if psi is None:\n",
    "        uniqueness = 1 - torch.diag(loading @ cor @ loading.T)\n",
    "    else:\n",
    "        uniqueness = psi * torch.ones((n_item, ))\n",
    "    return loading, uniqueness, cor\n",
    "\n",
    "def generate_fa_data(n_sample, loading, uniqueness, cor):\n",
    "    n_item = loading.size()[0]\n",
    "    mean = torch.zeros((n_item, ))\n",
    "    cov = loading @ cor @ loading.T + torch.diag(uniqueness)\n",
    "    mvn = torch.distributions.MultivariateNormal(\n",
    "        loc = mean, scale_tril = torch.cholesky(cov))\n",
    "    data = mvn.sample((n_sample,))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:03.158919Z",
     "iopub.status.busy": "2020-12-15T07:16:03.158360Z",
     "iopub.status.idle": "2020-12-15T07:16:03.165214Z",
     "shell.execute_reply": "2020-12-15T07:16:03.165762Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(246437)\n",
    "loading_true, uniqueness_true, cor_true = create_fa_model(n_factor = 4, n_item = 12, ld = .7)\n",
    "data = generate_fa_data(n_sample = 10000,\n",
    "                        loading = loading_true,\n",
    "                        uniqueness = uniqueness_true,\n",
    "                        cor = cor_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 梯度下降法求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:03.172095Z",
     "iopub.status.busy": "2020-12-15T07:16:03.171365Z",
     "iopub.status.idle": "2020-12-15T07:16:03.671704Z",
     "shell.execute_reply": "2020-12-15T07:16:03.672175Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-156878.2031, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.6415, -0.1556, -0.0308, -0.2295],\n",
      "        [ 0.6335, -0.1715, -0.0402, -0.2255],\n",
      "        [ 0.6439, -0.1686, -0.0293, -0.2139],\n",
      "        [ 0.1495,  0.6905,  0.0190, -0.0976],\n",
      "        [ 0.1363,  0.6855,  0.0191, -0.0832],\n",
      "        [ 0.1510,  0.6881,  0.0054, -0.0816],\n",
      "        [ 0.1802,  0.0225,  0.5769,  0.3776],\n",
      "        [ 0.1713,  0.0067,  0.5490,  0.3713],\n",
      "        [ 0.1773, -0.0078,  0.5651,  0.3506],\n",
      "        [ 0.1925,  0.0195, -0.4087,  0.5301],\n",
      "        [ 0.1839,  0.0250, -0.4217,  0.5419],\n",
      "        [ 0.1761,  0.0442, -0.4049,  0.5559]], requires_grad=True)\n",
      "tensor([0.5050, 0.5047, 0.5129, 0.5076, 0.5058, 0.5120, 0.4718, 0.5181, 0.5199,\n",
      "        0.5256, 0.4999, 0.5083], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "loading = .7 * loading_true\n",
    "uniqueness = .7 * uniqueness_true\n",
    "# loading_mask = 1 *  (loading_true != 0)\n",
    "loading.requires_grad_(requires_grad=True)\n",
    "uniqueness.requires_grad_(requires_grad=True)\n",
    "epochs = 200\n",
    "lr = .5\n",
    "optimizer = torch.optim.Adam([loading, uniqueness], lr = lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_fa = MultivariateNormal(\n",
    "        loc = torch.zeros((loading.size()[0], )),\n",
    "        scale_tril = torch.cholesky(\n",
    "            loading @ loading.T + torch.diag(uniqueness)))\n",
    "    loss_value = -torch.mean(model_fa.log_prob(data))\n",
    "    optimizer.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "print(torch.sum(model_fa.log_prob(data)))\n",
    "print(loading)\n",
    "print(uniqueness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 期望最大化算則求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:03.676252Z",
     "iopub.status.busy": "2020-12-15T07:16:03.675582Z",
     "iopub.status.idle": "2020-12-15T07:16:03.677934Z",
     "shell.execute_reply": "2020-12-15T07:16:03.678412Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "n_item = 12\n",
    "n_factor = 4\n",
    "beta = .7 * loading_true\n",
    "tau = .7 * torch.diag(uniqueness)\n",
    "y = data - data.mean(axis = 0)\n",
    "c_yy = (y.T @ y) / n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:03.683999Z",
     "iopub.status.busy": "2020-12-15T07:16:03.683298Z",
     "iopub.status.idle": "2020-12-15T07:16:03.721454Z",
     "shell.execute_reply": "2020-12-15T07:16:03.721987Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def e_step(c_yy, beta, tau):\n",
    "    sigma_inv = (beta @ beta.T + tau).inverse()\n",
    "    delta_small = sigma_inv @ beta\n",
    "    delta_big = torch.eye(n_factor) - beta.T @ sigma_inv @ beta\n",
    "    c_yz_hat = c_yy @ delta_small\n",
    "    c_zz_hat = delta_small.T @ c_yy @ delta_small + delta_big\n",
    "    return c_yz_hat, c_zz_hat\n",
    "\n",
    "def m_step(c_yy, c_yz_hat, c_zz_hat):\n",
    "    beta = c_yz_hat @ c_zz_hat.inverse()\n",
    "    tau = torch.diag(c_yy - c_yz_hat @ c_zz_hat.inverse() @ c_yz_hat.T)\n",
    "    tau = torch.diag(tau)\n",
    "    return beta, tau\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    c_yz_hat, c_zz_hat = e_step(c_yy, beta, tau)\n",
    "    beta, tau = m_step(c_yy, c_yz_hat, c_zz_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-15T07:16:03.725404Z",
     "iopub.status.busy": "2020-12-15T07:16:03.724766Z",
     "iopub.status.idle": "2020-12-15T07:16:03.729081Z",
     "shell.execute_reply": "2020-12-15T07:16:03.729616Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.9943e-01,  9.4414e-03,  7.0608e-03, -1.6059e-03],\n",
      "        [ 6.9496e-01, -8.3238e-03, -5.2081e-04,  4.1212e-03],\n",
      "        [ 6.9935e-01, -4.6706e-03,  1.7090e-02,  9.4043e-03],\n",
      "        [ 4.8433e-03,  7.1317e-01,  4.2302e-03, -1.3696e-02],\n",
      "        [-1.0689e-02,  7.0387e-01,  8.7874e-03, -6.3474e-03],\n",
      "        [ 2.4067e-03,  7.0916e-01,  2.0881e-03,  6.8173e-03],\n",
      "        [ 3.4815e-03,  1.9012e-02,  7.1275e-01,  2.1692e-03],\n",
      "        [ 2.6130e-03,  2.2796e-03,  6.8453e-01,  1.0449e-02],\n",
      "        [ 1.7609e-02, -7.6797e-03,  6.8791e-01, -1.4011e-02],\n",
      "        [ 1.8089e-02, -1.0909e-02, -2.5496e-03,  6.9649e-01],\n",
      "        [ 5.7851e-03, -9.0123e-03, -8.8994e-03,  7.1109e-01],\n",
      "        [-1.1556e-02,  6.4172e-03,  1.0414e-02,  7.1110e-01]],\n",
      "       grad_fn=<MmBackward>)\n",
      "tensor([0.5050, 0.5047, 0.5128, 0.5075, 0.5058, 0.5119, 0.4718, 0.5181, 0.5197,\n",
      "        0.5255, 0.4999, 0.5082], grad_fn=<DiagBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(beta)\n",
    "print(torch.diag(tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 練習\n",
    "1. 問如何確定前述兩種方法得到的解都是最大概似解。\n",
    "2. 如何對於因素間之因素進行估計。\n",
    "3. 如何僅估計真實模型中不為零之因素負荷量。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}