{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Lab: 數值微分與優化\n",
    "================\n",
    "\n",
    "在此 lab 中，我們將介紹\n",
    "\n",
    "1. 如何使用 `tensorflow` 進行數值微分。\n",
    "\n",
    "2. 如何使用 `tensorflow` 進行數值優化。\n",
    "\n",
    "3. 利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:29.254586Z",
     "iopub.status.busy": "2020-12-29T06:39:29.253988Z",
     "iopub.status.idle": "2020-12-29T06:39:31.535734Z",
     "shell.execute_reply": "2020-12-29T06:39:31.536232Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 數值微分\n",
    "\n",
    "而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first-order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。\n",
    "\n",
    "### 變量\n",
    "\n",
    "在 `tensorflow` 中，我們所欲進行微分之變量（variable）乃透過 `tf.Variable` 此類型來表徵，其可透過 `tf.Variable` 此建構式來建立："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.542922Z",
     "iopub.status.busy": "2020-12-29T06:39:31.542246Z",
     "iopub.status.idle": "2020-12-29T06:39:31.547286Z",
     "shell.execute_reply": "2020-12-29T06:39:31.547927Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([1., 1., 1.])>\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 1, 1], dtype = tf.float64)\n",
    "x = tf.Variable(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "這裏，我們建立了一尺寸為 $(3,)$ 的變量。乍看之下，變量與張量非常類似，兩者皆牽涉到資料、形狀、以及類型等面向，事實上，變量背後的資料結構的確是一張量，當我們進行運算時，使用的都是該張量資料，如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.552767Z",
     "iopub.status.busy": "2020-12-29T06:39:31.551776Z",
     "iopub.status.idle": "2020-12-29T06:39:31.556062Z",
     "shell.execute_reply": "2020-12-29T06:39:31.556487Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise addition: tf.Tensor([2. 2. 2.], shape=(3,), dtype=float64)\n",
      "element-wise multiplication: tf.Tensor([1. 1. 1.], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"element-wise addition:\", x + x)\n",
    "print(\"element-wise multiplication:\", x * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "然而，變量容許我們在程式執行的過程中，不斷地對其狀態進行更新。比如說，我們可以使用 `.assign` 此方法對一變量的張量資料進行更新，其會重新使用儲存該張量的記憶體："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.561281Z",
     "iopub.status.busy": "2020-12-29T06:39:31.560632Z",
     "iopub.status.idle": "2020-12-29T06:39:31.563699Z",
     "shell.execute_reply": "2020-12-29T06:39:31.564258Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([1., 2., 3.])>\n"
     ]
    }
   ],
   "source": [
    "x.assign([1, 2, 3,])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在 `tensorflow` 中，變量最重要的功能就是用來表徵模型的參數，其可透過不同的優化策略來更新其數值，因此，變量通常是可以被訓練的（trainable），我們可透過其 `.trainbale` 屬性來了解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.567965Z",
     "iopub.status.busy": "2020-12-29T06:39:31.567291Z",
     "iopub.status.idle": "2020-12-29T06:39:31.569508Z",
     "shell.execute_reply": "2020-12-29T06:39:31.569962Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 數值微分之基礎\n",
    "在 `tensorflow` 中，自動微分乃透過 `tf.GradientTape` 來進行。首先，我們利用 `tf.GradientTape()` 所建立的環境，紀錄該變量的計算過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.576002Z",
     "iopub.status.busy": "2020-12-29T06:39:31.575182Z",
     "iopub.status.idle": "2020-12-29T06:39:31.578818Z",
     "shell.execute_reply": "2020-12-29T06:39:31.579267Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor y: \n",
      " tf.Tensor([-2.  0.  2.], shape=(3,), dtype=float64)\n",
      "tensor z: \n",
      " tf.Tensor(8.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x - 4\n",
    "    z = tf.reduce_sum(y ** 2)\n",
    "print(\"tensor y: \\n\", y)\n",
    "print(\"tensor z: \\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在此例子中，整個計算過程可寫為 $y_i = 2(x_{i} - 2)$，$z = \\sum_{i=1}^3 y_i^2$。針對已紀錄之運算過程，想要獲得與該運算有關的梯度時，可以使用 `tf.GradientTape.gradient`此函數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.583525Z",
     "iopub.status.busy": "2020-12-29T06:39:31.582892Z",
     "iopub.status.idle": "2020-12-29T06:39:31.586834Z",
     "shell.execute_reply": "2020-12-29T06:39:31.586374Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "grad_x = tape.gradient(z, x)\n",
    "print(\"dz/dx: \", grad_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "這裡， `grad_x` 即為 $\\frac{\\partial z}{\\partial x}$ 之計算結果，其張量尺寸與 `x` 相同，即 `(3,1)`。\n",
    "\n",
    "需要特別注意的是，在執行完一次 `tf.GradientTape.gradient` 後，`tensorflow` 即會將該記錄器所使用的資源給釋放出來，故無法再次使用 `tf.GradientTape.gradient` 此指令。若想要多次執行 `tf.GradientTape.gradient`，可以在建立 `GradientTape` 時，使用 `persistent=True` 此指令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.590631Z",
     "iopub.status.busy": "2020-12-29T06:39:31.589960Z",
     "iopub.status.idle": "2020-12-29T06:39:31.592504Z",
     "shell.execute_reply": "2020-12-29T06:39:31.592918Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = 2 * x - 4\n",
    "    z = tf.reduce_sum(y ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "透過該指令，我們就能夠計算多個梯度的結果，如計算 $\\frac{\\partial z}{\\partial x}$ 與 $\\frac{\\partial z}{\\partial y}$ 兩者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.596872Z",
     "iopub.status.busy": "2020-12-29T06:39:31.596242Z",
     "iopub.status.idle": "2020-12-29T06:39:31.600562Z",
     "shell.execute_reply": "2020-12-29T06:39:31.601020Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)\n",
      "dz/dy:  tf.Tensor([-4.  0.  4.], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "grad_x = tape.gradient(z, x)\n",
    "grad_y = tape.gradient(z, y)\n",
    "print(\"dz/dx: \", grad_x)\n",
    "print(\"dz/dy: \", grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們也可以在單一的 `gradient` 指令下同時計算 `x` 與 `y` 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.604802Z",
     "iopub.status.busy": "2020-12-29T06:39:31.604156Z",
     "iopub.status.idle": "2020-12-29T06:39:31.607816Z",
     "shell.execute_reply": "2020-12-29T06:39:31.608279Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tf.Tensor([-8.  0.  8.], shape=(3,), dtype=float64)\n",
      "dz/dy:  tf.Tensor([-4.  0.  4.], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "grad = tape.gradient(z, {\"x\":x, \"y\":y})\n",
    "for name, value in grad.items():\n",
    "    print(\"dz/d\" + name + \": \", value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 二階微分矩陣\n",
    "`tensorflow` 也可以用於計算二次微分矩陣，即黑塞矩陣。其過程需使用到兩組 `GradeientTape`，其中一組用來計算一階導數，另外一組用來紀錄計算一階導數的過程，並計算二階導數，如以下範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:31.612680Z",
     "iopub.status.busy": "2020-12-29T06:39:31.611967Z",
     "iopub.status.idle": "2020-12-29T06:39:32.218605Z",
     "shell.execute_reply": "2020-12-29T06:39:32.219175Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d^2z/dx^2:  [[8. 0. 0.]\n",
      " [0. 8. 0.]\n",
      " [0. 0. 8.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape2:\n",
    "  with tf.GradientTape() as tape1:\n",
    "    y = 2 * x - 4\n",
    "    z = tf.reduce_sum(y ** 2)\n",
    "  grad_x = tape1.jacobian(z, x)\n",
    "hess = tape2.jacobian(grad_x, x)\n",
    "print(\"d^2z/dx^2: \", hess.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "然而，在此需要特別小心注意張量的尺寸會如何影響計算之過程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 數值優化\n",
    "\n",
    "### 手動撰寫優化算則\n",
    "\n",
    "前一小節所使用的範例，其計算過程可以寫為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= f(x)\\\\\n",
    " &= \\sum_{i=1}^3\\left[2(x_i-2)\\right]^2 \\\\\n",
    " &= \\sum_{i=1}^3 y_i^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "若想要找到一 $\\widehat{x}$，其使得 $f(\\widehat{x})$ 達到最小值的話，由於 $z$ 為 $y_1, y_2, y_3$ 的平方和，因此，其會在 $\\widehat{y} = (0, 0, 0)$的地方達到最小值，也意味著 $\\widehat{x} = (2,2,2)$。\n",
    "\n",
    "那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 $\\theta$ 表示模型參數（其扮演範例中$x$的角色），$\\mathcal{D}(\\theta)$ 表示度量模型好壞的目標函數（其扮演$f(x)$的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）$\\widehat{\\theta}$ 的更新規則為\n",
    "\n",
    "$$\n",
    "\\widehat{\\theta} \\leftarrow \\widehat{\\theta} - s \\times \\frac{\\partial \\mathcal{D}(\\widehat{\\theta})}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "這裏，$s$ 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 $\\mathcal{D}$ 足夠圓滑（smooth），且 $s$ 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 $\\mathcal{D}$ 最小值的發生位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在開始進行梯度下降前，我們先定義一函數 `f` 使得 `z = f(x)`，並了解起始狀態時，`f(x)` 與 `x` 的數值為何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.223174Z",
     "iopub.status.busy": "2020-12-29T06:39:32.222508Z",
     "iopub.status.idle": "2020-12-29T06:39:32.224712Z",
     "shell.execute_reply": "2020-12-29T06:39:32.225173Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    z = tf.reduce_sum((2 * x - 4) ** 2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.229529Z",
     "iopub.status.busy": "2020-12-29T06:39:32.228776Z",
     "iopub.status.idle": "2020-12-29T06:39:32.233094Z",
     "shell.execute_reply": "2020-12-29T06:39:32.233527Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([1, 2, 3],\n",
    "                dtype = tf.float64)\n",
    "z = f(x)\n",
    "print(\"f(x) = {:2.3f}, x = {}\".format(z.numpy(), x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用 `tensorflow` 進行梯度下降，需先計算在當下 `x` 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 `x` 進行更新，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.239010Z",
     "iopub.status.busy": "2020-12-29T06:39:32.238339Z",
     "iopub.status.idle": "2020-12-29T06:39:32.242955Z",
     "shell.execute_reply": "2020-12-29T06:39:32.243378Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = [1.8 2.  2.2]\n"
     ]
    }
   ],
   "source": [
    "lr = .1\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(x)\n",
    "grad_x = tape.gradient(z, x)\n",
    "x.assign_sub(lr * grad_x)\n",
    "print(\"f(x) = {:2.3f}, x = {}\".format(\n",
    "    z.numpy(), x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "這裡，我們將學習速率 `lr` 設為0.1，而張量的 `.assign_sub()` 方法則是就地減去括號內的數值直接更新。透過 `f(x)` 的數值，可觀察到梯度下降的確導致 `z` 數值的下降，而 `x` 也與 $\\widehat{x}=(2,2,2)$ 更加地靠近。\n",
    "\n",
    "梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 `for` 迴圈，重複更新$I$次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.249741Z",
     "iopub.status.busy": "2020-12-29T06:39:32.249111Z",
     "iopub.status.idle": "2020-12-29T06:39:32.277210Z",
     "shell.execute_reply": "2020-12-29T06:39:32.277718Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = [1.8 2.  2.2]\n",
      "f(x) = 0.320, x = [1.96 2.   2.04]\n",
      "f(x) = 0.013, x = [1.992 2.    2.008]\n",
      "f(x) = 0.001, x = [1.9984 2.     2.0016]\n",
      "f(x) = 0.000, x = [1.99968 2.      2.00032]\n",
      "f(x) = 0.000, x = [1.999936 2.       2.000064]\n",
      "f(x) = 0.000, x = [1.9999872 2.        2.0000128]\n",
      "f(x) = 0.000, x = [1.99999744 2.         2.00000256]\n",
      "f(x) = 0.000, x = [1.99999949 2.         2.00000051]\n",
      "f(x) = 0.000, x = [1.9999999 2.        2.0000001]\n",
      "f(x) = 0.000, x = [1.99999998 2.         2.00000002]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n",
      "f(x) = 0.000, x = [2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([1, 2, 3],\n",
    "                dtype = tf.float64)\n",
    "for i in range(1, 21):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    grad_x = tape.gradient(z, x)\n",
    "    x.assign_sub(lr * grad_x)\n",
    "    print(\"f(x) = {:2.3f}, x = {}\".format(\n",
    "        z.numpy(), x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 `tol` 時，算則就停止，其所對應之程式碼為：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.282987Z",
     "iopub.status.busy": "2020-12-29T06:39:32.282362Z",
     "iopub.status.idle": "2020-12-29T06:39:32.296683Z",
     "shell.execute_reply": "2020-12-29T06:39:32.297138Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = [1.8 2.  2.2]\n",
      "f(x) = 0.320, x = [1.96 2.   2.04]\n",
      "f(x) = 0.013, x = [1.992 2.    2.008]\n",
      "f(x) = 0.001, x = [1.9984 2.     2.0016]\n",
      "f(x) = 0.000, x = [1.99968 2.      2.00032]\n",
      "f(x) = 0.000, x = [1.999936 2.       2.000064]\n",
      "f(x) = 0.000, x = [1.9999872 2.        2.0000128]\n",
      "f(x) = 0.000, x = [1.99999744 2.         2.00000256]\n",
      "f(x) = 0.000, x = [1.99999949 2.         2.00000051]\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-4\n",
    "epochs = 20\n",
    "x = tf.Variable([1, 2, 3],\n",
    "                dtype = tf.float64)\n",
    "for i in range(1, 21):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    grad_x = tape.gradient(z, x)\n",
    "    x.assign_sub(lr * grad_x)\n",
    "    print(\"f(x) = {:2.3f}, x = {}\".format(\n",
    "        z.numpy(), x.numpy()))\n",
    "    if tf.reduce_max(tf.abs(grad_x)) < tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 使用`tf.optimizers.Optimizer`進行優化\n",
    "\n",
    "由於 `tensorflow` 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 `tf.optimizers.Optimizer` 此類型來求得函數的最小值。\n",
    "\n",
    "`tf.optimizers.SGD` 為進行梯度下降法之物件，由於 `tensorflow` 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini-batch gradient descent）來強化優化的效能，因此，`tensorflow` 使用 `SGD` 一詞。事實上，除了計算一階導數時資料量的差異外，`SGD` 與傳統的梯度下降並無差異。\n",
    "\n",
    "`tf.optimizers.SGD` 可透過以下的程式碼來使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-29T06:39:32.303190Z",
     "iopub.status.busy": "2020-12-29T06:39:32.302539Z",
     "iopub.status.idle": "2020-12-29T06:39:32.325802Z",
     "shell.execute_reply": "2020-12-29T06:39:32.326224Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = [1.80000001 2.         2.19999999]\n",
      "f(x) = 0.320, x = [1.96 2.   2.04]\n",
      "f(x) = 0.013, x = [1.992 2.    2.008]\n",
      "f(x) = 0.001, x = [1.9984 2.     2.0016]\n",
      "f(x) = 0.000, x = [1.99968 2.      2.00032]\n",
      "f(x) = 0.000, x = [1.999936 2.       2.000064]\n",
      "f(x) = 0.000, x = [1.9999872 2.        2.0000128]\n",
      "f(x) = 0.000, x = [1.99999744 2.         2.00000256]\n",
      "f(x) = 0.000, x = [1.99999949 2.         2.00000051]\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-4\n",
    "epochs = 20\n",
    "x = tf.Variable([1, 2, 3],\n",
    "                dtype = tf.float64)\n",
    "opt = tf.optimizers.SGD(learning_rate = lr)\n",
    "for i in range(1, 21):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    grad_x = tape.gradient(z, x)\n",
    "    opt.apply_gradients(zip([grad_x], [x]))\n",
    "    print(\"f(x) = {:2.3f}, x = {}\".format(\n",
    "        z.numpy(), x.numpy()))\n",
    "    if tf.reduce_max(tf.abs(grad_x)) < tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "這裡，我們使用 `tf.optimizers.SGD` 來生成優化器（optimizer）物件 `opt`，其在生成時，需要指定學習速率 `learning_rate`。使用內建優化器時，我們仍須自行計算梯度，並利用 `.apply_gradient` 此方法，給定梯度與變量的列表進行更新。乍看之下，使用優化器並為簡化原有之程式碼，不過，當所欲進行更新的變量很多時，優化器可以避免逐一對變量數值進行更新的麻煩。\n",
    "\n",
    "`tf.optimizers.SGD` 容許使用者加入動能（momentum）$m$（其預設為 0），此時，優化算則會改為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta & \\leftarrow m \\times  \\delta + \\frac{\\partial \\mathcal{D}(\\widehat{\\theta})}{\\partial \\theta} \\\\\n",
    "\\widehat{\\theta} &\\leftarrow \\widehat{\\theta} - s \\times \\delta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "這裡，$\\delta$ 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。\n",
    "\n",
    "在 `tf.optimizers` 中，有許多不同的優化器（見[`tf.optimizers`頁面](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)），如\n",
    "\n",
    "+ `Adadelta`（見[ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)）\n",
    "+ `Adagrad`（見[Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/v12/duchi11a.html)）\n",
    "+ `Adam`（見[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)）\n",
    "+ `RMSprop`（見[Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)）\n",
    "\n",
    "讀者可自行深入了解這些方法。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}