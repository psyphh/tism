{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Lab: 數值微分與優化\n",
    "================\n",
    "\n",
    "在此 lab 中，我們將介紹\n",
    "\n",
    "1. 如何使用 `torch` 進行數值微分。\n",
    "\n",
    "2. 如何使用 `torch` 進行數值優化。\n",
    "\n",
    "3. 利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:48.765199Z",
     "iopub.status.busy": "2020-08-28T03:37:48.764355Z",
     "iopub.status.idle": "2020-08-28T03:37:49.088120Z",
     "shell.execute_reply": "2020-08-28T03:37:49.088621Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 數值微分\n",
    "\n",
    "### 可獲得梯度張量之輸入\n",
    "\n",
    "而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first-order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。\n",
    "\n",
    "在 `torch` 中，張量不僅用於儲存資料，其亦用於儲存模型之參數。然而，誠如先前所述，我們很可能會需要用到對應於該參數之梯度訊息，因此，為了追朔該參數的歷史建立計算圖（computation graph），輸入該參數張量時需要加入 `requires_grad=True` 此指令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.093386Z",
     "iopub.status.busy": "2020-08-28T03:37:49.092695Z",
     "iopub.status.idle": "2020-08-28T03:37:49.098591Z",
     "shell.execute_reply": "2020-08-28T03:37:49.099016Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "這裏，我們建立了一尺寸為 $3$ 的張量，由於此張量具有 `requires_grad=True` 此標記，因此，接下來對此張量進行任何的運算，`torch` 皆會將此計算過程記錄下來。舉例來說："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.103318Z",
     "iopub.status.busy": "2020-08-28T03:37:49.102635Z",
     "iopub.status.idle": "2020-08-28T03:37:49.108641Z",
     "shell.execute_reply": "2020-08-28T03:37:49.109075Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor y: \n",
      " tensor([-2.,  0.,  2.], grad_fn=<SubBackward0>)\n",
      "tensor z: \n",
      " tensor(8., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2 * x - 4\n",
    "z = (y ** 2).sum()\n",
    "print(\"tensor y: \\n\", y)\n",
    "print(\"tensor z: \\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們可以看到，無論是 `y` 或是 `z`，其都具有 `requires_grad=True` 的標記。要特別注意的是，`requires_grad=True` 僅適用於資料類型為浮點數之張量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 數值微分之執行\n",
    "針對已追朔之運算過程，想要獲得與該運算有關的梯度時，可以使用 `.backward()`此方法。在前一小節的例子中，$z = \\sum_{i=1}^3 (2x_{i} - 4)^2$，若想要獲得 $\\frac{\\partial z}{\\partial x}$ 在當下 $x$ 的數值的話，可使用以下的程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.113032Z",
     "iopub.status.busy": "2020-08-28T03:37:49.112350Z",
     "iopub.status.idle": "2020-08-28T03:37:49.116580Z",
     "shell.execute_reply": "2020-08-28T03:37:49.117202Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tensor([-8.,  0.,  8.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(\"dz/dx: \", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接著，由於 $z$ 也可以寫為 $z = \\sum_{i=1}^3 y_{i}^2$，因此，我們是否也可以透過類似的程式碼獲得 $\\frac{\\partial z}{\\partial y}$ 呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.121162Z",
     "iopub.status.busy": "2020-08-28T03:37:49.120440Z",
     "iopub.status.idle": "2020-08-28T03:37:49.125047Z",
     "shell.execute_reply": "2020-08-28T03:37:49.125474Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dy:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phhaung/Documents/PycharmProject/tism/venv/lib/python3.8/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    }
   ],
   "source": [
    "print(\"dz/dy: \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "結果是不行，主因在於，`torch` 為了節省記憶體的使用，因此，僅可提供位於計算圖葉子（leaf）張量之一次微分。如果希望能夠獲得 $\\frac{\\partial z}{\\partial y}$ 的話，可以對 `y` 使用 `.retain_grad()` 此方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.130260Z",
     "iopub.status.busy": "2020-08-28T03:37:49.129373Z",
     "iopub.status.idle": "2020-08-28T03:37:49.133717Z",
     "shell.execute_reply": "2020-08-28T03:37:49.134108Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dy:  tensor([-4.,  0.,  4.])\n"
     ]
    }
   ],
   "source": [
    "y = 2 * x - 4\n",
    "z = (y ** 2).sum()\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "print(\"dz/dy: \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在評估完 $\\frac{\\partial z}{\\partial y}$ 後，讓我們重新檢視一下 `x.grad` 的數值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.138264Z",
     "iopub.status.busy": "2020-08-28T03:37:49.137401Z",
     "iopub.status.idle": "2020-08-28T03:37:49.140828Z",
     "shell.execute_reply": "2020-08-28T03:37:49.141249Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tensor([-16.,   0.,  16.])\n"
     ]
    }
   ],
   "source": [
    "print(\"dz/dx: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們會發現，這時的 `x.grad` 數值，變成了原先的兩倍，其背後的原因在於，`.backward()`此方法，會持續地將計算結果累積在變數所對應之 `.grad` 當中。若想要避免持續累積，可以使用 `.grad.zero_()` 方法將 `.grad` 中的數值歸零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.145422Z",
     "iopub.status.busy": "2020-08-28T03:37:49.144629Z",
     "iopub.status.idle": "2020-08-28T03:37:49.148556Z",
     "shell.execute_reply": "2020-08-28T03:37:49.148991Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tensor([0., 0., 0.])\n",
      "dz/dx:  tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "print(\"dz/dx: \", x.grad)\n",
    "print(\"dz/dx: \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接著，就可以使用原先的程式碼，計算 $\\frac{\\partial z}{\\partial x}$ 與 $\\frac{\\partial z}{\\partial y}$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.153548Z",
     "iopub.status.busy": "2020-08-28T03:37:49.152778Z",
     "iopub.status.idle": "2020-08-28T03:37:49.157100Z",
     "shell.execute_reply": "2020-08-28T03:37:49.157543Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx:  tensor([-8.,  0.,  8.])\n",
      "dz/dy:  tensor([-4.,  0.,  4.])\n"
     ]
    }
   ],
   "source": [
    "y = 2 * x - 4\n",
    "z = (y ** 2).sum()\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "print(\"dz/dx: \", x.grad)\n",
    "print(\"dz/dy: \", y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "不過要特別注意的是，如果計算圖沒有重新建立，連續進行兩次 `.backward()` 會引發錯誤的訊息。\n",
    "\n",
    "### 可獲得梯度張量之進階控制\n",
    "\n",
    "一個張量是否有被追朔以計算梯度，除了直接列印外，亦可透過 `.requires_grad` 此屬性來觀看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.161438Z",
     "iopub.status.busy": "2020-08-28T03:37:49.160788Z",
     "iopub.status.idle": "2020-08-28T03:37:49.163121Z",
     "shell.execute_reply": "2020-08-28T03:37:49.163553Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "如果想將一原先沒有要求梯度之張量，改為需要梯度時，可以使用 `.requires_grad_()` 此方法原地修改該向量的 `requires_grad` 類型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.167187Z",
     "iopub.status.busy": "2020-08-28T03:37:49.166474Z",
     "iopub.status.idle": "2020-08-28T03:37:49.168844Z",
     "shell.execute_reply": "2020-08-28T03:37:49.169275Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "如果想將張量 `x` 拷貝到另一變量 `x_ng`，卻不希望 `x_ng` 的計算會被追朔時，可以使用以下的程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.173056Z",
     "iopub.status.busy": "2020-08-28T03:37:49.172429Z",
     "iopub.status.idle": "2020-08-28T03:37:49.174703Z",
     "shell.execute_reply": "2020-08-28T03:37:49.175332Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x_ng = x.detach()\n",
    "print(x_ng.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "最後，如果希望可獲得梯度之向量後續的計算歷程不被追朔的話，可以將計算程式碼置於 `with torch.no_grad():` 此環境中，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.179442Z",
     "iopub.status.busy": "2020-08-28T03:37:49.178721Z",
     "iopub.status.idle": "2020-08-28T03:37:49.181682Z",
     "shell.execute_reply": "2020-08-28T03:37:49.182188Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = 2 * x - 4\n",
    "    z = (y ** 2).sum()\n",
    "print(y.requires_grad)\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 數值優化\n",
    "\n",
    "### 手動撰寫優化算則\n",
    "\n",
    "前一小節所使用的範例，其計算過程可以寫為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= f(x)\\\\\n",
    " &= \\sum_{i=1}^3\\left[2(x_i-2)\\right]^2 \\\\\n",
    " &= \\sum_{i=1}^3 y^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "若想要找到一 $\\widehat{x}$，其使得 $f(\\widehat{x})$ 達到最小值的話，由於 $z$ 為 $y_1, y_2, y_3$ 的平方和，因此，其會在 $\\widehat{y} = (0, 0, 0)$的地方達到最小值，也意味著 $\\widehat{x} = (2,2,2)$。\n",
    "\n",
    "那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 $\\theta$ 表示模型參數（其扮演範例中$x$的角色），$\\mathcal{D}(\\theta)$ 表示度量模型好壞的目標函數（其扮演$f(x)$的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）$\\widehat{\\theta}$ 的更新規則為\n",
    "\n",
    "$$\n",
    "\\widehat{\\theta} \\leftarrow \\widehat{\\theta} - s \\times \\frac{\\partial \\mathcal{D}(\\widehat{\\theta})}{\\partial \\theta}\n",
    "$$\n",
    "這裏，$s$ 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 $\\mathcal{D}$ 足夠圓滑（smooth），且 $s$ 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 $\\mathcal{D}$ 最小值的發生位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在開始進行梯度下降前，我們先定義一函數 `f` 使得`z = f(x)`，並了解起始狀態時，`f(x)` 與 `x` 的數值為何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.185978Z",
     "iopub.status.busy": "2020-08-28T03:37:49.185369Z",
     "iopub.status.idle": "2020-08-28T03:37:49.187376Z",
     "shell.execute_reply": "2020-08-28T03:37:49.187981Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    z = ((2 * x - 4) ** 2).sum()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.192052Z",
     "iopub.status.busy": "2020-08-28T03:37:49.191364Z",
     "iopub.status.idle": "2020-08-28T03:37:49.194836Z",
     "shell.execute_reply": "2020-08-28T03:37:49.195344Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 8.000, x = tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad=True)\n",
    "z = f(x)\n",
    "print(\"f(x) = {:2.3f}, x = {}\".format(z.item(), x.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用 `torch` 進行梯度下降，需先計算在當下 `x` 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 `x` 進行更新，即"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.199901Z",
     "iopub.status.busy": "2020-08-28T03:37:49.199088Z",
     "iopub.status.idle": "2020-08-28T03:37:49.202689Z",
     "shell.execute_reply": "2020-08-28T03:37:49.203141Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])\n"
     ]
    }
   ],
   "source": [
    "lr = .1\n",
    "z.backward()\n",
    "with torch.no_grad():\n",
    "    x.sub_(lr * x.grad)\n",
    "    x.grad.zero_()\n",
    "z = f(x)\n",
    "print(\"f(x) = {:2.3f}, x = {}\".format(z.item(), x.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "這裡，我們將學習速率 `lr` 設為0.1，而張量的 `.sub_()` 方法則是就地減去括號內的數值直接更新。透過 `f(x)` 的數值，可觀察到梯度下降的確導致 `z` 數值的下降，而 `x` 也與 $\\widehat{x}=(2,2,2)$ 更加地靠近。\n",
    "\n",
    "梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 `for` 迴圈，重複更新$I$次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.208302Z",
     "iopub.status.busy": "2020-08-28T03:37:49.207660Z",
     "iopub.status.idle": "2020-08-28T03:37:49.222274Z",
     "shell.execute_reply": "2020-08-28T03:37:49.222782Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1, f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])\n",
      "iter  2, f(x) = 0.013, x = tensor([1.9600, 2.0000, 2.0400])\n",
      "iter  3, f(x) = 0.001, x = tensor([1.9920, 2.0000, 2.0080])\n",
      "iter  4, f(x) = 0.000, x = tensor([1.9984, 2.0000, 2.0016])\n",
      "iter  5, f(x) = 0.000, x = tensor([1.9997, 2.0000, 2.0003])\n",
      "iter  6, f(x) = 0.000, x = tensor([1.9999, 2.0000, 2.0001])\n",
      "iter  7, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  8, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  9, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter 10, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter 11, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 12, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 13, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 14, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 15, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 16, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 17, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 18, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 19, f(x) = 0.000, x = tensor([2., 2., 2.])\n",
      "iter 20, f(x) = 0.000, x = tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad=True)\n",
    "z = f(x)\n",
    "for i in range(1, 21):\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x.sub_(lr * x.grad)\n",
    "    z = f(x)\n",
    "    print(\"iter {:2.0f}, f(x) = {:2.3f}, x = {}\".format(i, z.item(), x.data))\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 `tol` 時，算則就停止，其所對應之程式碼為："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.228364Z",
     "iopub.status.busy": "2020-08-28T03:37:49.227474Z",
     "iopub.status.idle": "2020-08-28T03:37:49.237878Z",
     "shell.execute_reply": "2020-08-28T03:37:49.238382Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])\n",
      "iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])\n",
      "iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])\n",
      "iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])\n",
      "iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])\n",
      "iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])\n",
      "iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-5\n",
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad=True)\n",
    "z = f(x)\n",
    "for i in range(1, 21):\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x.sub_(lr * x.grad)\n",
    "    z = f(x)\n",
    "    print(\"iter {:2.0f}, z = {:2.3f}, x = {}\".format(i, z.item(), x.data))\n",
    "    if (x.grad.abs().max().item() < tol):\n",
    "        break\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 使用`torch.optim`進行優化\n",
    "\n",
    "由於 `torch` 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 `torch.optim` 的類型來求得函數的最小值。\n",
    "\n",
    "`torch.optim.SGD` 為進行梯度下降法之物件，由於 `torch` 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini-batch gradient descent）來強化優化的效能，因此，`torch` 使用 `SGD` 一詞。事實上，除了計算一階導數時資料量的差異外，`SGD` 與傳統的梯度下降並無差異。\n",
    "\n",
    "`torch.optim.SGD` 可透過以下的程式碼來使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-08-28T03:37:49.244021Z",
     "iopub.status.busy": "2020-08-28T03:37:49.243246Z",
     "iopub.status.idle": "2020-08-28T03:37:49.252659Z",
     "shell.execute_reply": "2020-08-28T03:37:49.253167Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])\n",
      "iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])\n",
      "iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])\n",
      "iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])\n",
      "iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])\n",
      "iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])\n",
      "iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n",
      "iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3],\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad=True)\n",
    "opt = torch.optim.SGD((x,), lr=.1)\n",
    "z = f(x)\n",
    "for i in range(1, 21):\n",
    "    z.backward()\n",
    "    opt.step()\n",
    "    z = f(x)\n",
    "    print(\"iter {:2.0f}, z = {:2.3f}, x = {}\".format(i, z.item(), x.data))\n",
    "    if (x.grad.abs().max().item() < tol):\n",
    "        break\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "這裡，我們使用 `torch.optim.SGD` 來生成優化器（optimizer）物件 `opt`，其在生成時，需要指定其追朔的變量，並以一可迭代的物件（iterable object）作為輸入，因此，在該程式碼中，我們將 `x` 以一元組（tuple）的方式來輸入，並指定學習速率 `lr=.1`。使用內建優化器時，仍須手動對目標函數執行 `.backward()`，但更新估計值的步驟，可使用優化器的 `.step()` 來進行，而消除變量的 `.grad`，則可使用優化器的 `.zero_grad()` 方法。\n",
    "\n",
    "\n",
    "`torch.optim.SGD` 容許使用者加入動能（momentum）$m$（其預設為 0），此時，優化算則會改為\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta & \\leftarrow m \\times  \\delta + \\frac{\\partial \\mathcal{D}(\\widehat{\\theta})}{\\partial \\theta} \\\\\n",
    "\\widehat{\\theta} &\\leftarrow \\widehat{\\theta} - s \\times \\delta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "這裡，$\\delta$ 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。\n",
    "\n",
    "在 `torch.optim.SGD` 中，有許多不同的優化器（見[`torch.optim`頁面](https://pytorch.org/docs/stable/optim.html)），如\n",
    "\n",
    "+ `Adadelta`（見[ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)）\n",
    "+ `Adagrad`（見[Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/v12/duchi11a.html)）\n",
    "+ `Adam`（見[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)）\n",
    "+ `LBFGS`（見[On the limited memory BFGS method for large scale optimization](https://doi.org/10.1007/BF01589116)）\n",
    "+ `Adadelta`（見[ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)）\n",
    "+ `RMSprop`（見[Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}