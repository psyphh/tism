%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}




\title{統計建模技法}
\date{Aug 28, 2020}
\release{}
\author{Po\sphinxhyphen{}Hsien Huang}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


「心理計量學」（psychometrics）為一探討如何使用數量模型，刻畫人類外顯行為與內在歷程之學門。典型的「心理計量模型」（psychometric model）包含「因素分析」（factor analysis）、「試題反應理論」（item response theory）等試圖刻畫潛在構念（latent constructs）與觀察指標（observed indicators）間關係之方法。因此，潛在構念（或稱潛在變項）的出現，可說是「心理計量模型」的一大特色，而如何處理潛在變項對於統計推論與心理學研究帶來的挑戰，則為「心理計量學」的核心議題。

「心理計量學」可視為「統計學」與「心理學」兩學門領域的結合，因此，在進行「心理計量學」研究時，除了心理學的知識外，也需要具備以下三類的技能：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
數理技能（mathematical skills），包括線性代數（linear algebra）、微積分（calculus）、以及機率（probability）。

\item {} 
統計理論（statistical theory），包括建立估計（estimation）與推論（inference）程序的技術。

\item {} 
程式設計（programming），包括如何在電腦上進行線性代數之計算、數值微分與積分、以及機率物件的操弄。

\end{enumerate}

從個人的角度，「統計學」可分為「建模（modeling）」與「推論（inference）」兩個議題。「建模」指的是如何使用數量模型來合理地刻畫現象（如使用線性或是二次式來描述兩變項間的關係），而「推論」指的是如何考慮到資料的隨機性以對模型參數進行合理之詮釋（如對迴歸係數進行估計與檢定）。儘管在理論統計（theoretical statistics）的領域，仍不斷地有新的「推論」方法被發展出來，但在統計的實務應用時，研究者大多採用相對老舊的「推論」方法，但透過不同的「建模」策略來對資料背後的科學現象以更深入的了解。因此，「建模」可說是當代量化研究中很重要的一個面向。

此頁面放置了黃柏僩老師所撰寫的「統計建模技法」（Techniques in Statistical Modeling，TISM）講義，其旨在介紹「統計建模」所需的基本技術，以及如何使用概似函數（likelihood function）對模型進行評估。此外，本講義也將以「心理計量學」常使用之模型作為範例，並搭配python的深度學習套件PyTorch進行實作。

此講義主要用於訓練非統計背景的量化方法主修研究生，講義本身並無法達到「自給自足」（self\sphinxhyphen{}contained）的目標，因此，對於交代不清楚的部分，我會盡可能地附上參考文獻或是外部連結，讀者可以自行將該缺失的部分補足。

在閱讀此講義之前，讀者需具有以下之先備知識：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
數理基礎，包括向量（vector）與矩陣（matrix）之運算規則、微分（differentiation）與積分（integral）之幾何意義、機率分配與期望值（expectation）。

\item {} 
統計學的基礎，包括平均數、變異數、相關係數之計算，估計、假設檢定（hypothesis testing）、與信賴區間（confidence interval）之概念。

\item {} 
python程式設計基礎，包括基本資料類型、流程控制（flow control）、函數等概念。若無基礎，Jake VanderPlask 的 \sphinxhref{https://jakevdp.github.io/WhirlwindTourOfPython}{A Whirlwind Tour of Python} 會是一很好的學習資源。

\end{enumerate}

讀者不需精熟這些先備知識，但至少須知道這些概念的定義，以及不害怕深入了解這些概念。

若發現講義的內容有誤，請將勘誤寄至 psyphh@gmail.com，感謝！


\chapter{先備數學知識}
\label{\detokenize{notebook/mathematics-prerequisite:id1}}\label{\detokenize{notebook/mathematics-prerequisite::doc}}

\chapter{線性迴歸}
\label{\detokenize{notebook/linear-regression:id1}}\label{\detokenize{notebook/linear-regression::doc}}
線性迴歸（linear regression）可說是統計建模的基礎，其試圖建立一線性函數（linear function），以描述兩變項 \(x\) 與 \(y\) 之間的關係。這裡，\(x=(x_1,x_2,..., x_P)\)為一 \(P\) 維之向量，其常被稱獨變項（independent variable）、共變量（covariate），或是特徵（feature），而 \(y\) 則為一純量（scalar），其常被稱作依變項（dependent variable）或是反應變項（response variable）。

在此主題中，我們將會學習以下的重點：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
使用線性函數刻畫兩變項間的關係。

\item {} 
使用最小平方法（least squares method）來建立參數之估計準則。

\item {} 
使用一階導數（derivative）來刻畫最小平方估計值（estimate）之最適條件（optimality condition）

\item {} 
使用線性代數來解決線性迴歸之問題。

\end{enumerate}


\section{線性迴歸模型}
\label{\detokenize{notebook/linear-regression:id2}}
廣義來說，迴歸分析試圖使用一 \(P\) 維之向量 \(x\)，對於 \(y\) 進行預測，其假設 \(x\) 與 \(y\) 存在以下的關係
\begin{equation*}
\begin{split}y=f(x)+\epsilon\end{split}
\end{equation*}
這裡，\(f(x)\) 表示一函數，其描述了 \(x\) 與 \(y\) 系統性的關係，而 \(\epsilon\) 則表示一隨機誤差，其平均數為0，變異數為 \(\sigma_{\epsilon}^2\)，即 \(\epsilon \sim (0,\sigma_{\epsilon}^2)\)。

線性迴歸模型假設 \(f(x)\) 為一線性函數，即
\begin{equation*}
\begin{split}f(x) =\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_P x_P\end{split}
\end{equation*}
這裡，\(\beta_p\) 為 \(x_p\) 所對應之迴歸係數（regression coefficient），其亦被稱作權重（weight），反映 \(x_p\) 每變動一個單位時，預期 \(y\) 跟著變動的量，\(\beta_0\) 則稱作截距（intercept），亦稱作偏誤（bias），其反映當 \(x_1,x_2,..,x_P\)皆為0時，我們預期 \(y\) 的數值。利用連加的符號，\(f(x)\)可簡單的寫為
\begin{equation*}
\begin{split}f(x) = \beta_0 + \sum_{p=1}^P \beta_p x_p\end{split}
\end{equation*}
無論是迴歸係數 \(\beta_p\) 或是截距 \(\beta_0\)，由於其刻畫了 \(f(x)\) 的形狀，故其皆被稱作模型參數（model parameter），文獻中常簡單以一 \(P+1\) 維之向量 \(\beta = (\beta_0, \beta_1, ..., \beta_P)\) 來表示模型中的所有參數。線性迴歸分析的主要目的乃透過一樣本資料，獲得對於 \(\beta\) 之估計 \(\widehat{\beta}\)，一方面對於各參數 \(\widehat{\beta}_p\) 進行推論，二方面則是使用 \(\widehat{f}(x) = \widehat{\beta}_0 + \sum_{p=1}^P \widehat{\beta}_p x_p\) 對 \(y\) 進行預測。


\section{最小平方估計法}
\label{\detokenize{notebook/linear-regression:id3}}
線性迴歸分析主要採用最小平方法（least squares method，簡稱 LS 法）以對模型參數進行估計。令\((x_n, y_n)\) 表示第 \(n\) 位個體於 \(x\) 與 \(y\) 之觀測值，則給定一隨機樣本 \(\{(x_n, y_n) \}_{n=1}^N\)，LS 估計準則可寫為
\begin{equation*}
\begin{split}\begin{aligned}
\mathcal{D}(\beta)
= &\frac{1}{N} \sum_{n=1}^N \left (y_n - \beta_0 - \sum_{p=1}^P \beta_p x_{np} \right )^2
\end{aligned}\end{split}
\end{equation*}
由於迴歸模型假設 \(y=f(x)+\epsilon\)，且線性迴歸僅考慮線性的關係 \(f(x) = \beta_0 + \sum_{p=1}^P \beta_p x_p\)，因此，第 \(n\) 筆觀測值對應之殘差可寫為 \(\epsilon_n = y_n - f(x_n)\)，LS 估計準則亦可簡單地寫為
\($\begin{aligned}
\mathcal{D}(\beta)
= &\frac{1}{N} \sum_{n=1}^N \epsilon_n^2
\end{aligned}\)\$

LS估計法的目標在於找到一估計值 \(\widehat{\beta} = (\widehat{\beta}_0, \widehat{\beta}_1, ..., \widehat{\beta}_P)\)，其最小化 LS 估計準則，意即，\(\widehat{\beta}\) 可最小化樣本資料中所有殘差的平方和。


\section{一階導數與最適條件}
\label{\detokenize{notebook/linear-regression:id4}}

\section{線性代數與迴歸}
\label{\detokenize{notebook/linear-regression:id5}}
線性迴歸的問題可以簡單的使用矩陣與向量的方式來表徵
\begin{equation*}
\begin{split}y = X \beta + \epsilon,\end{split}
\end{equation*}\begin{equation*}
\begin{split}\mathop{\begin{pmatrix}
  y_{1} \\
  y_{2} \\
  \vdots \\
  y_{N}
 \end{pmatrix}}_{N \times 1}=
\mathop{\begin{pmatrix}
  1 & x_{11} & \cdots & x_{1P} \\
  1 & x_{21} & \cdots & x_{2P} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  1 & x_{N1} & \cdots & x_{NP}
 \end{pmatrix}}_{N \times (P+1)}
\mathop{\begin{pmatrix}
  \beta_{0} \\
  \beta_{1} \\
  \vdots \\
  \beta_{P}
 \end{pmatrix}}_{(P+1) \times 1}+
 \mathop{\begin{pmatrix}
  \epsilon_{1} \\
  \epsilon_{2} \\
  \vdots \\
  \epsilon_{N}
 \end{pmatrix}}_{N \times 1}.\end{split}
\end{equation*}
在這邊我們於符號使用上有稍微偷懶， \(y\) 與 \(\epsilon\) 在這用於表徵 \(N\) 維的向量。在前式的表徵下，LS 估計準則可以寫為
\begin{equation*}
\begin{split}\begin{aligned}
\mathcal{D}(\beta) & =  \frac{1}{N} \sum_{n=1}^N \epsilon_n^2 \\
  & = \frac{1}{N} \epsilon^T \epsilon\\
 & =  \frac{1}{N} (y-X\beta)^T(y-X\beta).
\end{aligned}\end{split}
\end{equation*}
透過對 \(\beta\) 的每一個成分做偏微分，我們可以得到刻畫 LS 解的一階條件
\begin{equation*}
\begin{split}\begin{aligned}
\frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta}&=
\begin{pmatrix}
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_0}  \\
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_1} \\
   \vdots \\
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_P}
 \end{pmatrix} \\
&=-\frac{2}{N} X^T(y-X\widehat{\beta})=0.
\end{aligned}\end{split}
\end{equation*}
此純量對向量（scalar by vector）的微分的計算，可按照定義對各個 \(\beta_p\) 進行微分後，再利用矩陣乘法之特性獲得。除此之外，亦可以參考矩陣微分（matrix calculus）中，對於\sphinxhref{https://en.wikipedia.org/wiki/Matrix\_calculus\#Scalar-by-vector\_identities}{純量對向量微分之規則}。\(-\frac{2}{N} X^T(y-X\widehat{\beta})=0\) 意味著 \(\widehat{\beta}\) 需滿足以下的等式
\begin{equation*}
\begin{split}
X^T X\widehat{\beta}=X^Ty.
\end{split}
\end{equation*}
因此，若 \(X^T X\) 存在反矩陣，則迴歸係數的 LS 估計值可寫為
\begin{equation*}
\begin{split}
\widehat{\beta} = (X^T X)^{-1} X^Ty
\end{split}
\end{equation*}
確保 \(X^T X\) 存在反矩陣的數學條件為其各直行向量（column vector）間並未存在線性相依（linear dependence）的狀況。所謂向量間有線性相依指的是某個向量，可以寫為其它向量的線性組合。一般來說，當樣本數大於變項數（\(N > P\)），各變項間變異數皆大於0，且皆存在其獨特的訊息時，\(X^T X\) 為可逆的。

\((X^T X)^{-1}\) 的計算，常採用\sphinxhref{https://en.wikipedia.org/wiki/Gaussian\_elimination}{高斯消去法}（Gaussian elimination）或是\sphinxhref{https://en.wikipedia.org/wiki/QR\_decomposition}{QR分解}（QR decomposition），這兩種方法的計算複雜度皆為 \(O(P^3)\)，意即，其牽涉到大約 \(K \times P^3\) 這麼多步驟的計算，這裡，\(K\) 表示一跟 \(P\) 無關的常數，因此，當 \(P\) 很大時，此反矩陣的計算可能會很耗時。


\chapter{Lab: 張量與線性代數}
\label{\detokenize{notebook/lab-torch-tensor:lab}}\label{\detokenize{notebook/lab-torch-tensor::doc}}
此 lab 中，我們將會透過 \sphinxcode{\sphinxupquote{torch}} 此套件，學習以下的主題。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
認識 \sphinxcode{\sphinxupquote{torch}} 的張量（tensor）之基礎。

\item {} 
了解如何對 \sphinxcode{\sphinxupquote{torch}} 張量進行操弄。

\item {} 
使用 \sphinxcode{\sphinxupquote{torch}} 進行線性代數之運算。

\item {} 
應用前述之知識，建立一可進行線性迴歸分析之類型（class）。

\end{enumerate}

\sphinxcode{\sphinxupquote{torch}}之安裝與基礎教學，可參考 \sphinxhref{https://pytorch.org/get-started/locally}{PyTorch官方網頁}。在安裝完成後，可透過以下的指令載入

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}


\section{張量之基礎}
\label{\detokenize{notebook/lab-torch-tensor:id1}}

\subsection{張量之輸入}
\label{\detokenize{notebook/lab-torch-tensor:id2}}
\sphinxcode{\sphinxupquote{torch}} 最基本的物件是張量（tensor），其與 \sphinxcode{\sphinxupquote{numpy}} 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 \sphinxcode{\sphinxupquote{python}} 的 \sphinxcode{\sphinxupquote{list}} 或是 \sphinxcode{\sphinxupquote{numpy}} 的 \sphinxcode{\sphinxupquote{ndarray}}），置於\sphinxcode{\sphinxupquote{torch.tensor}}函數中

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Tensor
\end{sphinxVerbatim}

透過 \sphinxcode{\sphinxupquote{type()}}，可看見其屬於 \sphinxcode{\sphinxupquote{torch.Tensor}} 此一類型（class），若欲了解 \sphinxcode{\sphinxupquote{a}} 的樣貌，我們可使用 \sphinxcode{\sphinxupquote{print}} 指令來列印其主要的內容

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
\end{sphinxVerbatim}

透過對 \sphinxcode{\sphinxupquote{a}} 列印的結果，我們可觀察到：
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{a}} 內部的資料數值（value）為 \sphinxcode{\sphinxupquote{{[}{[}1, 2, 3, 4{]}, {[}5, 6, 7, 8{]}, {[}9, 10, 11, 12{]}{]}}}。

\end{itemize}

除此之外，\sphinxcode{\sphinxupquote{a}} 還有兩個重要的屬性並未顯示在列印的結果中：
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{a}} 的尺寸（size）為 \sphinxcode{\sphinxupquote{(3, 4)}}，表示 \sphinxcode{\sphinxupquote{a}} 為一 \(3 \times 4\) 之張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則。

\item {} 
\sphinxcode{\sphinxupquote{a}} 的資料類型（data type）為 \sphinxcode{\sphinxupquote{int64}}，表示64位元的整數。在進行運算時，張量間的類型須相同。

\end{itemize}

稍後，我們會討論如何獲得 \sphinxcode{\sphinxupquote{torch}} 張量的尺寸與資料類型。


\subsection{張量之數值}
\label{\detokenize{notebook/lab-torch-tensor:id3}}
若要獲得張量的資料數值（value），可透過 \sphinxcode{\sphinxupquote{.numpy()}}獲得，其回傳該張量對應之 \sphinxcode{\sphinxupquote{numpy}} 陣列

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data of tensor is: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data of tensor is: 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{numpy}} 陣列是 \sphinxcode{\sphinxupquote{python}} 進行科學運算時，幾乎都會仰賴的資料格式。\sphinxcode{\sphinxupquote{torch}} 內建了多種函數，以協助產生具有特別數值結構之張量：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with all elements being ones }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with all elements being zeros }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{identity\PYGZhy{}like tensor }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{m} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{diagonal matrix }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with all elements being ones 
 [[1.]
 [1.]
 [1.]
 [1.]]
tensor with all elements being zeros 
 [[0. 0. 0.]
 [0. 0. 0.]]
identity\PYGZhy{}like tensor 
 [[1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]]
diagonal matrix 
 [[1 0 0 0]
 [0 2 0 0]
 [0 0 3 0]
 [0 0 0 4]]
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{torch}} 亦可指定分配產生隨機的資料

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with random elements from uniform(0, 1) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with uninitialized data }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with random elements from uniform(0, 1) 
 [[0.02895176 0.94688255 0.41787893 0.5014122  0.25421917 0.8561887 ]
 [0.02338874 0.30045998 0.8770489  0.9453684  0.5081183  0.68974465]]
tensor with uninitialized data 
 [[ 2.4259038e\PYGZhy{}18  1.7516231e\PYGZhy{}43  1.4012985e\PYGZhy{}45  2.3510604e\PYGZhy{}38
  \PYGZhy{}2.2662319e\PYGZhy{}34 \PYGZhy{}8.5920297e+09]
 [\PYGZhy{}2.4576386e\PYGZhy{}34 \PYGZhy{}8.5920297e+09  7.0064923e\PYGZhy{}45 \PYGZhy{}8.5899346e+09
   0.0000000e+00 \PYGZhy{}8.5899346e+09]]
\end{sphinxVerbatim}


\subsection{張量之形狀}
\label{\detokenize{notebook/lab-torch-tensor:id4}}
張量之形狀與形狀維度之數量，可透過張量物件的 \sphinxcode{\sphinxupquote{.size()}}（或 \sphinxcode{\sphinxupquote{.shape}}） 與 \sphinxcode{\sphinxupquote{dim}} 方法來獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dim of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{dim}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
size of tensor is torch.Size([3, 4])
size of tensor is torch.Size([3, 4])
dim of tensor is 2
\end{sphinxVerbatim}

如果要對張量的形狀進行改變的話，可透過 \sphinxcode{\sphinxupquote{.view()}} 此方法獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (4, 3): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (2, 2, 3): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12, 1): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12, 1) by (\PYGZhy{}1, 1): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12,): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with shape (4, 3): 
 [[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
tensor with shape (2, 2, 3): 
 [[[ 1  2  3]
  [ 4  5  6]]

 [[ 7  8  9]
  [10 11 12]]]
tensor with shape (12, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12, 1) by (\PYGZhy{}1, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12,): 
 [ 1  2  3  4  5  6  7  8  9 10 11 12]
\end{sphinxVerbatim}

注意，\sphinxcode{\sphinxupquote{(12, 1)}} 與 \sphinxcode{\sphinxupquote{(12,)}} 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。另外，\sphinxhyphen{}1表示該面向對應之尺寸，由其它面向決定。


\subsection{張量之資料類型}
\label{\detokenize{notebook/lab-torch-tensor:id5}}
張量的資料類型，可透過 \sphinxcode{\sphinxupquote{.dtype}} 方法獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data type of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data type of tensor is torch.int64
\end{sphinxVerbatim}

若是要調整資料類型的話，則可透過 \sphinxcode{\sphinxupquote{.type()}} 此方法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([[ 1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12.]], dtype=torch.float64)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{torch}} 內建多種資料類型，包含整數類型（如 \sphinxcode{\sphinxupquote{torch.int32}} 與 \sphinxcode{\sphinxupquote{torch.int64}}）與浮點數類型（如 \sphinxcode{\sphinxupquote{torch.float32}} 與 \sphinxcode{\sphinxupquote{torch.float64}}），完整的資料類型請見 \sphinxhref{https://pytorch.org/docs/stable/tensors.html}{torch.Tensor文件}。

在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 \sphinxcode{\sphinxupquote{torch}} 常用之資料類型為 \sphinxcode{\sphinxupquote{torch.float32}} 與 \sphinxcode{\sphinxupquote{torch.float64}}，前者所需的記憶體較小，但運算結果的數值誤差較大。


\section{張量之操弄}
\label{\detokenize{notebook/lab-torch-tensor:id6}}

\subsection{張量之切片}
\label{\detokenize{notebook/lab-torch-tensor:id7}}
若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。\sphinxcode{\sphinxupquote{torch}} 張量的切片方式，與 \sphinxcode{\sphinxupquote{numpy}} 類似，皆使用中括號 \sphinxcode{\sphinxupquote{{[}{]}}}，再搭配所欲擷取資料行列的索引（index）獲得。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 1st row: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 1st and 2nd rows: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 2nd column: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 2nd and 3rd columns: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
extract 1st row: 
 [1 2 3 4]
extract 1st and 2nd rows: 
 [[1 2 3 4]
 [5 6 7 8]]
extract 2nd column: 
 [ 2  6 10]
extract 2nd and 3rd columns: 
 [[ 2  3]
 [ 6  7]
 [10 11]]
\end{sphinxVerbatim}

進行切片時，有幾項重點需要注意。
\begin{itemize}
\item {} 
各面向之索引從0開始。

\item {} 
負號表示從結尾數回來，如 \sphinxcode{\sphinxupquote{\sphinxhyphen{}1}} 表示最後一個位置。

\item {} 
\sphinxcode{\sphinxupquote{:}}表示該面向所有元素皆挑選。

\item {} 
\sphinxcode{\sphinxupquote{start:stop}} 表示從 \sphinxcode{\sphinxupquote{start}} 開始挑選到 \sphinxcode{\sphinxupquote{stop\sphinxhyphen{}1}}。

\item {} 
\sphinxcode{\sphinxupquote{start:stop:step}} 表示從 \sphinxcode{\sphinxupquote{start}} 開始到 \sphinxcode{\sphinxupquote{stop\sphinxhyphen{}1}}，間隔 \sphinxcode{\sphinxupquote{step}} 挑選。

\end{itemize}


\subsection{張量之串接}
\label{\detokenize{notebook/lab-torch-tensor:id8}}
多個張量在維度可對應之前提下，可透過 \sphinxcode{\sphinxupquote{torch.cat}} 串接

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{vertical concatenation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{,} \PYG{n}{a}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{horizontal concatenation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{,} \PYG{n}{a}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
vertical concatenation 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
horizontal concatenation 
 [[ 1  2  3  4  1  2  3  4]
 [ 5  6  7  8  5  6  7  8]
 [ 9 10 11 12  9 10 11 12]]
\end{sphinxVerbatim}


\section{張量之運算}
\label{\detokenize{notebook/lab-torch-tensor:id9}}
考慮以下 \sphinxcode{\sphinxupquote{a}} 與 \sphinxcode{\sphinxupquote{b}} 兩張量

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor b is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{b}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor a is 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
tensor b is 
 [[1. 2.]
 [1. 2.]
 [1. 2.]]
\end{sphinxVerbatim}

我們將使用 \sphinxcode{\sphinxupquote{a}} 與 \sphinxcode{\sphinxupquote{b}} 來展示如何使用 \sphinxcode{\sphinxupquote{torch}} 進行張量間的計算。


\subsection{張量元素對元素之運算}
\label{\detokenize{notebook/lab-torch-tensor:id10}}
透過 \sphinxcode{\sphinxupquote{torch}} 的數學函數，可進行張量元素對元素的四則運算

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise add }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise subtract }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise multiply }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mul}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise divide }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{div}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
element\PYGZhy{}wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element\PYGZhy{}wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element\PYGZhy{}wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element\PYGZhy{}wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
\end{sphinxVerbatim}

前述採用的函數，皆可取代為其所對應之運算子計算

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise add }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{+} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise subtract }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise multiply }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{*} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise divide }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{/} \PYG{n}{b}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
element\PYGZhy{}wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element\PYGZhy{}wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element\PYGZhy{}wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element\PYGZhy{}wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
\end{sphinxVerbatim}

若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 \sphinxhref{https://pytorch.org/docs/stable/torch.html\#math-operations}{troch官方文件} 此模組中尋找對應的數學函數。


\subsection{張量線性代數之運算}
\label{\detokenize{notebook/lab-torch-tensor:id11}}
除了簡單的四則運算外，當張量的 \sphinxcode{\sphinxupquote{dim}} 為2時，\sphinxcode{\sphinxupquote{torch}} 提供了進行線性代數（linear algebra）相關的函數，如
\begin{itemize}
\item {} 
矩陣轉置（matrix transpose）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a\PYGZus{}t} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n+nb}{input}\PYG{o}{=}\PYG{n}{a}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transpose of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a\PYGZus{}t}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
transpose of a is 
 [[1. 3. 5.]
 [2. 4. 6.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
矩陣乘法（matrix multiplication）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c} \PYG{o}{=} \PYG{n}{a\PYGZus{}t} \PYG{o}{@} \PYG{n}{a}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c = a\PYGZus{}t @ a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
c = a\PYGZus{}t @ a is 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
反矩陣（matrix inverse）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c\PYGZus{}inv} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inverse of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c\PYGZus{}inv}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} c @ c\PYGZus{}inv should be identity matrix}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for inverse (left) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c\PYGZus{}inv} \PYG{o}{@} \PYG{n}{c}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for inverse (right) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c} \PYG{o}{@} \PYG{n}{c\PYGZus{}inv}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
inverse of c is 
 [[ 2.33333333 \PYGZhy{}1.83333333]
 [\PYGZhy{}1.83333333  1.45833333]]
check for inverse (left) 
 [[ 1.00000000e+00 \PYGZhy{}1.42108547e\PYGZhy{}14]
 [ 0.00000000e+00  1.00000000e+00]]
check for inverse (right) 
 [[1. 0.]
 [0. 1.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
Cholesky 拆解（Cholesky decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c\PYGZus{}chol} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cholesky factor of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c\PYGZus{}chol}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for Cholesky decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c\PYGZus{}chol} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{c\PYGZus{}chol}\PYG{p}{,} \PYG{l+m+mi}{0} \PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Cholesky factor of c is 
 [[5.91607978 0.        ]
 [7.43735744 0.82807867]]
check for Cholesky decomposition 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
特徵拆解（eigen\sphinxhyphen{}decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{e}\PYG{p}{,} \PYG{n}{v} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{symeig}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{,} \PYG{n}{eigenvectors}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eigenvalue of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{e}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eigenvector of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{v}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for eigen\PYGZhy{}decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{v} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)} \PYG{o}{@}
       \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{v}\PYG{p}{,} \PYG{l+m+mi}{0} \PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
eigenvalue of c is 
 [ 0.26450509 90.73549491]
eigenvector of c is 
 [[\PYGZhy{}0.78489445  0.61962948]
 [ 0.61962948  0.78489445]]
check for eigen\PYGZhy{}decomposition 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
奇異值拆解（singular value decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{u}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,} \PYG{n}{v} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{svd}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{singular value of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{s}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{left singular vector of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{u}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{right singular vector of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{v}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for singular value decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{u} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)} \PYG{o}{@}
       \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{v}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
singular value of a is 
 [9.52551809 0.51430058]
left singular vector of a is 
 [[\PYGZhy{}0.2298477   0.88346102]
 [\PYGZhy{}0.52474482  0.24078249]
 [\PYGZhy{}0.81964194 \PYGZhy{}0.40189603]]
right singular vector of a is 
 [[\PYGZhy{}0.61962948 \PYGZhy{}0.78489445]
 [\PYGZhy{}0.78489445  0.61962948]]
check for singular value decomposition 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
\end{sphinxVerbatim}


\subsection{對張量之數值進行摘要}
\label{\detokenize{notebook/lab-torch-tensor:id12}}
\sphinxcode{\sphinxupquote{torch}} 提供了一些化約（reduction）的函數，對張量內的數值進行摘要

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate standard deviation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate max }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate min }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate mean 
 3.5
calculate standard deviation 
 1.8708286933869707
calculate max 
 6.0
calculate min 
 1.0
\end{sphinxVerbatim}

我們亦可對張量的各面向，進行前述的摘要。以平均數為例：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean for each column }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean for each row }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate mean for each column 
 [3. 4.]
calculate mean for each row 
 [1.5 3.5 5.5]
\end{sphinxVerbatim}

其它的化約函數，可以參考\sphinxhref{https://pytorch.org/docs/stable/torch.html\#reduction-ops}{官方文件}。


\section{實徵範例}
\label{\detokenize{notebook/lab-torch-tensor:id13}}

\subsection{產生線性迴歸資料}
\label{\detokenize{notebook/lab-torch-tensor:id14}}
在開始之前，我們先設定一種子，以讓後續的亂數生成都能夠獲得相同的結果（不過，這裡的 \sphinxcode{\sphinxupquote{manual\_seed}} 僅適用於CPU，若使用GPU，請改為 \sphinxcode{\sphinxupquote{torch.cuda.manual\_seed}}）。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{manual\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{48}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}torch.\PYGZus{}C.Generator at 0x7fe88887dfb0\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a function to generate x and y}
\PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{coef}\PYG{p}{,}
                  \PYG{n}{intercept} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{coef} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{coef}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}feature} \PYG{o}{=} \PYG{n}{coef}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{n}{mean\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{n\PYGZus{}feature}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{e} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}residual}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{coef} \PYG{o}{=} \PYG{n}{coef}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{intercept} \PYG{o}{+} \PYG{n}{x} \PYG{o}{@} \PYG{n}{coef} \PYG{o}{+} \PYG{n}{e}
    \PYG{k}{return} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run generate\PYGZus{}data}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{coef} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{intercept} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{feature matrix x is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{response vector y is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
feature matrix x is 
 [[10.86425232  2.85930239 12.99748957]
 [ 7.98709525  9.79137811 12.98202577]
 [ 8.40585824 13.31645195  4.51125723]
 [10.52843635 10.28388618  8.43102705]
 [11.25624976 12.04005459 12.59301479]
 [12.31145327  9.15153143  7.96220487]
 [11.49807312 14.63609048  6.82995236]
 [11.7234386   7.16098399  9.8980089 ]
 [ 7.24735628  7.5570991   5.9154331 ]
 [11.05035931 10.47430089  4.86376669]]
response vector y is 
 [[\PYGZhy{}42.98404441]
 [ \PYGZhy{}4.94309144]
 [  0.31840093]
 [\PYGZhy{}16.21407503]
 [\PYGZhy{}13.77391657]
 [\PYGZhy{}31.18360077]
 [ \PYGZhy{}8.4871242 ]
 [\PYGZhy{}31.53590804]
 [ \PYGZhy{}7.8563037 ]
 [\PYGZhy{}19.27007283]]
\end{sphinxVerbatim}


\subsection{計算模型參數}
\label{\detokenize{notebook/lab-torch-tensor:id15}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a function to calculate model parameter}
\PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}parameter}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n}{dtype}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n}{dtype}\PYG{p}{:}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{u} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{x\PYGZus{}design} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{u}\PYG{p}{,} \PYG{n}{x}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{parameter} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}
        \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x\PYGZus{}design}\PYG{p}{)} \PYG{o}{@} \PYGZbs{}
                \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{y}
    \PYG{n}{intercept} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{coef} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{intercept}\PYG{p}{,} \PYG{n}{coef}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run calculate\PYGZus{}parameter}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{,}
                     \PYG{n}{coef} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{intercept} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n}{intercept}\PYG{p}{,} \PYG{n}{coef} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}parameter}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{intercept estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{intercept}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{coefficient estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{coef}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
intercept estimate is 
 4.996612423766793
coefficient estimate is 
 [\PYGZhy{}4.98003037  2.98762589 \PYGZhy{}0.00737968]
\end{sphinxVerbatim}


\subsection{建立一進行迴歸分析之物件}
\label{\detokenize{notebook/lab-torch-tensor:id16}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a class to fit linear regression}
\PYG{k}{class} \PYG{n+nc}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{intercept} \PYG{o}{=} \PYG{k+kc}{None}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{coef} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{k}{def} \PYG{n+nf}{fit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{n}{u} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{n}{x\PYGZus{}design} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{u}\PYG{p}{,} \PYG{n}{x}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{parameter} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}
            \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x\PYGZus{}design}\PYG{p}{)} \PYG{o}{@} \PYGZbs{}
                    \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{y}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{intercept} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{coef} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{k}{return} \PYG{n+nb+bp}{self}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{linear\PYGZus{}regression} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{linear\PYGZus{}regression}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{linear\PYGZus{}regression}\PYG{o}{.}\PYG{n}{intercept}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{linear\PYGZus{}regression}\PYG{o}{.}\PYG{n}{coef}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4.996612423766793
[\PYGZhy{}4.98003037  2.98762589 \PYGZhy{}0.00737968]
\end{sphinxVerbatim}


\chapter{邏輯斯迴歸}
\label{\detokenize{notebook/logistic-regression:id1}}\label{\detokenize{notebook/logistic-regression::doc}}
邏輯斯迴歸（logistic regression）與線性迴歸相似，都是透過一線性函數 \(f(x)\) 以描述兩變項 \(x\) 與 \(y\) 之間的關係，但不同之處在於邏輯斯迴歸考慮的 \(y\) 為類別變項，其類別數為2，此外，\(f(x)\) 與 \(y\) 之間的關係，需再透過一邏輯斯（logistic）函數進行轉換。邏輯斯迴歸可說是統計領域最基本之二元分類（binary classification）方法，其可視為線性迴歸於分類問題上之拓展。

在此主題中，我們將會學習以下的重點：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
使用伯努利分配（bernoulli distribution）刻畫類別變數之隨機行為。

\item {} 
使用最大概似法（maximum likelihood method，簡稱ML法），對邏輯斯迴歸模型參數進行估計。

\item {} 
利用數值優化（numerical optimization）的技術，對函數逐步地進行優化以求解。

\end{enumerate}


\section{邏輯斯迴歸模型}
\label{\detokenize{notebook/logistic-regression:id2}}
廣義來說，二元分類之問題關注的是如何使用一 \(P\) 維之向量 \(x\)，對於二元變項 \(y\) 進行預測，這裡，\(y\)的數值只能為0或1，即\(y \in \{0,1\}\)，\(y=1\) 表示觀測值屬於某一類，而 \(y=0\)則表示觀測值屬於另外一類。在二元分類的問題下，研究者常試圖刻畫在給定 \(x\) 之下，\(y\) 的條件機率（conditional probability），即：
\begin{equation*}
\begin{split}\mathbb{P}(y|x)=\frac{\mathbb{P}(y,x)}{\mathbb{P}(x)}\end{split}
\end{equation*}
這裡，\(\mathbb{P}(y,x)\) 表示同時考慮 \(x\) 與 \(y\) 的聯合機率（joint probability），而 \(\mathbb{P}(x)\) 則為僅考慮 \(x\) 之邊際機率（marginal probability）。在此講義中，我們將簡單的使用\(\pi_1(x)\)與\(\pi_0(x)\)來表示在給定 \(x\) 之下，\(y=1\) 與 \(y=0\) 之條件機率，即
\begin{equation*}
\begin{split}\pi_1(x) =\mathbb{P}(y=1|x)\end{split}
\end{equation*}
與
\begin{equation*}
\begin{split}\pi_0(x) =\mathbb{P}(y=0|x)\end{split}
\end{equation*}
令\(f(x)=\beta_0 + \sum_{p=1}^P \beta_p x_p\)表示一線性函數，\(\beta_p\)與\(\beta_0\)分別表示迴歸係數與截距，邏輯斯迴歸試圖使用一邏輯斯函數來刻畫\(f(x)\)與\(\pi_1(x)\)的關聯性，即
\begin{equation*}
\begin{split}\pi_1(x) = \frac{\exp{ \left[ f(x) \right] }}{1+\exp{ \left[ f(x) \right] }}\end{split}
\end{equation*}
由於 \(\pi_1(x)\) 與 \(\pi_0(x)\) 兩者的和須為1，因此，\(\pi_0(x)\) 則可寫為
\begin{equation*}
\begin{split}\pi_1(x) = \frac{1}{1+\exp{ \left[ f(x) \right] }}\end{split}
\end{equation*}
透過邏輯斯迴歸模型的結構，我們可以觀察到以下兩件事情：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(\pi_1(x)\) 與 \(\pi_0(x)\) 兩者之數值皆介於0到1之間，符合機率的公理（axiom）。

\item {} 
當 \(f(x)\) 數值大時，\(\pi_1(x)\) 的數值將很靠近1，意味著獲得 \(y=1\) 的可能性很大，反之，\(\pi_0(x)\) 的數值則較大，獲得 \(y=0\) 的可能性較高。

\end{enumerate}

在迴歸係數的解讀方面，\(\beta_p\) 越大，表示 \(x_p\) 對於獲得 \(y=1\) 有較大的影響，反之，則對 \(y=0\) 有較大的影響，然而，\(\beta_p\) 影響的具體效果則不容易解讀，一般來說，需透過比較給定 \(x\) 下的對數勝率（log\sphinxhyphen{}odds）進行解讀：
\begin{equation*}
\begin{split}\begin{aligned}
\log \left[ \frac{\pi_1(x)}{\pi_0(x)} \right]
=& \log \left\{ \frac{\frac{\exp{ \left[ f(x) \right] }}{1+\exp{ \left[ f(x) \right] }}}{\frac{1}{1+\exp{ \left[ f(x) \right] }}} \right\} \\
=& \log \left\{ \exp \left[ f(x) \right] \right\} \\
=& f(x)  \\
=& \beta_0 + \sum_{p=1}^P \beta_p x_p
\end{aligned}\end{split}
\end{equation*}
因此，\(\beta_p\) 可解讀為當 \(x_p\) 每變動一個單位時，預期對數勝率跟著變動的單位。


\section{最大概似估計法}
\label{\detokenize{notebook/logistic-regression:id3}}

\section{數值優化技術與求解}
\label{\detokenize{notebook/logistic-regression:id4}}

\chapter{Lab: 數值微分與優化}
\label{\detokenize{notebook/lab-torch-diff-opt:lab}}\label{\detokenize{notebook/lab-torch-diff-opt::doc}}
在此 lab 中，我們將介紹
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
如何使用 \sphinxcode{\sphinxupquote{torch}} 進行數值微分。

\item {} 
如何使用 \sphinxcode{\sphinxupquote{torch}} 進行數值優化。

\item {} 
利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}


\section{數值微分}
\label{\detokenize{notebook/lab-torch-diff-opt:id1}}

\subsection{可獲得梯度張量之輸入}
\label{\detokenize{notebook/lab-torch-diff-opt:id2}}
而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first\sphinxhyphen{}order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。

在 \sphinxcode{\sphinxupquote{torch}} 中，張量不僅用於儲存資料，其亦用於儲存模型之參數。然而，誠如先前所述，我們很可能會需要用到對應於該參數之梯度訊息，因此，為了追朔該參數的歷史建立計算圖（computation graph），輸入該參數張量時需要加入 \sphinxcode{\sphinxupquote{requires\_grad=True}} 此指令：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([1., 2., 3.], requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

這裏，我們建立了一尺寸為 \(3\) 的張量，由於此張量具有 \sphinxcode{\sphinxupquote{requires\_grad=True}} 此標記，因此，接下來對此張量進行任何的運算，\sphinxcode{\sphinxupquote{torch}} 皆會將此計算過程記錄下來。舉例來說：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor y: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor z: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor y: 
 tensor([\PYGZhy{}2.,  0.,  2.], grad\PYGZus{}fn=\PYGZlt{}SubBackward0\PYGZgt{})
tensor z: 
 tensor(8., grad\PYGZus{}fn=\PYGZlt{}SumBackward0\PYGZgt{})
\end{sphinxVerbatim}

我們可以看到，無論是 \sphinxcode{\sphinxupquote{y}} 或是 \sphinxcode{\sphinxupquote{z}}，其都具有 \sphinxcode{\sphinxupquote{requires\_grad=True}} 的標記。要特別注意的是，\sphinxcode{\sphinxupquote{requires\_grad=True}} 僅適用於資料類型為浮點數之張量。


\subsection{數值微分之執行}
\label{\detokenize{notebook/lab-torch-diff-opt:id3}}
針對已追朔之運算過程，想要獲得與該運算有關的梯度時，可以使用 \sphinxcode{\sphinxupquote{.backward()}}此方法。在前一小節的例子中，\(z = \sum_{i=1}^3 (2x_{i} - 4)^2\)，若想要獲得 \(\frac{\partial z}{\partial x}\) 在當下 \(x\) 的數值的話，可使用以下的程式碼：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}8.,  0.,  8.])
\end{sphinxVerbatim}

接著，由於 \(z\) 也可以寫為 \(z = \sum_{i=1}^3 y_{i}^2\)，因此，我們是否也可以透過類似的程式碼獲得 \(\frac{\partial z}{\partial y}\) 呢？

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dy:  None
\end{sphinxVerbatim}
\begin{sphinxalltt}
/Users/phhaung/Documents/PycharmProject/tism/venv/lib/python3.8/site\sphinxhyphen{}packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won\textquotesingle{}t be populated during autograd.backward(). If you indeed want the gradient for a non\sphinxhyphen{}leaf Tensor, use .retain\_grad() on the non\sphinxhyphen{}leaf Tensor. If you access the non\sphinxhyphen{}leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "

\end{sphinxalltt}

結果是不行，主因在於，\sphinxcode{\sphinxupquote{torch}} 為了節省記憶體的使用，因此，僅可提供位於計算圖葉子（leaf）張量之一次微分。如果希望能夠獲得 \(\frac{\partial z}{\partial y}\) 的話，可以對 \sphinxcode{\sphinxupquote{y}} 使用 \sphinxcode{\sphinxupquote{.retain\_grad()}} 此方法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{retain\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dy:  tensor([\PYGZhy{}4.,  0.,  4.])
\end{sphinxVerbatim}

在評估完 \(\frac{\partial z}{\partial y}\) 後，讓我們重新檢視一下 \sphinxcode{\sphinxupquote{x.grad}} 的數值：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}16.,   0.,  16.])
\end{sphinxVerbatim}

我們會發現，這時的 \sphinxcode{\sphinxupquote{x.grad}} 數值，變成了原先的兩倍，其背後的原因在於，\sphinxcode{\sphinxupquote{.backward()}}此方法，會持續地將計算結果累積在變數所對應之 \sphinxcode{\sphinxupquote{.grad}} 當中。若想要避免持續累積，可以使用 \sphinxcode{\sphinxupquote{.grad.zero\_()}} 方法將 \sphinxcode{\sphinxupquote{.grad}} 中的數值歸零：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([0., 0., 0.])
dz/dx:  tensor([0., 0., 0.])
\end{sphinxVerbatim}

接著，就可以使用原先的程式碼，計算 \(\frac{\partial z}{\partial x}\) 與 \(\frac{\partial z}{\partial y}\)：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{retain\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}8.,  0.,  8.])
dz/dy:  tensor([\PYGZhy{}4.,  0.,  4.])
\end{sphinxVerbatim}

不過要特別注意的是，如果計算圖沒有重新建立，連續進行兩次 \sphinxcode{\sphinxupquote{.backward()}} 會引發錯誤的訊息。


\subsection{可獲得梯度張量之進階控制}
\label{\detokenize{notebook/lab-torch-diff-opt:id4}}
一個張量是否有被追朔以計算梯度，除了直接列印外，亦可透過 \sphinxcode{\sphinxupquote{.requires\_grad}} 此屬性來觀看

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}

如果想將一原先沒有要求梯度之張量，改為需要梯度時，可以使用 \sphinxcode{\sphinxupquote{.requires\_grad\_()}} 此方法原地修改該向量的 \sphinxcode{\sphinxupquote{requires\_grad}} 類型：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad\PYGZus{}}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
True
\end{sphinxVerbatim}

如果想將張量 \sphinxcode{\sphinxupquote{x}} 拷貝到另一變量 \sphinxcode{\sphinxupquote{x\_ng}}，卻不希望 \sphinxcode{\sphinxupquote{x\_ng}} 的計算會被追朔時，可以使用以下的程式碼：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x\PYGZus{}ng} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x\PYGZus{}ng}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}

最後，如果希望可獲得梯度之向量後續的計算歷程不被追朔的話，可以將計算程式碼置於 \sphinxcode{\sphinxupquote{with torch.no\_grad():}} 此環境中，即

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
    \PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{y}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
False
\end{sphinxVerbatim}


\section{數值優化}
\label{\detokenize{notebook/lab-torch-diff-opt:id5}}

\subsection{手動撰寫優化算則}
\label{\detokenize{notebook/lab-torch-diff-opt:id6}}
前一小節所使用的範例，其計算過程可以寫為
\begin{equation*}
\begin{split}
\begin{aligned}
z &= f(x)\\
 &= \sum_{i=1}^3\left[2(x_i-2)\right]^2 \\
 &= \sum_{i=1}^3 y^2
\end{aligned}
\end{split}
\end{equation*}
若想要找到一 \(\widehat{x}\)，其使得 \(f(\widehat{x})\) 達到最小值的話，由於 \(z\) 為 \(y_1, y_2, y_3\) 的平方和，因此，其會在 \(\widehat{y} = (0, 0, 0)\)的地方達到最小值，也意味著 \(\widehat{x} = (2,2,2)\)。

那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 \(\theta\) 表示模型參數（其扮演範例中\(x\)的角色），\(\mathcal{D}(\theta)\) 表示度量模型好壞的目標函數（其扮演\(f(x)\)的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）\(\widehat{\theta}\) 的更新規則為
\begin{equation*}
\begin{split}
\widehat{\theta} \leftarrow \widehat{\theta} - s \times \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta}
\end{split}
\end{equation*}
這裏，\(s\) 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 \(\mathcal{D}\) 足夠圓滑（smooth），且 \(s\) 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 \(\mathcal{D}\) 最小值的發生位置。

在開始進行梯度下降前，我們先定義一函數 \sphinxcode{\sphinxupquote{f}} 使得\sphinxcode{\sphinxupquote{z = f(x)}}，並了解起始狀態時，\sphinxcode{\sphinxupquote{f(x)}} 與 \sphinxcode{\sphinxupquote{x}} 的數值為何：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{z}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
f(x) = 8.000, x = tensor([1., 2., 3.])
\end{sphinxVerbatim}

使用 \sphinxcode{\sphinxupquote{torch}} 進行梯度下降，需先計算在當下 \sphinxcode{\sphinxupquote{x}} 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 \sphinxcode{\sphinxupquote{x}} 進行更新，即

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
\end{sphinxVerbatim}

這裡，我們將學習速率 \sphinxcode{\sphinxupquote{lr}} 設為0.1，而張量的 \sphinxcode{\sphinxupquote{.sub\_()}} 方法則是就地減去括號內的數值直接更新。透過 \sphinxcode{\sphinxupquote{f(x)}} 的數值，可觀察到梯度下降的確導致 \sphinxcode{\sphinxupquote{z}} 數值的下降，而 \sphinxcode{\sphinxupquote{x}} 也與 \(\widehat{x}=(2,2,2)\) 更加地靠近。

梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 \sphinxcode{\sphinxupquote{for}} 迴圈，重複更新\(I\)次：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{21}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, f(x) = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, f(x) = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, f(x) = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, f(x) = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, f(x) = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 11, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 12, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 13, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 14, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 15, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 16, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 17, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 18, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 19, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 20, f(x) = 0.000, x = tensor([2., 2., 2.])
\end{sphinxVerbatim}

然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 \sphinxcode{\sphinxupquote{tol}} 時，算則就停止，其所對應之程式碼為：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tol} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}5}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{21}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, z = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{tol}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
\end{sphinxVerbatim}


\subsection{使用\sphinxstyleliteralintitle{\sphinxupquote{torch.optim}}進行優化}
\label{\detokenize{notebook/lab-torch-diff-opt:torch-optim}}
由於 \sphinxcode{\sphinxupquote{torch}} 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 \sphinxcode{\sphinxupquote{torch.optim}} 的類型來求得函數的最小值。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 為進行梯度下降法之物件，由於 \sphinxcode{\sphinxupquote{torch}} 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini\sphinxhyphen{}batch gradient descent）來強化優化的效能，因此，\sphinxcode{\sphinxupquote{torch}} 使用 \sphinxcode{\sphinxupquote{SGD}} 一詞。事實上，除了計算一階導數時資料量的差異外，\sphinxcode{\sphinxupquote{SGD}} 與傳統的梯度下降並無差異。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 可透過以下的程式碼來使用：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{21}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, z = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{tol}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
\end{sphinxVerbatim}

這裡，我們使用 \sphinxcode{\sphinxupquote{torch.optim.SGD}} 來生成優化器（optimizer）物件 \sphinxcode{\sphinxupquote{opt}}，其在生成時，需要指定其追朔的變量，並以一可迭代的物件（iterable object）作為輸入，因此，在該程式碼中，我們將 \sphinxcode{\sphinxupquote{x}} 以一元組（tuple）的方式來輸入，並指定學習速率 \sphinxcode{\sphinxupquote{lr=.1}}。使用內建優化器時，仍須手動對目標函數執行 \sphinxcode{\sphinxupquote{.backward()}}，但更新估計值的步驟，可使用優化器的 \sphinxcode{\sphinxupquote{.step()}} 來進行，而消除變量的 \sphinxcode{\sphinxupquote{.grad}}，則可使用優化器的 \sphinxcode{\sphinxupquote{.zero\_grad()}} 方法。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 容許使用者加入動能（momentum）\(m\)（其預設為 0），此時，優化算則會改為
\begin{equation*}
\begin{split}
\begin{aligned}
\delta & \leftarrow m \times  \delta + \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta} \\
\widehat{\theta} &\leftarrow \widehat{\theta} - s \times \delta
\end{aligned}
\end{split}
\end{equation*}
這裡，\(\delta\) 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。

在 \sphinxcode{\sphinxupquote{torch.optim.SGD}} 中，有許多不同的優化器（見\sphinxhref{https://pytorch.org/docs/stable/optim.html}{\sphinxcode{\sphinxupquote{torch.optim}}頁面}），如
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{Adadelta}}（見\sphinxhref{https://arxiv.org/abs/1212.5701}{ADADELTA: An Adaptive Learning Rate Method}）

\item {} 
\sphinxcode{\sphinxupquote{Adagrad}}（見\sphinxhref{https://jmlr.org/papers/v12/duchi11a.html}{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}）

\item {} 
\sphinxcode{\sphinxupquote{Adam}}（見\sphinxhref{https://arxiv.org/abs/1412.6980}{Adam: A Method for Stochastic Optimization}）

\item {} 
\sphinxcode{\sphinxupquote{LBFGS}}（見\sphinxhref{https://doi.org/10.1007/BF01589116}{On the limited memory BFGS method for large scale optimization}）

\item {} 
\sphinxcode{\sphinxupquote{Adadelta}}（見\sphinxhref{https://arxiv.org/abs/1212.5701}{ADADELTA: An Adaptive Learning Rate Method}）

\item {} 
\sphinxcode{\sphinxupquote{RMSprop}}（見\sphinxhref{https://arxiv.org/abs/1308.0850}{Generating Sequences With Recurrent Neural Networks}）

\end{itemize}


\chapter{機率分佈}
\label{\detokenize{notebook/probability-distribution:id1}}\label{\detokenize{notebook/probability-distribution::doc}}

\chapter{最大概似法}
\label{\detokenize{notebook/maximum-likelihood:id1}}\label{\detokenize{notebook/maximum-likelihood::doc}}

\chapter{Lab: 最大概似估計}
\label{\detokenize{notebook/lab-torch-mle:lab}}\label{\detokenize{notebook/lab-torch-mle::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Normal}
\PYG{n}{normal} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with shape ():
 tensor(\PYGZhy{}0.4205)
random sample with shape (3,):
 tensor([\PYGZhy{}1.3577,  0.2594, \PYGZhy{}0.3872])
random sample with shape (2,3):
 tensor([[\PYGZhy{}0.3462,  0.0221,  0.5617],
        [\PYGZhy{}1.1793,  0.1680, \PYGZhy{}0.2688]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cumulative probability given value with shape ():
 tensor(0.5000) 

cumulative probability given value with (3,):
 tensor([0.1587, 0.5000, 0.6915]) 

cumulative probability given value with (2,3):
 tensor([[0.1587, 0.5000, 0.6915],
        [0.0228, 0.8413, 0.9987]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cumulative probability given value with shape ():
 tensor(\PYGZhy{}0.9189) 

cumulative probability given value with (3,):
 tensor([\PYGZhy{}1.4189, \PYGZhy{}0.9189, \PYGZhy{}1.0439]) 

cumulative probability given value with (2,3):
 tensor([[\PYGZhy{}1.4189, \PYGZhy{}0.9189, \PYGZhy{}1.0439],
        [\PYGZhy{}2.9189, \PYGZhy{}1.4189, \PYGZhy{}5.4189]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Normal(loc: 0.0, scale: 1.0)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{o}{.}\PYG{n}{batch\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{o}{.}\PYG{n}{event\PYGZus{}shape}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Size([])
torch.Size([])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{normal\PYGZus{}batch} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Normal(loc: torch.Size([2]), scale: torch.Size([2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([0.3196, 2.1923]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[ 0.0940,  0.1338],
        [\PYGZhy{}0.3400,  2.4002],
        [\PYGZhy{}1.0516, \PYGZhy{}0.1826]]) 

random sample with sample\PYGZus{}shape (2,3):
 tensor([[[ 0.1398,  4.7928],
         [\PYGZhy{}0.1455, \PYGZhy{}0.5193],
         [\PYGZhy{}1.4244, \PYGZhy{}0.6089]],

        [[\PYGZhy{}0.1050,  2.6404],
         [\PYGZhy{}1.6125,  1.1516],
         [ 0.2476,  1.0377]]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,1):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
log\PYGZhy{}probability given value with shape ():
 tensor([\PYGZhy{}0.9189, \PYGZhy{}1.5466]) 

log\PYGZhy{}probability given value with shape (2,):
 tensor([\PYGZhy{}0.9189, \PYGZhy{}1.5466]) 

log\PYGZhy{}probability given value with shape (2,1):
 tensor([[\PYGZhy{}0.9189, \PYGZhy{}1.5466],
        [\PYGZhy{}0.9189, \PYGZhy{}1.5466]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{MultivariateNormal}
\PYG{n}{mvn} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{mvn}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
MultivariateNormal(loc: torch.Size([2]), scale\PYGZus{}tril: torch.Size([2, 2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2, 3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([2.2865, 0.6410]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[\PYGZhy{}0.1174,  0.8089],
        [ 1.7223,  2.2532],
        [ 0.7753,  2.0000]]) 

random sample with sample\PYGZus{}shape (2, 3):
 tensor([[[\PYGZhy{}0.0497,  2.2984],
         [\PYGZhy{}0.5064,  0.9095],
         [ 0.2189,  1.6590]],

        [[ 0.5981,  1.3462],
         [ 1.7386,  0.7478],
         [\PYGZhy{}0.4982,  1.1488]]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,1):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
log\PYGZhy{}probability given value with shape (2,):
 tensor(\PYGZhy{}2.4913) 

log\PYGZhy{}probability given value with shape (2,1):
 tensor([\PYGZhy{}2.4913, \PYGZhy{}2.4913])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Independent}
\PYG{n}{normal\PYGZus{}batch} \PYG{o}{=} \PYG{n}{Independent}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{p}{,} \PYG{n}{reinterpreted\PYGZus{}batch\PYGZus{}ndims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{batch\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{event\PYGZus{}shape}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Size([])
torch.Size([2])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mvn\PYGZus{}batch} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{mvn\PYGZus{}batch}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
MultivariateNormal(loc: torch.Size([3, 2]), scale\PYGZus{}tril: torch.Size([3, 2, 2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2, 3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([[1.1275, 1.7382],
        [0.5327, 2.4657],
        [2.4923, 2.9444]]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[[ 1.7485e\PYGZhy{}03,  1.1704e+00],
         [\PYGZhy{}7.7522e\PYGZhy{}01,  1.0061e+00],
         [ 3.8714e+00,  2.9748e+00]],

        [[ 3.0326e\PYGZhy{}01,  4.6701e\PYGZhy{}01],
         [\PYGZhy{}3.1945e\PYGZhy{}01,  3.0077e\PYGZhy{}01],
         [ 3.7882e+00,  2.7624e+00]],

        [[\PYGZhy{}1.1375e+00,  3.3560e\PYGZhy{}01],
         [ 7.5519e\PYGZhy{}01,  2.2413e+00],
         [ 3.0676e+00,  3.7033e+00]]]) 

random sample with sample\PYGZus{}shape (2, 3):
 tensor([[[[\PYGZhy{}0.8206,  0.4010],
          [ 1.1678,  2.1332],
          [ 1.0008,  2.4261]],

         [[\PYGZhy{}0.2140, \PYGZhy{}0.2980],
          [ 1.1422,  2.5305],
          [ 1.4001,  3.7645]],

         [[ 0.9449,  0.4744],
          [ 1.0263,  3.2151],
          [ 1.9302,  3.2841]]],


        [[[ 1.6932,  1.7799],
          [ 0.8284,  1.8066],
          [ 3.0099,  3.1478]],

         [[ 0.3715,  0.4171],
          [ 0.2260,  1.6781],
          [ 1.0694,  2.1480]],

         [[\PYGZhy{}0.0609,  0.9179],
          [ 1.5631,  2.2728],
          [ 3.8856,  3.7695]]]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mu\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{5.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{2.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{normal\PYGZus{}model\PYGZus{}true} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu\PYGZus{}true}\PYG{p}{,}
    \PYG{n}{scale}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}true}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{normal model:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{normal\PYGZus{}model\PYGZus{}true}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
normal model:
 Normal(loc: tensor([5.]), scale: tensor([2.])) 

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{normal\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{normal\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{negative likelihood value is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{loss\PYGZus{}value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
negative likelihood value is tensor(2.1462)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{lr} \PYG{o}{=} \PYG{l+m+mf}{1.0}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sigma} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{[}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{normal\PYGZus{}model} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{sigma}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{normal\PYGZus{}model}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute the gradient}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by gradient descent:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML std by gradient descent:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by gradient descent: tensor([4.8263], requires\PYGZus{}grad=True)
ML std by gradient descent: tensor([2.0574], requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by formula:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML std by formula:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{unbiased}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by formula: tensor(4.8261)
ML std by formula: tensor(2.0598)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mu\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}tril\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{3.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{mvn\PYGZus{}model\PYGZus{}true} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu\PYGZus{}true}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}tril\PYGZus{}true}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true mean vector: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mvn\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true covariance matrix: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mvn\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{covariance\PYGZus{}matrix}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
true mean vector: 
 tensor([\PYGZhy{}1.,  0.,  1.])
true covariance matrix: 
 tensor([[9.0000, 6.0000, 1.2000],
        [6.0000, 5.0000, 1.3000],
        [1.2000, 1.3000, 0.6600]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{mvn\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{mvn\PYGZus{}model\PYGZus{}true}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{negative likelihood value is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{loss\PYGZus{}value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
negative likelihood value is tensor(4.6442)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{lr} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}
    \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}tril} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}
    \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{[}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma\PYGZus{}tril}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n}{lr}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{mvn\PYGZus{}model} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}tril}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{mvn\PYGZus{}model}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute the gradient}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by gradient descent: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mu}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML covariance by gradient descent: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sigma\PYGZus{}tril} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{sigma\PYGZus{}tril}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by gradient descent: 
 tensor([\PYGZhy{}0.9729, \PYGZhy{}0.0022,  0.9979], requires\PYGZus{}grad=True)
ML covariance by gradient descent: 
 tensor([[9.5867, 6.1855, 1.1623],
        [6.1855, 4.9734, 1.2497],
        [1.1623, 1.2497, 0.6229]], grad\PYGZus{}fn=\PYGZlt{}MmBackward\PYGZgt{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}mean} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}moment2} \PYG{o}{=} \PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x}\PYG{p}{)} \PYG{o}{/} \PYG{n}{sample\PYGZus{}size}
\PYG{n}{sample\PYGZus{}cov} \PYG{o}{=} \PYG{n}{sample\PYGZus{}moment2} \PYG{o}{\PYGZhy{}} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ger}\PYG{p}{(}\PYG{n}{sample\PYGZus{}mean}\PYG{p}{,} \PYG{n}{sample\PYGZus{}mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by formula: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sample\PYGZus{}mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML covariance by formula: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sample\PYGZus{}cov}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by formula: 
 tensor([\PYGZhy{}0.9729, \PYGZhy{}0.0022,  0.9979])
ML covariance by formula: 
 tensor([[9.5868, 6.1855, 1.1624],
        [6.1855, 4.9734, 1.2497],
        [1.1624, 1.2497, 0.6229]])
\end{sphinxVerbatim}


\chapter{概似推論}
\label{\detokenize{notebook/likelihood-inference:id1}}\label{\detokenize{notebook/likelihood-inference::doc}}

\chapter{真實分數模型}
\label{\detokenize{notebook/true-score-model:id1}}\label{\detokenize{notebook/true-score-model::doc}}

\chapter{因素分析}
\label{\detokenize{notebook/factor-analysis:id1}}\label{\detokenize{notebook/factor-analysis::doc}}

\chapter{試題反應理論}
\label{\detokenize{notebook/item-response-theory:id1}}\label{\detokenize{notebook/item-response-theory::doc}}

\chapter{混合建模}
\label{\detokenize{notebook/mixture-modeling:id1}}\label{\detokenize{notebook/mixture-modeling::doc}}






\renewcommand{\indexname}{Index}
\printindex
\end{document}