%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}




\title{統計建模技法}
\date{Sep 11, 2020}
\release{}
\author{Po\sphinxhyphen{}Hsien Huang}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


「心理計量學」（psychometrics）為一探討如何使用數量模型，刻畫人類外顯行為與內在歷程之學門。典型的「心理計量模型」（psychometric model）包含「因素分析」（factor analysis）、「試題反應理論」（item response theory）等試圖刻畫潛在構念（latent constructs）與觀察指標（observed indicators）間關係之方法。因此，潛在構念（或稱潛在變項）的出現，可說是「心理計量模型」的一大特色，而如何處理潛在變項對於統計推論與心理學研究帶來的挑戰，則為「心理計量學」的核心議題。

「心理計量學」可視為「統計學」與「心理學」兩學門領域的結合，因此，在進行「心理計量學」研究時，除了心理學的知識外，也需要具備以下三類的技能：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
數理技能（mathematical skills），包括線性代數（linear algebra）、微積分（calculus）、以及機率（probability）。

\item {} 
統計理論（statistical theory），包括建立估計（estimation）與推論（inference）程序的技術。

\item {} 
程式設計（programming），包括如何在電腦上進行線性代數之計算、數值微分與積分、以及機率物件的操弄。

\end{enumerate}

從個人的角度，「統計學」可分為「建模（modeling）」與「推論（inference）」兩個議題。「建模」指的是如何使用數量模型來合理地刻畫現象（如使用線性或是二次式來描述兩變項間的關係），而「推論」指的是如何考慮到資料的隨機性以對模型參數進行合理之詮釋（如對迴歸係數進行估計與檢定）。儘管在理論統計（theoretical statistics）的領域，仍不斷地有新的「推論」方法被發展出來，但在統計的實務應用時，研究者大多採用相對老舊的「推論」方法，但透過不同的「建模」策略來對資料背後的科學現象以更深入的了解。因此，「建模」可說是當代量化研究中很重要的一個面向。

此頁面放置了黃柏僩老師所撰寫的「統計建模技法」（Techniques in Statistical Modeling，TISM）講義，其旨在介紹「統計建模」所需的基本技術，以及如何使用概似函數（likelihood function）對模型進行評估。此外，本講義也將以「心理計量學」常使用之模型作為範例，並搭配python的深度學習套件PyTorch進行實作。

此講義主要用於訓練非統計背景的量化方法主修研究生，講義本身並無法達到「自給自足」（self\sphinxhyphen{}contained）的目標，因此，對於交代不清楚的部分，我會盡可能地附上參考文獻或是外部連結，讀者可以自行將該缺失的部分補足。

在閱讀此講義之前，讀者需具有以下之先備知識：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
數理基礎，包括向量（vector）與矩陣（matrix）之運算規則、微分（differentiation）與積分（integral）之幾何意義、機率分配與期望值（expectation）。

\item {} 
統計學的基礎，包括平均數、變異數、相關係數之計算，估計、假設檢定（hypothesis testing）、與信賴區間（confidence interval）之概念。

\item {} 
python程式設計基礎，包括基本資料類型、流程控制（flow control）、函數等概念。若無基礎，Jake VanderPlask 的 \sphinxhref{https://jakevdp.github.io/WhirlwindTourOfPython}{A Whirlwind Tour of Python} 會是一很好的學習資源。

\end{enumerate}

讀者不需精熟這些先備知識，但至少須知道這些概念的定義，以及不害怕深入了解這些概念。

若發現講義的內容有誤，請將勘誤寄至 psyphh@gmail.com，感謝！


\chapter{先備數學知識}
\label{\detokenize{notebook/mathematics-prerequisite:id1}}\label{\detokenize{notebook/mathematics-prerequisite::doc}}

\section{向量}
\label{\detokenize{notebook/mathematics-prerequisite:id2}}

\subsection{何謂向量？}
\label{\detokenize{notebook/mathematics-prerequisite:id3}}
在進行統計建模時，一筆觀測值（observation）常透過\sphinxstylestrong{向量}（vector）來表徵。令 \(x\) 表示一 \(N\) 維之直行向量（column vector），則其包含了 \(x_1,x_2,...,x_N\) 共 \(N\) 個元素（element），按順序由上至下排列而成，即
\begin{equation*}
\begin{split}
x =
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_N
\end{pmatrix}
\end{split}
\end{equation*}
在此講義中，我們通常使用大寫的英文字母作為一組數列的元素個數，而該字母的小寫則作為索引使用，因此，\(x\) 的元素個素為 \(N\)，我們用\(x_n\) 來表示 \(x\) 的第 \(n\) 個元素。

我們可以將 \(x\) 進行\sphinxstylestrong{轉置}（transpose），將其轉為一橫列向量（row vector），即
\begin{equation*}
\begin{split}
x^T =
\begin{pmatrix}
x_1 & x_2 & \cdots & x_N
\end{pmatrix}
\end{split}
\end{equation*}
透過轉置，原本由上而下的排列，改成有左至右的排列。不過，請記得在文獻中向量一詞，大多指稱的是直行向量。

有時為了節省呈現空間，會將一直行向量寫為 \(x = (x_1, x_2,...,x_N)\)，利用逗點來分隔不同的上下欄位。注意，這跟橫列向量利用空格將元素左右分隔的做法是不同的。

一個向量\sphinxstylestrong{長度}（length），可透過其\sphinxstylestrong{範數}（norm）來獲得。範數有許多種定義方式，不過，最常見的為 \(L_2\) 範數，即
\begin{equation*}
\begin{split}
\begin{aligned}
||x|| &= \sqrt{x_1^2 + x_2^2 +...+x_N^2}\\
& = \sqrt{ \sum_{n=1}^N x_n^2}
\end{aligned}
\end{split}
\end{equation*}
此式被稱作 \(L_2\) 範數的原因在於，其使用了二次方來處理每一項。從公式中可以看出來，唯有當 \(x\) 的所有成分皆為0時，其範數才會是0。

當一向量的範數為1時，我們會說其為標準化的向量（normalized vector），給定任何長度不為0的向量 \(x\)，我們可以透過以下公式對其進行標準化
\begin{equation*}
\begin{split}
x^* = \frac{1}{||x||} x =
\begin{pmatrix}
\frac{1}{||x||} x_1 \\
\frac{1}{||x||} x_2 \\
\vdots \\
\frac{1}{||x||} x_N
\end{pmatrix}
\end{split}
\end{equation*}
這邊牽涉到純量對向量的乘法，請見下一小節。


\subsection{向量的運算}
\label{\detokenize{notebook/mathematics-prerequisite:id4}}
令 \(x\) 與 \(y\) 表示兩 \(N\) 維之向量，則我們可以將向量的加法與減法分別定義為
\begin{equation*}
\begin{split}
\begin{aligned}
x  + y=
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_N
\end{pmatrix}
+
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{pmatrix}
=
\begin{pmatrix}
x_1 + y_1 \\
x_2 + y_2 \\
\vdots \\
x_N + y_N
\end{pmatrix}
\end{aligned}
\end{split}
\end{equation*}
與
\begin{equation*}
\begin{split}
\begin{aligned}
x  - y=
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_N
\end{pmatrix}
-
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{pmatrix}
=
\begin{pmatrix}
x_1 - y_1 \\
x_2 - y_2 \\
\vdots \\
x_N - y_N
\end{pmatrix}
\end{aligned}
\end{split}
\end{equation*}
即所謂元素對元素的（element to element）加法與減法。

依循相同的邏輯，我們可定義所謂元素對元素的乘法與除法，不過，在線性代數（linear algebra）此一學門中，甚少直接使用此類的運算子。相較之下，純量對向量之乘法則較常使用。令 \(\alpha\) 表示一純量（scalar），純量對向量之乘法定義為
\begin{equation*}
\begin{split}
\begin{aligned}
\alpha x =
\alpha
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_N
\end{pmatrix}
=
\begin{pmatrix}
\alpha x_1 \\
\alpha x_2 \\
\vdots \\
\alpha x_N
\end{pmatrix}
\end{aligned}
\end{split}
\end{equation*}

\subsection{距離、內積、與餘弦值}
\label{\detokenize{notebook/mathematics-prerequisite:id5}}
在度量兩向量是否相似時，最基本的做法是計算兩者之間的\sphinxstylestrong{距離}（distance），即
\begin{equation*}
\begin{split}
\begin{aligned}
d(x, y) &= ||x - y|| \\
&=
\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_N - y_N)^2} \\
&=
\sqrt{\sum_{n=1}^N (x_n - y_n)^2 }
\end{aligned}
\end{split}
\end{equation*}
前述根據 \(L_2\) 範數計算的距離，亦稱做 \(L_2\) 距離，或是歐幾里德距離（Euclidean distance）。從公式中可以看出來，\(x\) 和 \(y\) 唯有在所有元素都相等的前提之下，其距離才會等於0。當 \(x\) 與 \(y\) 的內積為0時，我們會說 \(x\) 與 \(y\) 為垂直（orthogonal），表示兩向量在 \(N\) 為空間中，呈現90度的夾角，兩垂直的向量常被解讀為其具有獨立未重疊的訊息。

另外一種度量兩向量是否相似的做法是，計算其\sphinxstylestrong{內積}（inner product），即
\begin{equation*}
\begin{split}
\begin{aligned}
\langle x,y \rangle &=
x_1 y_1 + x_2 y_2 + ... + x_N y_N \\
& = \sum_{n=1}^N x_n y_n
\end{aligned}
\end{split}
\end{equation*}
當 \(x\) 與 \(y\) 個元素間存在同時大同時小的關係時，兩者的內積會很大，若存在一個大另一個小的關係時，則內積會很小（指的是存在負號的很小），若未存在前述的組型時，則內積會靠近0。

有時，\(x\) 與 \(y\) 的內積會簡單的寫為 \(x^T y\)，這與下一小節會提到的矩陣乘法有關。

然而，內積並未考慮到 \(x\) 和 \(y\) 自身的長度，其數值大小較難直接做解釋。故此，令\(\theta\) 表示兩向量之夾角，則其\sphinxstylestrong{餘弦值}（cosine）之計算，乃將兩向量之內積除上各自的長度，即
\begin{equation*}
\begin{split}
\text{cos}(\theta) = \frac{\langle x,y \rangle}{||x|| ||y||}
\end{split}
\end{equation*}
當兩向量夾角的餘弦值，可透過以下的方式解釋：
\begin{itemize}
\item {} 
\(\text{cos}(\theta)\) 靠近1時，表示兩向量在同一方向上，相似性高。

\item {} 
\(\text{cos}(\theta)\) 靠近\sphinxhyphen{}1時，表示兩向量在相反方向上，相似性亦高，但存在相反的關係。

\item {} 
\(\text{cos}(\theta)\) 靠近0時，表示兩向量靠近垂直的關係，相似性低。

\end{itemize}

不過，餘弦值大多在向量內部數值皆為正的情境下作為相似性指標（即所謂的第一象限），此時，\(0 \leq\text{cos}(\theta) \leq 1\)，不需考慮 \(-1 \leq\text{cos}(\theta) \leq 0\) 的情況。


\section{矩陣}
\label{\detokenize{notebook/mathematics-prerequisite:id6}}

\subsection{何謂矩陣}
\label{\detokenize{notebook/mathematics-prerequisite:id7}}
若把 \(M\) 個 \(N\) 維的橫列向量由上至下排列，則可形成一尺寸為 \(M \times N\) 之\sphinxstylestrong{矩陣}（matrix）。這裡，\(M\) 為矩陣的橫列（row）個數，\(N\) 則為矩陣的直行（column）個數。

舉例來說，令 \(a_1, a_2,...,a_M\) 皆表示 \(N\) 維之向量，則我們可以將其排為一矩陣 \(A\)
\begin{equation*}
\begin{split}
A =
\begin{pmatrix}
  a_{1}^T \\
   a_{2}^T \\
    \vdots \\
     a_{M}^T \\
 \end{pmatrix}\end{split}
\end{equation*}
注意，在這邊，我們有過度使用符號的狀況，在前一小節，\(a_n\)用於表示向量 \(a\) 的第 \(n\) 個元素，但在這邊，\(a_m\) 被用於表示第 \(m\) 個 \(N\) 維之向量，讀者應嘗試理解以具備獨立判斷的能力。

若我們將前述的矩陣 \(A\) 每一個元素寫開來，則可以表示為
\begin{equation*}
\begin{split}
A =
 \begin{pmatrix}
  a_{1,1} & a_{1,2} & \cdots & a_{1,N} \\
  a_{2,1} & a_{2,2} & \cdots & a_{2,N} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{M,1} & a_{M,2} & \cdots & a_{M,N}
 \end{pmatrix}
\end{split}
\end{equation*}
這裡，\(A\) 的第 \((m,n)\) 個元素，我們使用 \(a_{m,n}\)（或\(a_{mn}\)）來表示。

給定一尺寸為 \(M \times N\)之矩陣 \(A\)，其第 \((m,n)\) 個元素為 \(a_{mn}\)，則 \(A\) 的\sphinxstylestrong{轉置}（transpose）被定義為
\begin{equation*}
\begin{split}
A^T =
 \begin{pmatrix}
  a_{11} & a_{21} & \cdots & a_{N1} \\
  a_{12} & a_{22} & \cdots & a_{N2} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{1M} & a_{2M} & \cdots & a_{NM}
 \end{pmatrix}
\end{split}
\end{equation*}
因此，\(A^T\) 可視為將 \(A\) 的直行與橫列訊息互換的結果，而 \(A^T\) 的尺寸則轉為了 \(N \times M\)。

舉例來說，若矩陣 \(A =  \begin{pmatrix}
  1 & 2 & 3  \\
  4 & 5 & 6
 \end{pmatrix}\)為一\(2 \times 3\)的矩陣，則其轉置為一 \(3 \times 2\) 之矩陣 \(A^T =  \begin{pmatrix}
  1 & 4 \\
  2 & 5 \\
  3 & 6
 \end{pmatrix}\)

在矩陣的世界中，有幾種特別的矩陣讀者需要特別認識：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
當 \(A\) 的尺寸為 \(M \times M\)時，則 \(A\) 被稱作\sphinxstylestrong{方陣}（square matrix）。

\item {} 
當 \(A\) 為方陣，且進一步滿足 \(A = A^T\) 時，則其稱作\sphinxstylestrong{對稱矩陣}（symmetric matrix）。

\item {} 
當 \(A\) 為方陣，且其只有對角線左下角（可包含對角線）之元素不為0時，其稱作\sphinxstylestrong{下三角矩陣}（lower triangular matrix），反之，若只有對角線右上角之元素不為0時，則其稱作\sphinxstylestrong{上三角矩陣}（upper triangular matrix）。

\item {} 
當 \(A\) 為方陣，且其只有對角線元素不為0時，則其稱作\sphinxstylestrong{對角矩陣}（diagonal matrix）。

\item {} 
當 \(A\) 為方陣，且其僅有對角線元素數值為1，其它為0時，則其稱作\sphinxstylestrong{單位矩陣}（identity matrix）。其在矩陣的世界中，扮演像是純量1的角色。

\end{enumerate}

這幾個特別的矩陣，在計算矩陣乘法、反矩陣、或是拆解時，可能會有一些好的特性讓計算變得比較容易。


\subsection{矩陣的運算}
\label{\detokenize{notebook/mathematics-prerequisite:id8}}
令 \(A\) 與 \(B\) 皆表示 \(M \times N\) 之矩陣，其內部元素分別為 \(a_{mn}\) 與 \(b_{mn}\)，則矩陣加減法，即 \(A \pm B\)，被定義元素對元素的加減法：
\begin{equation*}
\begin{split}
A  + B=
 \begin{pmatrix}
  a_{11} \pm b_{11} & a_{12}\pm  b_{12} & \cdots & a_{1N}\pm  b_{1N} \\
  a_{21} \pm  b_{21} & a_{22}\pm  b_{22} & \cdots & a_{2N}\pm  b_{2N} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{M1} \pm  b_{M1}& a_{M2}\pm  b_{M2} & \cdots & a_{MN}\pm  b_{MN}
 \end{pmatrix}
\end{split}
\end{equation*}
由於此為元素對元素的運算，因此，\(A\) 和 \(B\) 兩者的尺寸必須一樣。

既然可以計算元素對元素的加減法，自然也可以同樣定義元素對元素的乘除法，不過，線性代數的領域依然很少直接用到此運算子。

\sphinxstylestrong{矩陣乘法}（matrix multiplication）在統計建模中則是相當的關鍵。令 \(A\) 表示一 \(M \times N\) 之矩陣，\(x\) 表示一 \(N\) 維之向量，或視為一 \(N \times 1\) 的矩陣，則矩陣對向量的乘法被定義為
\begin{equation*}
\begin{split}
\begin{aligned}
Ax &=  \underbrace{\begin{pmatrix}
  a_{11} & a_{12} & \cdots & a_{1N} \\
  a_{21} & a_{22} & \cdots & a_{2N} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{M1} & a_{M2} & \cdots & a_{MN}
 \end{pmatrix}}_{M \times N}
\underbrace{\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_N
\end{pmatrix}}_{N \times 1} \\
&=
\underbrace{\begin{pmatrix}
  a_{11} x_1 + a_{12} x_2 + ... + a_{1N} x_N \\
  a_{21} x_1 + a_{22} x_2 + ... + a_{2N} x_N \\
  \vdots \\
  a_{M1} x_1 + a_{M2} x_2 + ... + a_{MN} x_N
 \end{pmatrix}}_{M \times 1}\\
&=
\begin{pmatrix}
  \sum_{n=1}^N a_{1n} x_n \\
  \sum_{n=1}^N a_{2n} x_n\\
  \vdots \\
  \sum_{n=1}^N a_{Mn} x_n
 \end{pmatrix}
\end{aligned}
\end{split}
\end{equation*}
在此，我們可以觀察到兩件事情
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(x\) 的維度，必須跟 \(A\) 的直行個數相等，此矩陣向量之乘法才能夠被合法定義。

\item {} 
\(A\) 和 \(x\) 相乘後的結果乃一 \(M\) 維之向量，其亦可理解為 \(M \times 1\) 之矩陣。

\item {} 
\(Ax\) 的第 \(m\) 個元素，可以視為 \(A\) 的第 \(m\) 個橫列所形成之向量，對 \(x\) 做內積的結果。

\end{enumerate}

因此，若我們將 \(A\) 寫為 \(A =
\begin{pmatrix}
  a_1^T \\
  a_2^T  \\
  \vdots \\
  a_{M}^T
 \end{pmatrix}\)，這裡，\(a_m^T\) 表示由 \(A\) 的第 \(m\) 個橫列形成的橫列向量，則 \(Ax\) 可以表示為
\begin{equation*}
\begin{split}
 Ax
 = \begin{pmatrix}
   a_1^Tx  \\
   a_2^Tx  \\
  \vdots \\
   a_M^Tx
 \end{pmatrix}
 = \begin{pmatrix}
  \langle a_1,x \rangle \\
  \langle a_2,x \rangle \\
  \vdots \\
  \langle a_M,x \rangle
 \end{pmatrix}
 \end{split}
\end{equation*}
我們可延伸前述的概念，來定義矩陣對矩陣的乘法。令 \(A\) 與 \(B\) 分別表示 \(M \times N\) 與 \(N \times P\) 之矩陣，其內部元素分別為 \(a_{mn}\) 與 \(b_{np}\)，則 \(A\) 與 \(B\) 相乘被定義為
\begin{equation*}
\begin{split}
\begin{aligned}
AB &=
 \underbrace{\begin{pmatrix}
  a_{11} & a_{12} & \cdots & a_{1N} \\
  a_{21} & a_{22} & \cdots & a_{2N} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{M1} & a_{M2} & \cdots & a_{MN}
 \end{pmatrix}}_{M \times N}
  \underbrace{\begin{pmatrix}
  b_{11} & b_{12} & \cdots & b_{1P} \\
  b_{21} & b_{22} & \cdots & b_{2P} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  b_{N1} & b_{N2} & \cdots & b_{NP}
 \end{pmatrix}}_{N \times P} \\
  &=
  \underbrace{\begin{pmatrix}
  \sum_{n=1}^N a_{1n} b_{n1} & \sum_{n=1}^N a_{1n} b_{n2} & \cdots & \sum_{n=1}^N a_{1n} b_{nP} \\
  \sum_{n=1}^N a_{2n} b_{n1} & \sum_{n=1}^N a_{2n} b_{n2} & \cdots & \sum_{n=1}^N a_{2n} b_{nP} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sum_{n=1}^N a_{Nn} b_{n1} & \sum_{n=1}^N a_{Nn} b_{n2} & \cdots & \sum_{n=1}^N a_{Nn} b_{nP}
 \end{pmatrix}}_{M \times P}
 \end{aligned}
\end{split}
\end{equation*}
其相乘之結果為一 \(N \times P\) 之矩陣。

前述的公式，不太容易看出來矩陣乘法的結構，因此，我們將 \(A\) 與 \(B\) 重新寫成以下的結構
\begin{equation*}
\begin{split}
A =
\begin{pmatrix}
  a_1^T \\
  a_2^T  \\
  \vdots \\
  a_{M}^T
 \end{pmatrix},
 B =
\begin{pmatrix}
  b_1 & b_2  & \cdots & b_{P}
 \end{pmatrix}
\end{split}
\end{equation*}
這裡，\(a_m^T\) 表示由 \(A\) 的第 \(m\) 個橫列形成的橫列向量，\(b_p\) 表示由 \(B\) 的第 \(p\) 個直行形成的直行向量，則 \(AB\) 可寫為
\begin{equation*}
\begin{split}
AB =
  \begin{pmatrix}
  a_1^T b_1 & a_1^T b_2 & \cdots & a_1^T b_P \\
  a_2^T b_1 & a_2^T b_2 & \cdots & a_2^T b_P \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_M^T b_1 & a_M^T b_2 & \cdots & a_M^T b_P \\
 \end{pmatrix}
\end{split}
\end{equation*}

\subsection{反矩陣}
\label{\detokenize{notebook/mathematics-prerequisite:id9}}
當 \(A\) 為 \(N \times N\)之方陣，且其行或列並未包含累贅的（redundant）訊息時，則 \(A\) 存在\sphinxstylestrong{反矩陣}（inverse）。意即，存在一 \(N \times N\) 矩陣 \(A^{-1}\)，其滿足
\begin{equation*}
\begin{split}
A^{-1} A = A A^{-1} = I
\end{split}
\end{equation*}
這裡 \(I\) 為 \(N \times N\) 單位矩陣。當反矩陣存在時，其為獨特的（unique），意思是，反矩陣只會有一個。

反矩陣的計算常與 \(N\) 元一次方程組的求解有關。考慮以下的聯立方程組
\begin{equation*}
\begin{split}
\begin{cases}
a_{11} x_1 + a_{12} x_2 + ... + a_{1N} x_N = b_1 \\
a_{21} x_1 + a_{22} x_2 + ... + a_{2N} x_N = b_2 \\
\vdots \\
a_{N1} x_1 + a_{N2} x_2 + ... + a_{NN} x_N = b_N \\
\end{cases}
\end{split}
\end{equation*}
在此方程組中，有\(x_1, x_2,...,x_N\) 共 \(N\) 個未知數，並有 \(N\) 條式子。令\(A\)表示由 \(a_{mn}\)組成的 \(N \times N\) 矩陣（\(1 \leq m,n\leq N\)），\(x\) 與 \(b\) 則表示由 \(x_n\) 與 \(b_n\) 形成的 \(N\) 維向量。前述方程組可以用矩陣向量乘法的形式來表徵
\begin{equation*}
\begin{split}
Ax=b
\end{split}
\end{equation*}
因此，若可以獲得 \(A^{-1}\)，則根據 \(A^{-1}Ax=A^{-1}b\)，方程組的解即為 \(x=A^{-1}b\)。

反矩陣的計算主要仰賴電腦程式，採用\sphinxhref{https://en.wikipedia.org/wiki/Gaussian\_elimination}{\sphinxstylestrong{高斯消去法}}（Gaussian elimination）或是\sphinxhref{https://en.wikipedia.org/wiki/QR\_decomposition}{\sphinxstylestrong{QR分解}}（QR decomposition），這兩種方法的計算複雜度皆為 \(O(N^3)\)，意即，其牽涉到大約 \(K \times N^3\) 這麼多步驟的計算，這裡，\(K\) 表示一跟 \(N\) 無關的常數，因此，當 \(N\) 很大時，此反矩陣的計算可能會很耗時。

而讀者需要特別注意的是，究竟 \(A\) 是否真的存在反矩陣，\(A\) 存在反矩陣的條件為，其 \(N\) 個直行（或橫列）所形成的向量，並不存在線性相依（linear dependent）的狀況。令 \(a_1, a_2, ..., a_N\) 表示 \(A\) 的 \(N\) 個直行所對應之向量，若其中存在某 \(a_i\)，其可以寫為 \(a_i = \sum_{n \neq i} w_n a_n\) 的話，則表示有線性相依的狀況。這裡，\(w_n\) 為純量，用於表示一權重係數。事實上，線性相依的問題意味著在該聯立方程組的系統中，存在了多餘訊息，各位聰明的讀者，應該都知道此時存在無窮多組解，故不存在 \(A^{-1}\)。


\subsection{分解}
\label{\detokenize{notebook/mathematics-prerequisite:id10}}
在矩陣的世界中，存在許多將矩陣拆成幾個比較簡單矩陣乘積的\sphinxstylestrong{分解}（decomposition），我們在這邊做個簡單的介紹（這邊只考慮實數矩陣，不考慮虛數）。

a. \(LU\) 分解

當 \(A\) 為 \(N \times N\) 可逆方陣時，則其存在 \sphinxstylestrong{\(LU\) 分解}：
\begin{equation*}
\begin{split}
A = LU
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(L\)表示一 \(N \times N\) 下三角矩陣。

\item {} 
\(U\)表示一 \(N \times N\) 上三角矩陣。

\end{itemize}

b. \(QR\) 分解

當 \(A\) 為 \(N \times N\) 可逆方陣時，則其存在 \sphinxstylestrong{\(QR\) 分解}：
\begin{equation*}
\begin{split}
A = QR
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(Q\) 表示一 \(N \times N\) 的\sphinxstylestrong{垂直矩陣}（orthogonal matrix）（垂直矩陣滿足 \(Q Q^T = Q^T Q = I\)）。

\item {} 
\(R\)則表示一 \(N \times N\) 上三角矩陣。

\end{itemize}

c. Cholesky 分解

當 \(A\) 為 \(N \times N\) 正定對稱矩陣（positive definite symmetric matrix）時（在此，我們先不對「正定」一詞多做解釋，其在矩陣中扮演類似正數的概念），則其存在 \sphinxstylestrong{Cholesky 分解}：
\begin{equation*}
\begin{split}
A = LL^T
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(L\)表示一 \(N \times N\) 下三角矩陣，\(L^T\) 表示 \(L\) 的轉置

\end{itemize}

d. 特徵值分解

當 \(A\) 為 \(N \times N\) 對稱矩陣時，其存在以下之\sphinxstylestrong{特徵值分解}（eigendecomposition）
\begin{equation*}
\begin{split}
A = Q \Lambda Q ^T
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(\Lambda\) 表示一 \(N \times N\) 對角矩陣，其對角線元素為 \(A\) 的特徵值。

\item {} 
\(Q\) 表示一 \(N \times N\) 的垂直矩陣，其第 \(n\) 個直行表示 \(\Lambda\) 第 \(n\) 個元素所對應之特徵向量。

\end{itemize}

e. 奇異值分解

對於任何的 \(M \times N\)矩陣 \(A\)，其存在\sphinxstylestrong{奇異值分解}（singular value decomposition）：
\begin{equation*}
\begin{split}
A = U \Sigma V^T
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\(\Sigma\) 表示一 \(M \times N\) 的長方對角矩陣（rectangular diagonal matrix），其對角線元素為 \(A\) 的\sphinxstylestrong{奇異值}（singular value）

\item {} 
\(U\) 表示一 \(M \times M\) 的垂直矩陣。

\item {} 
\(V\) 表示一 \(N \times N\) 的垂直矩陣。

\end{itemize}


\section{微分}
\label{\detokenize{notebook/mathematics-prerequisite:id11}}
\sphinxstylestrong{微分}（differentiation）乃微積分（calculus）此科目中，用於瞭解函數局部訊息的重要技術。在統計領域，此技術常用於逼近目標函數，以及求得參數解有關。

若讀者對於微分的概念很不熟悉的話，可以參考台灣大學開放式課程的\sphinxhref{http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/103S121}{影片}，特別是單元5跟6，以及28和29的內容。


\subsection{何謂微分？}
\label{\detokenize{notebook/mathematics-prerequisite:id12}}
給定一函數 \(f(x)\)，其將一實數變量 \(x\)，從其\sphinxstylestrong{定義域}（domain）透過轉換（transformation）送至其\sphinxstylestrong{值域}（range）。若 \(f(x)\) 在 \(x=x^*\) 此位置為\sphinxstylestrong{可微分}（differentiable）的話，我們使用 \(f'(x^*)\) 來表示此微分值，其計算方式為
\begin{equation*}
\begin{split}
f'(x^*) = \lim_{\Delta x \to 0} \frac{f(x^* + \Delta x) - f(x^*)}{\Delta x}
\end{split}
\end{equation*}
由於 \(\frac{f(x^* + \Delta x) - f(x^*)}{\Delta x}\) 可視為對函數 \(f\) 於 \(x^*\) 位置斜率之逼近，透過使用極限（limit）的運算將 \(\Delta x\) 趨近於0，事實上，\(f'(x^*)\) 表徵的就是 \(f\) 於 \(x^*\) 位置切線之斜率。

實務上，我們常使用以下的符號來表示對函數 \(f\) 的 \(x\) 進行微分
\begin{equation*}
\begin{split}
f'(x) = \frac{\text{d} f(x)}{\text{d} x}
\end{split}
\end{equation*}
注意，在這邊 \(f'(x)\) 與 \(f'(x^*)\) 表徵的事情略有不同。\(f'(x)\) 表徵的是函數 \(f\) 的\sphinxstylestrong{一階導數}（first\sphinxhyphen{}order derivative），其為一函數，帶入不同的 \(x\) 則 \(f'(x)\) 會輸出不同的數值來，而 \(f'(x^*)\) 強調的是，該一階導數於 \(x = x^*\) 之數值。一般來說，比較精確的寫法是
\begin{equation*}
\begin{split}
f'(x^*) = \frac{\text{d} f(x)}{\text{d} x} \bigg|_{x = x^*}
\end{split}
\end{equation*}
下表呈現了一些常見函數的一階導數


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
函數名稱
&\sphinxstyletheadfamily 
函數形式
&\sphinxstyletheadfamily 
一階導數
\\
\hline
常數函數
&
\(f(x)=c\)
&
\(f'(x)=0\)
\\
\hline
單位函數
&
\(f(x)=x\)
&
\(f'(x)=1\)
\\
\hline
多項式函數
&
\(f(x)=x^K\)
&
\(f'(x)=K x^{K-1}\)
\\
\hline
對數函數
&
\(f(x)=\log(x)\)
&
\(f'(x)=\frac{1}{x}\)
\\
\hline
指數函數
&
\(f(x)=\exp(x)\)
&
\(f'(x)=\exp(x)\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

在獲得了 \(f(x)\) 的一階導數 \(f'(x)\) 後，我們也可以將 \(f'(x)\) 視為一新的函數再去計算其導數，此時，我們等同於計算 \(f(x)\) 的二階導數
\begin{equation*}
\begin{split}
f''(x) = \frac{\text{d}^2 f(x)}{\text{d} x^2} = \frac{\text{d} f'(x)}{\text{d} x}
\end{split}
\end{equation*}

\subsection{微分的規則}
\label{\detokenize{notebook/mathematics-prerequisite:id13}}
前一小節的表格，呈現的都是一些較為簡單形式的函數，面對實務的問題，研究者常須處理較為複雜的函數，這時，我們會需要一些微分的規則。令 \(f(x)\) 與 \(g(x)\) 表示兩定義於 \(x\) 此變數之函數，假設 \(f(x)\) 與 \(g(x)\) 皆為可微分之函數，則我們有以下的規則
\begin{itemize}
\item {} 
\sphinxstylestrong{線性規則}：\(\frac{\text{d} }{\text{d}x} ( a f(x) + b g(x)) = a f'(x) + b g'(x)\)

\item {} 
\sphinxstylestrong{乘法規則}：\(\frac{\text{d} }{\text{d}x} (f(x) g(x)) = f'(x) g(x) + f(x) g'(x)\)

\item {} 
\sphinxstylestrong{除法規則}：\(\frac{\text{d} }{\text{d}x} (f(x)/g(x)) = \frac{f'(x) g(x) - f(x) g'(x)}{[g(x)]^2}\)（\(g(x)\) 不可為0）

\end{itemize}

除此之外，\sphinxstylestrong{連鎖規則}（chain rule）亦為一相當重要的手法。令 \(g(x)\) 表示一定義於 \(x\) 變數的函數，其輸出為 \(y\)，而 \(f(y)\) 表示一定義於變數 \(y\) 之函數，其輸出為 \(z\)。考慮一組成函數（composition function）\(h\)，其建構方式為\(h =f \circ g\)，意思是，\(h(x) = f(g(x))\)。根據連鎖規則，\(h\) 的一階導數可透過以下的公式計算
\begin{equation*}
\begin{split}
h'(x) = f'(g(x))  g'(x)
\end{split}
\end{equation*}

\subsection{多變數函數的微分}
\label{\detokenize{notebook/mathematics-prerequisite:id14}}
在統計的問題中，研究者常須處理多變數的實函數，意即，一函數\(f\) 其定義於 \(x_1,x_2,...,x_N\)，並透過一轉換將此 \(N\) 個變數送到一實數空間。在面對此類函數時，我們需要利用\sphinxstylestrong{偏微分}（partial differentiation）的技術，計算 \(f(x)=f(x_1,x_2,...,x_N)\) 於各 \(x_n\) 方向上的\sphinxstylestrong{偏導數}（partial derivative），其定義為
\begin{equation*}
\begin{split}
\frac{\partial  f(x)}{\partial x_n}  = \lim_{\Delta x_n \to 0} \frac{f(x_1,...,x_n + \Delta x_n,...,x_N) - f(x_1,...,x_n,...,x_N)}{\Delta x_n}
\end{split}
\end{equation*}
在實作上，計算 \(\frac{\partial f(x)}{\partial x_n}\)時，僅須將 \(f(x)\) 視為 \(x_n\) 的函數，其它的變數視為常數即可。

在獲得 \(f(x)\) 對於每個 \(x_n\) 的偏導數後，我們可以將這些導數收集起來排成一 \(N\) 維之向量，即
\begin{equation*}
\begin{split}
\nabla f(x) =
\begin{pmatrix}
\frac{\partial  f(x)}{\partial x_1} \\
\frac{\partial  f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial  f(x)}{\partial x_N} \\
\end{pmatrix}
\end{split}
\end{equation*}
我們將 \(\nabla f(x)\) 稱作\sphinxstylestrong{梯度}（gradient），其在了解多變數函數時，提供了相當重要的訊息：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
梯度的第 \(n\) 個成分，表徵了該函數於 \(x_n\) 方向上切線的斜率訊息。

\item {} 
給定一具體的方向 \(d\)，則該函數於方向 \(d\) 的方向導數（directional derivative）可以寫為 \(\langle \nabla f(x),d/||d|| \rangle\)。

\end{enumerate}


\chapter{線性迴歸}
\label{\detokenize{notebook/linear-regression:id1}}\label{\detokenize{notebook/linear-regression::doc}}
線性迴歸（linear regression）可說是統計建模的基礎，其試圖建立一線性函數（linear function），以描述兩變項 \(x\) 與 \(y\) 之間的關係。這裡，\(x=(x_1,x_2,..., x_P)\)為一 \(P\) 維之向量，其常被稱獨變項（independent variable）、共變量（covariate），或是特徵（feature），而 \(y\) 則為一純量（scalar），其常被稱作依變項（dependent variable）或是反應變項（response variable）。

在此主題中，我們將會學習以下的重點：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
使用線性函數刻畫兩變項間的關係。

\item {} 
使用最小平方法（least squares method）來建立參數之估計準則。

\item {} 
使用一階導數（derivative）來刻畫最小平方估計值（estimate）之最適條件（optimality condition）

\item {} 
使用線性代數來解決線性迴歸之問題。

\end{enumerate}


\section{線性迴歸模型}
\label{\detokenize{notebook/linear-regression:id2}}
廣義來說，迴歸分析試圖使用一 \(P\) 維之向量 \(x\)，對於 \(y\) 進行預測，其假設 \(x\) 與 \(y\) 存在以下的關係
\begin{equation*}
\begin{split}y=f(x)+\epsilon\end{split}
\end{equation*}
這裡，\(f(x)\) 表示一函數，其描述了 \(x\) 與 \(y\) 系統性的關係，而 \(\epsilon\) 則表示一隨機誤差，其平均數為0，變異數為 \(\sigma_{\epsilon}^2\)，即 \(\epsilon \sim (0,\sigma_{\epsilon}^2)\)。

線性迴歸模型假設 \(f(x)\) 為一線性函數，即
\begin{equation*}
\begin{split}f(x) =\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_P x_P\end{split}
\end{equation*}
這裡，\(\beta_p\) 為 \(x_p\) 所對應之迴歸係數（regression coefficient），其亦被稱作權重（weight），反映 \(x_p\) 每變動一個單位時，預期 \(y\) 跟著變動的量，\(\beta_0\) 則稱作截距（intercept），亦稱作偏誤（bias），其反映當 \(x_1,x_2,..,x_P\)皆為0時，我們預期 \(y\) 的數值。利用連加的符號，\(f(x)\)可簡單的寫為
\begin{equation*}
\begin{split}f(x) = \beta_0 + \sum_{p=1}^P \beta_p x_p\end{split}
\end{equation*}
無論是迴歸係數 \(\beta_p\) 或是截距 \(\beta_0\)，由於其刻畫了 \(f(x)\) 的形狀，故其皆被稱作模型參數（model parameter），文獻中常簡單以一 \(P+1\) 維之向量 \(\beta = (\beta_0, \beta_1, ..., \beta_P)\) 來表示模型中的所有參數。線性迴歸分析的主要目的乃透過一樣本資料，獲得對於 \(\beta\) 之估計 \(\widehat{\beta}\)，一方面對於各參數 \(\widehat{\beta}_p\) 進行推論，二方面則是使用 \(\widehat{f}(x) = \widehat{\beta}_0 + \sum_{p=1}^P \widehat{\beta}_p x_p\) 對 \(y\) 進行預測。


\section{最小平方估計法}
\label{\detokenize{notebook/linear-regression:id3}}
線性迴歸分析主要採用最小平方法（least squares method，簡稱 LS 法）以對模型參數進行估計。令\((x_n, y_n)\) 表示第 \(n\) 位個體於 \(x\) 與 \(y\) 之觀測值，則給定一隨機樣本 \(\{(x_n, y_n) \}_{n=1}^N\)，LS 估計準則可寫為
\begin{equation*}
\begin{split}\begin{aligned}
\mathcal{D}(\beta)
= &\frac{1}{N} \sum_{n=1}^N \left (y_n - \beta_0 - \sum_{p=1}^P \beta_p x_{np} \right )^2
\end{aligned}\end{split}
\end{equation*}
由於迴歸模型假設 \(y=f(x)+\epsilon\)，且線性迴歸僅考慮線性的關係 \(f(x) = \beta_0 + \sum_{p=1}^P \beta_p x_p\)，因此，第 \(n\) 筆觀測值對應之殘差可寫為 \(\epsilon_n = y_n - f(x_n)\)，LS 估計準則亦可簡單地寫為
\($\begin{aligned}
\mathcal{D}(\beta)
= &\frac{1}{N} \sum_{n=1}^N \epsilon_n^2
\end{aligned}\)\$

LS估計法的目標在於找到一估計值 \(\widehat{\beta} = (\widehat{\beta}_0, \widehat{\beta}_1, ..., \widehat{\beta}_P)\)，其最小化 LS 估計準則，意即，\(\widehat{\beta}\) 可最小化樣本資料中所有殘差的平方和。


\section{一階導數與最適條件}
\label{\detokenize{notebook/linear-regression:id4}}

\section{線性代數與迴歸}
\label{\detokenize{notebook/linear-regression:id5}}
線性迴歸的問題可以簡單的使用矩陣與向量的方式來表徵
\begin{equation*}
\begin{split}y = X \beta + \epsilon,\end{split}
\end{equation*}\begin{equation*}
\begin{split}\mathop{\begin{pmatrix}
  y_{1} \\
  y_{2} \\
  \vdots \\
  y_{N}
 \end{pmatrix}}_{N \times 1}=
\mathop{\begin{pmatrix}
  1 & x_{11} & \cdots & x_{1P} \\
  1 & x_{21} & \cdots & x_{2P} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  1 & x_{N1} & \cdots & x_{NP}
 \end{pmatrix}}_{N \times (P+1)}
\mathop{\begin{pmatrix}
  \beta_{0} \\
  \beta_{1} \\
  \vdots \\
  \beta_{P}
 \end{pmatrix}}_{(P+1) \times 1}+
 \mathop{\begin{pmatrix}
  \epsilon_{1} \\
  \epsilon_{2} \\
  \vdots \\
  \epsilon_{N}
 \end{pmatrix}}_{N \times 1}.\end{split}
\end{equation*}
在這邊我們於符號使用上有稍微偷懶， \(y\) 與 \(\epsilon\) 在這用於表徵 \(N\) 維的向量。在前式的表徵下，LS 估計準則可以寫為
\begin{equation*}
\begin{split}\begin{aligned}
\mathcal{D}(\beta) & =  \frac{1}{N} \sum_{n=1}^N \epsilon_n^2 \\
  & = \frac{1}{N} \epsilon^T \epsilon\\
 & =  \frac{1}{N} (y-X\beta)^T(y-X\beta).
\end{aligned}\end{split}
\end{equation*}
透過對 \(\beta\) 的每一個成分做偏微分，我們可以得到刻畫 LS 解的一階條件
\begin{equation*}
\begin{split}\begin{aligned}
\frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta}&=
\begin{pmatrix}
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_0}  \\
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_1} \\
   \vdots \\
  \frac{\partial \mathcal{D}(\widehat{\beta})}{\partial \beta_P}
 \end{pmatrix} \\
&=-\frac{2}{N} X^T(y-X\widehat{\beta})=0.
\end{aligned}\end{split}
\end{equation*}
此純量對向量（scalar by vector）的微分的計算，可按照定義對各個 \(\beta_p\) 進行微分後，再利用矩陣乘法之特性獲得。除此之外，亦可以參考矩陣微分（matrix calculus）中，對於\sphinxhref{https://en.wikipedia.org/wiki/Matrix\_calculus\#Scalar-by-vector\_identities}{純量對向量微分之規則}。\(-\frac{2}{N} X^T(y-X\widehat{\beta})=0\) 意味著 \(\widehat{\beta}\) 需滿足以下的等式
\begin{equation*}
\begin{split}
X^T X\widehat{\beta}=X^Ty.
\end{split}
\end{equation*}
因此，若 \(X^T X\) 存在反矩陣，則迴歸係數的 LS 估計值可寫為
\begin{equation*}
\begin{split}
\widehat{\beta} = (X^T X)^{-1} X^Ty
\end{split}
\end{equation*}
確保 \(X^T X\) 存在反矩陣的數學條件為其各直行向量（column vector）間並未存在線性相依（linear dependence）的狀況。所謂向量間有線性相依指的是某個向量，可以寫為其它向量的線性組合。一般來說，當樣本數大於變項數（\(N > P\)），各變項間變異數皆大於0，且皆存在其獨特的訊息時，\(X^T X\) 為可逆的。

\((X^T X)^{-1}\) 的計算，常採用\sphinxhref{https://en.wikipedia.org/wiki/Gaussian\_elimination}{高斯消去法}（Gaussian elimination）或是\sphinxhref{https://en.wikipedia.org/wiki/QR\_decomposition}{QR分解}（QR decomposition），這兩種方法的計算複雜度皆為 \(O(P^3)\)，意即，其牽涉到大約 \(K \times P^3\) 這麼多步驟的計算，這裡，\(K\) 表示一跟 \(P\) 無關的常數，因此，當 \(P\) 很大時，此反矩陣的計算可能會很耗時。


\chapter{Lab: 張量與線性代數}
\label{\detokenize{notebook/lab-torch-tensor:lab}}\label{\detokenize{notebook/lab-torch-tensor::doc}}
此 lab 中，我們將會透過 \sphinxcode{\sphinxupquote{torch}} 此套件，學習以下的主題。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
認識 \sphinxcode{\sphinxupquote{torch}} 的張量（tensor）之基礎。

\item {} 
了解如何對 \sphinxcode{\sphinxupquote{torch}} 張量進行操弄。

\item {} 
使用 \sphinxcode{\sphinxupquote{torch}} 進行線性代數之運算。

\item {} 
應用前述之知識，建立一可進行線性迴歸分析之類型（class）。

\end{enumerate}

\sphinxcode{\sphinxupquote{torch}}之安裝與基礎教學，可參考 \sphinxhref{https://pytorch.org/get-started/locally}{PyTorch官方網頁}。在安裝完成後，可透過以下的指令載入

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}


\section{張量之基礎}
\label{\detokenize{notebook/lab-torch-tensor:id1}}

\subsection{張量之輸入}
\label{\detokenize{notebook/lab-torch-tensor:id2}}
\sphinxcode{\sphinxupquote{torch}} 最基本的物件是張量（tensor），其與 \sphinxcode{\sphinxupquote{numpy}} 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 \sphinxcode{\sphinxupquote{python}} 的 \sphinxcode{\sphinxupquote{list}} 或是 \sphinxcode{\sphinxupquote{numpy}} 的 \sphinxcode{\sphinxupquote{ndarray}}），置於\sphinxcode{\sphinxupquote{torch.tensor}}函數中

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Tensor
\end{sphinxVerbatim}

透過 \sphinxcode{\sphinxupquote{type()}}，可看見其屬於 \sphinxcode{\sphinxupquote{torch.Tensor}} 此一類型（class），若欲了解 \sphinxcode{\sphinxupquote{a}} 的樣貌，我們可使用 \sphinxcode{\sphinxupquote{print}} 指令來列印其主要的內容

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
\end{sphinxVerbatim}

透過對 \sphinxcode{\sphinxupquote{a}} 列印的結果，我們可觀察到：
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{a}} 內部的資料數值（value）為 \sphinxcode{\sphinxupquote{{[}{[}1, 2, 3, 4{]}, {[}5, 6, 7, 8{]}, {[}9, 10, 11, 12{]}{]}}}。

\end{itemize}

除此之外，\sphinxcode{\sphinxupquote{a}} 還有兩個重要的屬性並未顯示在列印的結果中：
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{a}} 的尺寸（size）為 \sphinxcode{\sphinxupquote{(3, 4)}}，表示 \sphinxcode{\sphinxupquote{a}} 為一 \(3 \times 4\) 之張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則。

\item {} 
\sphinxcode{\sphinxupquote{a}} 的資料類型（data type）為 \sphinxcode{\sphinxupquote{int64}}，表示64位元的整數。在進行運算時，張量間的類型須相同。

\end{itemize}

稍後，我們會討論如何獲得 \sphinxcode{\sphinxupquote{torch}} 張量的尺寸與資料類型。


\subsection{張量之數值}
\label{\detokenize{notebook/lab-torch-tensor:id3}}
若要獲得張量的資料數值（value），可透過 \sphinxcode{\sphinxupquote{.numpy()}}獲得，其回傳該張量對應之 \sphinxcode{\sphinxupquote{numpy}} 陣列

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data of tensor is: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data of tensor is: 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{numpy}} 陣列是 \sphinxcode{\sphinxupquote{python}} 進行科學運算時，幾乎都會仰賴的資料格式。\sphinxcode{\sphinxupquote{torch}} 內建了多種函數，以協助產生具有特別數值結構之張量：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with all elements being ones }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with all elements being zeros }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{identity\PYGZhy{}like tensor }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{m} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{diagonal matrix }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with all elements being ones 
 [[1.]
 [1.]
 [1.]
 [1.]]
tensor with all elements being zeros 
 [[0. 0. 0.]
 [0. 0. 0.]]
identity\PYGZhy{}like tensor 
 [[1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]]
diagonal matrix 
 [[1 0 0 0]
 [0 2 0 0]
 [0 0 3 0]
 [0 0 0 4]]
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{torch}} 亦可指定分配產生隨機的資料

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with random elements from uniform(0, 1) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with uninitialized data }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with random elements from uniform(0, 1) 
 [[0.02196592 0.78253865 0.39747602 0.31172973 0.00485885 0.3159684 ]
 [0.46485758 0.14995414 0.5118554  0.04464334 0.35136598 0.99198097]]
tensor with uninitialized data 
 [[1.e\PYGZhy{}44 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]
 [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]
\end{sphinxVerbatim}


\subsection{張量之形狀}
\label{\detokenize{notebook/lab-torch-tensor:id4}}
張量之形狀與形狀維度之數量，可透過張量物件的 \sphinxcode{\sphinxupquote{.size()}}（或 \sphinxcode{\sphinxupquote{.shape}}） 與 \sphinxcode{\sphinxupquote{dim}} 方法來獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dim of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{dim}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
size of tensor is torch.Size([3, 4])
size of tensor is torch.Size([3, 4])
dim of tensor is 2
\end{sphinxVerbatim}

如果要對張量的形狀進行改變的話，可透過 \sphinxcode{\sphinxupquote{.view()}} 此方法獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (4, 3): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (2, 2, 3): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12, 1): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12, 1) by (\PYGZhy{}1, 1): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor with shape (12,): }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor with shape (4, 3): 
 [[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
tensor with shape (2, 2, 3): 
 [[[ 1  2  3]
  [ 4  5  6]]

 [[ 7  8  9]
  [10 11 12]]]
tensor with shape (12, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12, 1) by (\PYGZhy{}1, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12,): 
 [ 1  2  3  4  5  6  7  8  9 10 11 12]
\end{sphinxVerbatim}

注意，\sphinxcode{\sphinxupquote{(12, 1)}} 與 \sphinxcode{\sphinxupquote{(12,)}} 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。另外，\sphinxhyphen{}1表示該面向對應之尺寸，由其它面向決定。


\subsection{張量之資料類型}
\label{\detokenize{notebook/lab-torch-tensor:id5}}
張量的資料類型，可透過 \sphinxcode{\sphinxupquote{.dtype}} 方法獲得

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data type of tensor is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data type of tensor is torch.int64
\end{sphinxVerbatim}

若是要調整資料類型的話，則可透過 \sphinxcode{\sphinxupquote{.type()}} 此方法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([[ 1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12.]], dtype=torch.float64)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{torch}} 內建多種資料類型，包含整數類型（如 \sphinxcode{\sphinxupquote{torch.int32}} 與 \sphinxcode{\sphinxupquote{torch.int64}}）與浮點數類型（如 \sphinxcode{\sphinxupquote{torch.float32}} 與 \sphinxcode{\sphinxupquote{torch.float64}}），完整的資料類型請見 \sphinxhref{https://pytorch.org/docs/stable/tensors.html}{torch.Tensor文件}。

在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 \sphinxcode{\sphinxupquote{torch}} 常用之資料類型為 \sphinxcode{\sphinxupquote{torch.float32}} 與 \sphinxcode{\sphinxupquote{torch.float64}}，前者所需的記憶體較小，但運算結果的數值誤差較大。


\section{張量之操弄}
\label{\detokenize{notebook/lab-torch-tensor:id6}}

\subsection{張量之切片}
\label{\detokenize{notebook/lab-torch-tensor:id7}}
若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。\sphinxcode{\sphinxupquote{torch}} 張量的切片方式，與 \sphinxcode{\sphinxupquote{numpy}} 類似，皆使用中括號 \sphinxcode{\sphinxupquote{{[}{]}}}，再搭配所欲擷取資料行列的索引（index）獲得。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 1st row: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 1st and 2nd rows: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 2nd column: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{extract 2nd and 3rd columns: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
extract 1st row: 
 [1 2 3 4]
extract 1st and 2nd rows: 
 [[1 2 3 4]
 [5 6 7 8]]
extract 2nd column: 
 [ 2  6 10]
extract 2nd and 3rd columns: 
 [[ 2  3]
 [ 6  7]
 [10 11]]
\end{sphinxVerbatim}

進行切片時，有幾項重點需要注意。
\begin{itemize}
\item {} 
各面向之索引從0開始。

\item {} 
負號表示從結尾數回來，如 \sphinxcode{\sphinxupquote{\sphinxhyphen{}1}} 表示最後一個位置。

\item {} 
\sphinxcode{\sphinxupquote{:}}表示該面向所有元素皆挑選。

\item {} 
\sphinxcode{\sphinxupquote{start:stop}} 表示從 \sphinxcode{\sphinxupquote{start}} 開始挑選到 \sphinxcode{\sphinxupquote{stop\sphinxhyphen{}1}}。

\item {} 
\sphinxcode{\sphinxupquote{start:stop:step}} 表示從 \sphinxcode{\sphinxupquote{start}} 開始到 \sphinxcode{\sphinxupquote{stop\sphinxhyphen{}1}}，間隔 \sphinxcode{\sphinxupquote{step}} 挑選。

\end{itemize}


\subsection{張量之串接}
\label{\detokenize{notebook/lab-torch-tensor:id8}}
多個張量在維度可對應之前提下，可透過 \sphinxcode{\sphinxupquote{torch.cat}} 串接

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{vertical concatenation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{,} \PYG{n}{a}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{horizontal concatenation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{,} \PYG{n}{a}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
vertical concatenation 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
horizontal concatenation 
 [[ 1  2  3  4  1  2  3  4]
 [ 5  6  7  8  5  6  7  8]
 [ 9 10 11 12  9 10 11 12]]
\end{sphinxVerbatim}


\section{張量之運算}
\label{\detokenize{notebook/lab-torch-tensor:id9}}
考慮以下 \sphinxcode{\sphinxupquote{a}} 與 \sphinxcode{\sphinxupquote{b}} 兩張量

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor b is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{b}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor a is 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
tensor b is 
 [[1. 2.]
 [1. 2.]
 [1. 2.]]
\end{sphinxVerbatim}

我們將使用 \sphinxcode{\sphinxupquote{a}} 與 \sphinxcode{\sphinxupquote{b}} 來展示如何使用 \sphinxcode{\sphinxupquote{torch}} 進行張量間的計算。


\subsection{張量元素對元素之運算}
\label{\detokenize{notebook/lab-torch-tensor:id10}}
透過 \sphinxcode{\sphinxupquote{torch}} 的數學函數，可進行張量元素對元素的四則運算

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise add }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise subtract }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sub}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise multiply }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mul}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise divide }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{div}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
element\PYGZhy{}wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element\PYGZhy{}wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element\PYGZhy{}wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element\PYGZhy{}wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
\end{sphinxVerbatim}

前述採用的函數，皆可取代為其所對應之運算子計算

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise add }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{+} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise subtract }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise multiply }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{*} \PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{element\PYGZhy{}wise divide }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{a} \PYG{o}{/} \PYG{n}{b}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
element\PYGZhy{}wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element\PYGZhy{}wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element\PYGZhy{}wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element\PYGZhy{}wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
\end{sphinxVerbatim}

若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 \sphinxhref{https://pytorch.org/docs/stable/torch.html\#math-operations}{troch官方文件} 此模組中尋找對應的數學函數。


\subsection{張量線性代數之運算}
\label{\detokenize{notebook/lab-torch-tensor:id11}}
除了簡單的四則運算外，當張量的 \sphinxcode{\sphinxupquote{dim}} 為2時，\sphinxcode{\sphinxupquote{torch}} 提供了進行線性代數（linear algebra）相關的函數，如
\begin{itemize}
\item {} 
矩陣轉置（matrix transpose）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a\PYGZus{}t} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n+nb}{input}\PYG{o}{=}\PYG{n}{a}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transpose of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{a\PYGZus{}t}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
transpose of a is 
 [[1. 3. 5.]
 [2. 4. 6.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
矩陣乘法（matrix multiplication）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c} \PYG{o}{=} \PYG{n}{a\PYGZus{}t} \PYG{o}{@} \PYG{n}{a}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c = a\PYGZus{}t @ a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
c = a\PYGZus{}t @ a is 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
反矩陣（matrix inverse）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c\PYGZus{}inv} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inverse of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c\PYGZus{}inv}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} c @ c\PYGZus{}inv should be identity matrix}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for inverse (left) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c\PYGZus{}inv} \PYG{o}{@} \PYG{n}{c}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for inverse (right) }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c} \PYG{o}{@} \PYG{n}{c\PYGZus{}inv}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
inverse of c is 
 [[ 2.33333333 \PYGZhy{}1.83333333]
 [\PYGZhy{}1.83333333  1.45833333]]
check for inverse (left) 
 [[ 1.00000000e+00 \PYGZhy{}1.42108547e\PYGZhy{}14]
 [ 0.00000000e+00  1.00000000e+00]]
check for inverse (right) 
 [[1. 0.]
 [0. 1.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
Cholesky 拆解（Cholesky decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c\PYGZus{}chol} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cholesky factor of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{c\PYGZus{}chol}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for Cholesky decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{c\PYGZus{}chol} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{c\PYGZus{}chol}\PYG{p}{,} \PYG{l+m+mi}{0} \PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Cholesky factor of c is 
 [[5.91607978 0.        ]
 [7.43735744 0.82807867]]
check for Cholesky decomposition 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
特徵拆解（eigen\sphinxhyphen{}decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{e}\PYG{p}{,} \PYG{n}{v} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{symeig}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{c}\PYG{p}{,} \PYG{n}{eigenvectors}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eigenvalue of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{e}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eigenvector of c is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{v}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for eigen\PYGZhy{}decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{v} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)} \PYG{o}{@}
       \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{v}\PYG{p}{,} \PYG{l+m+mi}{0} \PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
eigenvalue of c is 
 [ 0.26450509 90.73549491]
eigenvector of c is 
 [[\PYGZhy{}0.78489445  0.61962948]
 [ 0.61962948  0.78489445]]
check for eigen\PYGZhy{}decomposition 
 [[35. 44.]
 [44. 56.]]
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
奇異值拆解（singular value decomposition）

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{u}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,} \PYG{n}{v} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{svd}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{singular value of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{s}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{left singular vector of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{u}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{right singular vector of a is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{v}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check for singular value decomposition }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{p}{(}\PYG{n}{u} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)} \PYG{o}{@}
       \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{v}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
singular value of a is 
 [9.52551809 0.51430058]
left singular vector of a is 
 [[\PYGZhy{}0.2298477   0.88346102]
 [\PYGZhy{}0.52474482  0.24078249]
 [\PYGZhy{}0.81964194 \PYGZhy{}0.40189603]]
right singular vector of a is 
 [[\PYGZhy{}0.61962948 \PYGZhy{}0.78489445]
 [\PYGZhy{}0.78489445  0.61962948]]
check for singular value decomposition 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
\end{sphinxVerbatim}


\subsection{對張量之數值進行摘要}
\label{\detokenize{notebook/lab-torch-tensor:id12}}
\sphinxcode{\sphinxupquote{torch}} 提供了一些化約（reduction）的函數，對張量內的數值進行摘要

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate standard deviation }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate max }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate min }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate mean 
 3.5
calculate standard deviation 
 1.8708286933869707
calculate max 
 6.0
calculate min 
 1.0
\end{sphinxVerbatim}

我們亦可對張量的各面向，進行前述的摘要。以平均數為例：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean for each column }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{calculate mean for each row }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{input} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate mean for each column 
 [3. 4.]
calculate mean for each row 
 [1.5 3.5 5.5]
\end{sphinxVerbatim}

其它的化約函數，可以參考\sphinxhref{https://pytorch.org/docs/stable/torch.html\#reduction-ops}{官方文件}。


\section{實徵範例}
\label{\detokenize{notebook/lab-torch-tensor:id13}}

\subsection{產生線性迴歸資料}
\label{\detokenize{notebook/lab-torch-tensor:id14}}
在開始之前，我們先設定一種子，以讓後續的亂數生成都能夠獲得相同的結果（不過，這裡的 \sphinxcode{\sphinxupquote{manual\_seed}} 僅適用於CPU，若使用GPU，請改為 \sphinxcode{\sphinxupquote{torch.cuda.manual\_seed}}）。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{manual\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{48}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}torch.\PYGZus{}C.Generator at 0x7f8df8667fb0\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a function to generate x and y}
\PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{weight}\PYG{p}{,}
                  \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{weight}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}feature} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{n}{mean\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{n\PYGZus{}feature}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{e} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}residual}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{bias} \PYG{o}{+} \PYG{n}{x} \PYG{o}{@} \PYG{n}{weight} \PYG{o}{+} \PYG{n}{e}
    \PYG{k}{return} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run generate\PYGZus{}data}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{weight} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{feature matrix x is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{response vector y is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
feature matrix x is 
 [[10.86425232  2.85930239 12.99748957]
 [ 7.98709525  9.79137811 12.98202577]
 [ 8.40585824 13.31645195  4.51125723]
 [10.52843635 10.28388618  8.43102705]
 [11.25624976 12.04005459 12.59301479]
 [12.31145327  9.15153143  7.96220487]
 [11.49807312 14.63609048  6.82995236]
 [11.7234386   7.16098399  9.8980089 ]
 [ 7.24735628  7.5570991   5.9154331 ]
 [11.05035931 10.47430089  4.86376669]]
response vector y is 
 [[\PYGZhy{}42.98404441]
 [ \PYGZhy{}4.94309144]
 [  0.31840093]
 [\PYGZhy{}16.21407503]
 [\PYGZhy{}13.77391657]
 [\PYGZhy{}31.18360077]
 [ \PYGZhy{}8.4871242 ]
 [\PYGZhy{}31.53590804]
 [ \PYGZhy{}7.8563037 ]
 [\PYGZhy{}19.27007283]]
\end{sphinxVerbatim}


\subsection{計算模型參數}
\label{\detokenize{notebook/lab-torch-tensor:id15}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a function to calculate model parameter}
\PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}parameter}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n}{dtype}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n}{dtype}\PYG{p}{:}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{u} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{x\PYGZus{}design} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{u}\PYG{p}{,} \PYG{n}{x}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{parameter} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}
        \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x\PYGZus{}design}\PYG{p}{)} \PYG{o}{@} \PYGZbs{}
                \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{y}
    \PYG{n}{bias} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{bias}\PYG{p}{,} \PYG{n}{weight}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run calculate\PYGZus{}parameter}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{,}
                     \PYG{n}{weight} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}residual} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{n}{bias}\PYG{p}{,} \PYG{n}{weight} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}parameter}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bias estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{bias}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weight estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
bias estimate is 
 4.996612423766793
weight estimate is 
 [\PYGZhy{}4.98003037  2.98762589 \PYGZhy{}0.00737968]
\end{sphinxVerbatim}


\subsection{建立一進行迴歸分析之物件}
\label{\detokenize{notebook/lab-torch-tensor:id16}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a class to fit linear regression}
\PYG{k}{class} \PYG{n+nc}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias} \PYG{o}{=} \PYG{k+kc}{None}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{k}{def} \PYG{n+nf}{fit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{n}{u} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{n}{x\PYGZus{}design} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{u}\PYG{p}{,} \PYG{n}{x}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{parameter} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{inverse}\PYG{p}{(}
            \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x\PYGZus{}design}\PYG{p}{)} \PYG{o}{@} \PYGZbs{}
                    \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x\PYGZus{}design}\PYG{p}{,} \PYG{n}{dim0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{dim1}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{y}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight} \PYG{o}{=} \PYG{n}{parameter}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{k}{return} \PYG{n+nb+bp}{self}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model\PYGZus{}lr} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bias estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{bias}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weight estimate is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{weight}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
bias estimate is 
 4.996612423766793
weight estimate is 
 [\PYGZhy{}4.98003037  2.98762589 \PYGZhy{}0.00737968]
\end{sphinxVerbatim}


\chapter{邏輯斯迴歸}
\label{\detokenize{notebook/logistic-regression:id1}}\label{\detokenize{notebook/logistic-regression::doc}}
邏輯斯迴歸（logistic regression）與線性迴歸相似，都是透過一線性函數 \(f(x)\) 以描述兩變項 \(x\) 與 \(y\) 之間的關係，但不同之處在於邏輯斯迴歸考慮的 \(y\) 為類別變項，其類別數為2，此外，\(f(x)\) 與 \(y\) 之間的關係，需再透過一邏輯斯（logistic）函數進行轉換。邏輯斯迴歸可說是統計領域最基本之二元分類（binary classification）方法，其可視為線性迴歸於分類問題上之拓展。

在此主題中，我們將會學習以下的重點：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
使用伯努利分配（bernoulli distribution）刻畫類別變數之隨機行為。

\item {} 
使用最大概似法（maximum likelihood method，簡稱ML法），對邏輯斯迴歸模型參數進行估計。

\item {} 
利用數值優化（numerical optimization）的技術，對函數逐步地進行優化以求解。

\end{enumerate}


\section{邏輯斯迴歸模型}
\label{\detokenize{notebook/logistic-regression:id2}}
廣義來說，二元分類之問題關注的是如何使用一 \(P\) 維之向量 \(x\)，對於二元變項 \(y\) 進行預測，這裡，\(y\)的數值只能為0或1，即\(y \in \{0,1\}\)，\(y=1\) 表示觀測值屬於某一類，而 \(y=0\)則表示觀測值屬於另外一類。在二元分類的問題下，研究者常試圖刻畫在給定 \(x\) 之下，\(y\) 的條件機率（conditional probability），即：
\begin{equation*}
\begin{split}\mathbb{P}(y|x)=\frac{\mathbb{P}(y,x)}{\mathbb{P}(x)}\end{split}
\end{equation*}
這裡，\(\mathbb{P}(y,x)\) 表示同時考慮 \(x\) 與 \(y\) 的聯合機率（joint probability），而 \(\mathbb{P}(x)\) 則為僅考慮 \(x\) 之邊際機率（marginal probability）。在此講義中，我們將簡單的使用\(\pi_1(x)\)與\(\pi_0(x)\)來表示在給定 \(x\) 之下，\(y=1\) 與 \(y=0\) 之條件機率，即
\begin{equation*}
\begin{split}\pi_1(x) =\mathbb{P}(y=1|x)\end{split}
\end{equation*}
與
\begin{equation*}
\begin{split}\pi_0(x) =\mathbb{P}(y=0|x)\end{split}
\end{equation*}
令\(f(x)=\beta_0 + \sum_{p=1}^P \beta_p x_p\)表示一線性函數，\(\beta_p\)與\(\beta_0\)分別表示迴歸係數與截距，邏輯斯迴歸試圖使用一邏輯斯函數來刻畫\(f(x)\)與\(\pi_1(x)\)的關聯性，即
\begin{equation*}
\begin{split}\pi_1(x) = \frac{\exp{ \left[ f(x) \right] }}{1+\exp{ \left[ f(x) \right] }}\end{split}
\end{equation*}
由於 \(\pi_1(x)\) 與 \(\pi_0(x)\) 兩者的和須為1，因此，\(\pi_0(x)\) 則可寫為
\begin{equation*}
\begin{split}\pi_1(x) = \frac{1}{1+\exp{ \left[ f(x) \right] }}\end{split}
\end{equation*}
透過邏輯斯迴歸模型的結構，我們可以觀察到以下兩件事情：
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(\pi_1(x)\) 與 \(\pi_0(x)\) 兩者之數值皆介於0到1之間，符合機率的公理（axiom）。

\item {} 
當 \(f(x)\) 數值大時，\(\pi_1(x)\) 的數值將很靠近1，意味著獲得 \(y=1\) 的可能性很大，反之，\(\pi_0(x)\) 的數值則較大，獲得 \(y=0\) 的可能性較高。

\end{enumerate}

在迴歸係數的解讀方面，\(\beta_p\) 越大，表示 \(x_p\) 對於獲得 \(y=1\) 有較大的影響，反之，則對 \(y=0\) 有較大的影響，然而，\(\beta_p\) 影響的具體效果則不容易解讀，一般來說，需透過比較給定 \(x\) 下的對數勝率（log\sphinxhyphen{}odds）進行解讀：
\begin{equation*}
\begin{split}\begin{aligned}
\log \left[ \frac{\pi_1(x)}{\pi_0(x)} \right]
=& \log \left\{ \frac{\frac{\exp{ \left[ f(x) \right] }}{1+\exp{ \left[ f(x) \right] }}}{\frac{1}{1+\exp{ \left[ f(x) \right] }}} \right\} \\
=& \log \left\{ \exp \left[ f(x) \right] \right\} \\
=& f(x)  \\
=& \beta_0 + \sum_{p=1}^P \beta_p x_p
\end{aligned}\end{split}
\end{equation*}
因此，\(\beta_p\) 可解讀為當 \(x_p\) 每變動一個單位時，預期對數勝率跟著變動的單位。


\section{最大概似估計法}
\label{\detokenize{notebook/logistic-regression:id3}}

\section{數值優化技術與求解}
\label{\detokenize{notebook/logistic-regression:id4}}

\chapter{Lab: 數值微分與優化}
\label{\detokenize{notebook/lab-torch-diff-opt:lab}}\label{\detokenize{notebook/lab-torch-diff-opt::doc}}
在此 lab 中，我們將介紹
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
如何使用 \sphinxcode{\sphinxupquote{torch}} 進行數值微分。

\item {} 
如何使用 \sphinxcode{\sphinxupquote{torch}} 進行數值優化。

\item {} 
利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}


\section{數值微分}
\label{\detokenize{notebook/lab-torch-diff-opt:id1}}

\subsection{可獲得梯度張量之輸入}
\label{\detokenize{notebook/lab-torch-diff-opt:id2}}
而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first\sphinxhyphen{}order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。

在 \sphinxcode{\sphinxupquote{torch}} 中，張量不僅用於儲存資料，其亦用於儲存模型之參數。然而，誠如先前所述，我們很可能會需要用到對應於該參數之梯度訊息，因此，為了追朔該參數的歷史建立計算圖（computation graph），輸入該參數張量時需要加入 \sphinxcode{\sphinxupquote{requires\_grad=True}} 此指令：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([1., 2., 3.], requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

這裏，我們建立了一尺寸為 \(3\) 的張量，由於此張量具有 \sphinxcode{\sphinxupquote{requires\_grad=True}} 此標記，因此，接下來對此張量進行任何的運算，\sphinxcode{\sphinxupquote{torch}} 皆會將此計算過程記錄下來。舉例來說：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor y: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensor z: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{z}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor y: 
 tensor([\PYGZhy{}2.,  0.,  2.], grad\PYGZus{}fn=\PYGZlt{}SubBackward0\PYGZgt{})
tensor z: 
 tensor(8., grad\PYGZus{}fn=\PYGZlt{}SumBackward0\PYGZgt{})
\end{sphinxVerbatim}

我們可以看到，無論是 \sphinxcode{\sphinxupquote{y}} 或是 \sphinxcode{\sphinxupquote{z}}，其都具有 \sphinxcode{\sphinxupquote{requires\_grad=True}} 的標記。要特別注意的是，\sphinxcode{\sphinxupquote{requires\_grad=True}} 僅適用於資料類型為浮點數之張量。


\subsection{數值微分之執行}
\label{\detokenize{notebook/lab-torch-diff-opt:id3}}
針對已追朔之運算過程，想要獲得與該運算有關的梯度時，可以使用 \sphinxcode{\sphinxupquote{.backward()}}此方法。在前一小節的例子中，\(z = \sum_{i=1}^3 (2x_{i} - 4)^2\)，若想要獲得 \(\frac{\partial z}{\partial x}\) 在當下 \(x\) 的數值的話，可使用以下的程式碼：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}8.,  0.,  8.])
\end{sphinxVerbatim}

接著，由於 \(z\) 也可以寫為 \(z = \sum_{i=1}^3 y_{i}^2\)，因此，我們是否也可以透過類似的程式碼獲得 \(\frac{\partial z}{\partial y}\) 呢？

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dy:  None
\end{sphinxVerbatim}
\begin{sphinxalltt}
/Users/phhaung/Documents/PycharmProject/tism/venv/lib/python3.8/site\sphinxhyphen{}packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won\textquotesingle{}t be populated during autograd.backward(). If you indeed want the gradient for a non\sphinxhyphen{}leaf Tensor, use .retain\_grad() on the non\sphinxhyphen{}leaf Tensor. If you access the non\sphinxhyphen{}leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "

\end{sphinxalltt}

結果是不行，主因在於，\sphinxcode{\sphinxupquote{torch}} 為了節省記憶體的使用，因此，僅可提供位於計算圖葉子（leaf）張量之一次微分。如果希望能夠獲得 \(\frac{\partial z}{\partial y}\) 的話，可以對 \sphinxcode{\sphinxupquote{y}} 使用 \sphinxcode{\sphinxupquote{.retain\_grad()}} 此方法：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{retain\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dy:  tensor([\PYGZhy{}4.,  0.,  4.])
\end{sphinxVerbatim}

在評估完 \(\frac{\partial z}{\partial y}\) 後，讓我們重新檢視一下 \sphinxcode{\sphinxupquote{x.grad}} 的數值：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}16.,   0.,  16.])
\end{sphinxVerbatim}

我們會發現，這時的 \sphinxcode{\sphinxupquote{x.grad}} 數值，變成了原先的兩倍，其背後的原因在於，\sphinxcode{\sphinxupquote{.backward()}}此方法，會持續地將計算結果累積在變數所對應之 \sphinxcode{\sphinxupquote{.grad}} 當中。若想要避免持續累積，可以使用 \sphinxcode{\sphinxupquote{.grad.zero\_()}} 方法將 \sphinxcode{\sphinxupquote{.grad}} 中的數值歸零：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([0., 0., 0.])
dz/dx:  tensor([0., 0., 0.])
\end{sphinxVerbatim}

接著，就可以使用原先的程式碼，計算 \(\frac{\partial z}{\partial x}\) 與 \(\frac{\partial z}{\partial y}\)：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
\PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y}\PYG{o}{.}\PYG{n}{retain\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dx: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dz/dy: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dz/dx:  tensor([\PYGZhy{}8.,  0.,  8.])
dz/dy:  tensor([\PYGZhy{}4.,  0.,  4.])
\end{sphinxVerbatim}

不過要特別注意的是，如果計算圖沒有重新建立，連續進行兩次 \sphinxcode{\sphinxupquote{.backward()}} 會引發錯誤的訊息。


\subsection{可獲得梯度張量之進階控制}
\label{\detokenize{notebook/lab-torch-diff-opt:id4}}
一個張量是否有被追朔以計算梯度，除了直接列印外，亦可透過 \sphinxcode{\sphinxupquote{.requires\_grad}} 此屬性來觀看

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}

如果想將一原先沒有要求梯度之張量，改為需要梯度時，可以使用 \sphinxcode{\sphinxupquote{.requires\_grad\_()}} 此方法原地修改該向量的 \sphinxcode{\sphinxupquote{requires\_grad}} 類型：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad\PYGZus{}}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
True
\end{sphinxVerbatim}

如果想將張量 \sphinxcode{\sphinxupquote{x}} 拷貝到另一變量 \sphinxcode{\sphinxupquote{x\_ng}}，卻不希望 \sphinxcode{\sphinxupquote{x\_ng}} 的計算會被追朔時，可以使用以下的程式碼：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x\PYGZus{}ng} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x\PYGZus{}ng}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}

最後，如果希望可獲得梯度之向量後續的計算歷程不被追朔的話，可以將計算程式碼置於 \sphinxcode{\sphinxupquote{with torch.no\_grad():}} 此環境中，即

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}
    \PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{y}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{requires\PYGZus{}grad}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
False
\end{sphinxVerbatim}


\section{數值優化}
\label{\detokenize{notebook/lab-torch-diff-opt:id5}}

\subsection{手動撰寫優化算則}
\label{\detokenize{notebook/lab-torch-diff-opt:id6}}
前一小節所使用的範例，其計算過程可以寫為
\begin{equation*}
\begin{split}
\begin{aligned}
z &= f(x)\\
 &= \sum_{i=1}^3\left[2(x_i-2)\right]^2 \\
 &= \sum_{i=1}^3 y^2
\end{aligned}
\end{split}
\end{equation*}
若想要找到一 \(\widehat{x}\)，其使得 \(f(\widehat{x})\) 達到最小值的話，由於 \(z\) 為 \(y_1, y_2, y_3\) 的平方和，因此，其會在 \(\widehat{y} = (0, 0, 0)\)的地方達到最小值，也意味著 \(\widehat{x} = (2,2,2)\)。

那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 \(\theta\) 表示模型參數（其扮演範例中\(x\)的角色），\(\mathcal{D}(\theta)\) 表示度量模型好壞的目標函數（其扮演\(f(x)\)的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）\(\widehat{\theta}\) 的更新規則為
\begin{equation*}
\begin{split}
\widehat{\theta} \leftarrow \widehat{\theta} - s \times \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta}
\end{split}
\end{equation*}
這裏，\(s\) 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 \(\mathcal{D}\) 足夠圓滑（smooth），且 \(s\) 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 \(\mathcal{D}\) 最小值的發生位置。

在開始進行梯度下降前，我們先定義一函數 \sphinxcode{\sphinxupquote{f}} 使得\sphinxcode{\sphinxupquote{z = f(x)}}，並了解起始狀態時，\sphinxcode{\sphinxupquote{f(x)}} 與 \sphinxcode{\sphinxupquote{x}} 的數值為何：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{z}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
f(x) = 8.000, x = tensor([1., 2., 3.])
\end{sphinxVerbatim}

使用 \sphinxcode{\sphinxupquote{torch}} 進行梯度下降，需先計算在當下 \sphinxcode{\sphinxupquote{x}} 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 \sphinxcode{\sphinxupquote{x}} 進行更新，即

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lr} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
\end{sphinxVerbatim}

這裡，我們將學習速率 \sphinxcode{\sphinxupquote{lr}} 設為0.1，而張量的 \sphinxcode{\sphinxupquote{.sub\_()}} 方法則是就地減去括號內的數值直接更新。透過 \sphinxcode{\sphinxupquote{f(x)}} 的數值，可觀察到梯度下降的確導致 \sphinxcode{\sphinxupquote{z}} 數值的下降，而 \sphinxcode{\sphinxupquote{x}} 也與 \(\widehat{x}=(2,2,2)\) 更加地靠近。

梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 \sphinxcode{\sphinxupquote{for}} 迴圈，重複更新\(I\)次：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{21}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, f(x) = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, f(x) = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, f(x) = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, f(x) = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, f(x) = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, f(x) = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 11, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 12, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 13, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 14, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 15, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 16, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 17, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 18, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 19, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 20, f(x) = 0.000, x = tensor([2., 2., 2.])
\end{sphinxVerbatim}

然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 \sphinxcode{\sphinxupquote{tol}} 時，算則就停止，其所對應之程式碼為：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tol} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}5}
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{20}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{sub\PYGZus{}}\PYG{p}{(}\PYG{n}{lr} \PYG{o}{*} \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, z = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}
        \PYG{n}{epoch} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{tol}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{zero\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
\end{sphinxVerbatim}


\subsection{使用\sphinxstyleliteralintitle{\sphinxupquote{torch.optim}}進行優化}
\label{\detokenize{notebook/lab-torch-diff-opt:torch-optim}}
由於 \sphinxcode{\sphinxupquote{torch}} 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 \sphinxcode{\sphinxupquote{torch.optim}} 的類型來求得函數的最小值。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 為進行梯度下降法之物件，由於 \sphinxcode{\sphinxupquote{torch}} 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini\sphinxhyphen{}batch gradient descent）來強化優化的效能，因此，\sphinxcode{\sphinxupquote{torch}} 使用 \sphinxcode{\sphinxupquote{SGD}} 一詞。事實上，除了計算一階導數時資料量的差異外，\sphinxcode{\sphinxupquote{SGD}} 與傳統的梯度下降並無差異。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 可透過以下的程式碼來使用：

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float}\PYG{p}{,}
                 \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{iter }\PYG{l+s+si}{\PYGZob{}:2.0f\PYGZcb{}}\PYG{l+s+s2}{, z = }\PYG{l+s+si}{\PYGZob{}:2.3f\PYGZcb{}}\PYG{l+s+s2}{, x = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}
        \PYG{n}{epoch} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{z}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{grad}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{tol}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
\end{sphinxVerbatim}

這裡，我們使用 \sphinxcode{\sphinxupquote{torch.optim.SGD}} 來生成優化器（optimizer）物件 \sphinxcode{\sphinxupquote{opt}}，其在生成時，需要指定其追朔的變量，並以一可迭代的物件（iterable object）作為輸入，因此，在該程式碼中，我們將 \sphinxcode{\sphinxupquote{x}} 以一元組（tuple）的方式來輸入，並指定學習速率 \sphinxcode{\sphinxupquote{lr=.1}}。使用內建優化器時，仍須手動對目標函數執行 \sphinxcode{\sphinxupquote{.backward()}}，但更新估計值的步驟，可使用優化器的 \sphinxcode{\sphinxupquote{.step()}} 來進行，而消除變量的 \sphinxcode{\sphinxupquote{.grad}}，則可使用優化器的 \sphinxcode{\sphinxupquote{.zero\_grad()}} 方法。

\sphinxcode{\sphinxupquote{torch.optim.SGD}} 容許使用者加入動能（momentum）\(m\)（其預設為 0），此時，優化算則會改為
\begin{equation*}
\begin{split}
\begin{aligned}
\delta & \leftarrow m \times  \delta + \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta} \\
\widehat{\theta} &\leftarrow \widehat{\theta} - s \times \delta
\end{aligned}
\end{split}
\end{equation*}
這裡，\(\delta\) 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。

在 \sphinxcode{\sphinxupquote{torch.optim.SGD}} 中，有許多不同的優化器（見\sphinxhref{https://pytorch.org/docs/stable/optim.html}{\sphinxcode{\sphinxupquote{torch.optim}}頁面}），如
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{Adadelta}}（見\sphinxhref{https://arxiv.org/abs/1212.5701}{ADADELTA: An Adaptive Learning Rate Method}）

\item {} 
\sphinxcode{\sphinxupquote{Adagrad}}（見\sphinxhref{https://jmlr.org/papers/v12/duchi11a.html}{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}）

\item {} 
\sphinxcode{\sphinxupquote{Adam}}（見\sphinxhref{https://arxiv.org/abs/1412.6980}{Adam: A Method for Stochastic Optimization}）

\item {} 
\sphinxcode{\sphinxupquote{LBFGS}}（見\sphinxhref{https://doi.org/10.1007/BF01589116}{On the limited memory BFGS method for large scale optimization}）

\item {} 
\sphinxcode{\sphinxupquote{Adadelta}}（見\sphinxhref{https://arxiv.org/abs/1212.5701}{ADADELTA: An Adaptive Learning Rate Method}）

\item {} 
\sphinxcode{\sphinxupquote{RMSprop}}（見\sphinxhref{https://arxiv.org/abs/1308.0850}{Generating Sequences With Recurrent Neural Networks}）

\end{itemize}

讀者可自行深入了解這些方法。


\section{實徵範例}
\label{\detokenize{notebook/lab-torch-diff-opt:id7}}

\subsection{產生邏吉斯迴歸資料}
\label{\detokenize{notebook/lab-torch-diff-opt:id8}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{manual\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{48}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}torch.\PYGZus{}C.Generator at 0x7fdf402fefb0\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Bernoulli}
\PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,}
                  \PYG{n}{weight}\PYG{p}{,}
                  \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{weight}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}feature} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{n}{mean\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{n\PYGZus{}feature}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{logit} \PYG{o}{=} \PYG{n}{bias} \PYG{o}{+} \PYG{n}{x} \PYG{o}{@} \PYG{n}{weight}
    \PYG{n}{bernoulli} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits} \PYG{o}{=} \PYG{n}{logit}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{bernoulli}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run generate\PYGZus{}data}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{,}
                     \PYG{n}{weight} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{feature matrix x is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{response vector y is }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
feature matrix x is 
 [[10.86425232  2.85930239 12.99748957]
 [ 7.98709525  9.79137811 12.98202577]
 [ 8.40585824 13.31645195  4.51125723]
 [10.52843635 10.28388618  8.43102705]
 [11.25624976 12.04005459 16.67369106]
 [ 8.69996305  7.24918979  6.04079975]
 [11.15669941  7.70897067  6.53992666]
 [ 9.84083719  3.8656981   9.92935665]
 [ 7.08780983  7.95524324  8.11661451]
 [ 8.9847464  11.48372895 10.09772223]]
response vector y is 
 [[0.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
\end{sphinxVerbatim}


\chapter{機率分佈}
\label{\detokenize{notebook/probability-distribution:id1}}\label{\detokenize{notebook/probability-distribution::doc}}

\chapter{最大概似法}
\label{\detokenize{notebook/maximum-likelihood:id1}}\label{\detokenize{notebook/maximum-likelihood::doc}}

\chapter{Lab: 最大概似估計}
\label{\detokenize{notebook/lab-torch-mle:lab}}\label{\detokenize{notebook/lab-torch-mle::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\end{sphinxVerbatim}


\section{\sphinxstyleliteralintitle{\sphinxupquote{torch}} 分配物件}
\label{\detokenize{notebook/lab-torch-mle:torch}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Normal}
\PYG{n}{normal} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mf}{1.}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with shape (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with shape ():
 tensor(0.3471)
random sample with shape (3,):
 tensor([ 0.2764,  1.3294, \PYGZhy{}0.0920])
random sample with shape (2,3):
 tensor([[ 2.6177,  0.8466,  1.3437],
        [\PYGZhy{}1.4105,  1.9438,  0.5512]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cumulative probability given value with shape ():
 tensor(0.5000) 

cumulative probability given value with (3,):
 tensor([0.1587, 0.5000, 0.6915]) 

cumulative probability given value with (2,3):
 tensor([[0.1587, 0.5000, 0.6915],
        [0.0228, 0.8413, 0.9987]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cumulative probability given value with (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cumulative probability given value with shape ():
 tensor(\PYGZhy{}0.9189) 

cumulative probability given value with (3,):
 tensor([\PYGZhy{}1.4189, \PYGZhy{}0.9189, \PYGZhy{}1.0439]) 

cumulative probability given value with (2,3):
 tensor([[\PYGZhy{}1.4189, \PYGZhy{}0.9189, \PYGZhy{}1.0439],
        [\PYGZhy{}2.9189, \PYGZhy{}1.4189, \PYGZhy{}5.4189]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Normal(loc: 0.0, scale: 1.0)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{o}{.}\PYG{n}{batch\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal}\PYG{o}{.}\PYG{n}{event\PYGZus{}shape}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Size([])
torch.Size([])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{normal\PYGZus{}batch} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Normal(loc: torch.Size([2]), scale: torch.Size([2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2,3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([0.4789, 1.5501]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[ 0.4099, \PYGZhy{}1.1181],
        [ 1.0750, \PYGZhy{}0.2275],
        [ 0.6607,  0.8156]]) 

random sample with sample\PYGZus{}shape (2,3):
 tensor([[[\PYGZhy{}0.3418,  2.8886],
         [ 0.5395,  3.4939],
         [ 0.4751,  1.0449]],

        [[\PYGZhy{}0.6914,  0.2891],
         [\PYGZhy{}1.2024,  0.6521],
         [\PYGZhy{}0.9135,  0.6125]]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,1):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
log\PYGZhy{}probability given value with shape ():
 tensor([\PYGZhy{}0.9189, \PYGZhy{}1.5466]) 

log\PYGZhy{}probability given value with shape (2,):
 tensor([\PYGZhy{}0.9189, \PYGZhy{}1.5466]) 

log\PYGZhy{}probability given value with shape (2,1):
 tensor([[\PYGZhy{}0.9189, \PYGZhy{}1.5466],
        [\PYGZhy{}0.9189, \PYGZhy{}1.5466]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{MultivariateNormal}
\PYG{n}{mvn} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{mvn}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
MultivariateNormal(loc: torch.Size([2]), scale\PYGZus{}tril: torch.Size([2, 2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2, 3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([0.1373, 1.4793]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[1.7331, 2.1149],
        [0.1035, 0.5050],
        [0.3499, 0.9203]]) 

random sample with sample\PYGZus{}shape (2, 3):
 tensor([[[\PYGZhy{}0.0942,  2.6992],
         [ 0.4968,  1.4207],
         [ 0.7308,  0.2046]],

        [[ 0.0976,  0.4487],
         [ 1.9930,  0.9622],
         [\PYGZhy{}0.0584,  0.3790]]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log\PYGZhy{}probability given value with shape (2,1):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
log\PYGZhy{}probability given value with shape (2,):
 tensor(\PYGZhy{}2.4913) 

log\PYGZhy{}probability given value with shape (2,1):
 tensor([\PYGZhy{}2.4913, \PYGZhy{}2.4913])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Independent}
\PYG{n}{normal\PYGZus{}batch} \PYG{o}{=} \PYG{n}{Independent}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{p}{,} \PYG{n}{reinterpreted\PYGZus{}batch\PYGZus{}ndims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{batch\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{normal\PYGZus{}batch}\PYG{o}{.}\PYG{n}{event\PYGZus{}shape}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Size([])
torch.Size([2])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mvn\PYGZus{}batch} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{cholesky}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{mvn\PYGZus{}batch}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
MultivariateNormal(loc: torch.Size([3, 2]), scale\PYGZus{}tril: torch.Size([3, 2, 2]))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape ():}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (3,):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{random sample with sample\PYGZus{}shape (2, 3):}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mvn\PYGZus{}batch}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
random sample with sample\PYGZus{}shape ():
 tensor([[\PYGZhy{}0.9025,  0.7102],
        [\PYGZhy{}1.4402,  0.8393],
        [ 1.4704,  3.6990]]) 

random sample with sample\PYGZus{}shape (3,):
 tensor([[[\PYGZhy{}2.1170,  1.3513],
         [ 1.5338,  1.4740],
         [ 2.7437,  2.3366]],

        [[ 0.2841,  0.7145],
         [ 0.4565,  3.0112],
         [ 0.8437,  3.9906]],

        [[ 0.5743,  0.8828],
         [ 1.8611,  2.7959],
         [ 3.2177,  3.9537]]]) 

random sample with sample\PYGZus{}shape (2, 3):
 tensor([[[[ 0.2302,  0.1240],
          [ 0.0167,  0.5930],
          [ 1.0884,  1.6395]],

         [[ 0.8134,  1.7372],
          [ 1.5693,  2.0849],
          [ 4.6024,  2.9109]],

         [[ 0.0443,  0.3992],
          [ 1.2023,  2.6121],
          [ 2.3752,  4.1484]]],


        [[[\PYGZhy{}0.7692,  1.5281],
          [ 0.0667,  0.8561],
          [ 1.9542,  4.6272]],

         [[ 0.9022,  0.7512],
          [ 2.5382,  2.2632],
          [ 0.8168,  3.9857]],

         [[ 0.0511,  1.4663],
          [ 0.4689,  2.6174],
          [ 1.5000,  2.6020]]]])
\end{sphinxVerbatim}


\section{計算最大概似估計值}
\label{\detokenize{notebook/lab-torch-mle:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mu\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{5.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{2.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{model\PYGZus{}normal\PYGZus{}true} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu\PYGZus{}true}\PYG{p}{,}
    \PYG{n}{scale}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}true}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{normal model:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{model\PYGZus{}normal\PYGZus{}true}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
normal model:
 Normal(loc: tensor([5.]), scale: tensor([2.])) 

\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{model\PYGZus{}normal\PYGZus{}true}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{model\PYGZus{}normal\PYGZus{}true}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{negative likelihood value is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{loss\PYGZus{}value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
negative likelihood value is tensor(2.0891)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{lr} \PYG{o}{=} \PYG{l+m+mf}{1.0}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sigma} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{[}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{model\PYGZus{}normal} \PYG{o}{=} \PYG{n}{Normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{n}{sigma}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{model\PYGZus{}normal}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute the gradient}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by gradient descent:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML std by gradient descent:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by gradient descent: tensor([5.0607], requires\PYGZus{}grad=True)
ML std by gradient descent: tensor([1.9505], requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by formula:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML std by formula:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{unbiased}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by formula: tensor(5.0605)
ML std by formula: tensor(1.9526)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mu\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}tril\PYGZus{}true} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{3.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{model\PYGZus{}mvn\PYGZus{}true} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu\PYGZus{}true}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}tril\PYGZus{}true}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true mean vector: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{model\PYGZus{}mvn\PYGZus{}true}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{true covariance matrix: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{model\PYGZus{}mvn\PYGZus{}true}\PYG{o}{.}\PYG{n}{covariance\PYGZus{}matrix}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
true mean vector: 
 tensor([\PYGZhy{}1.,  0.,  1.])
true covariance matrix: 
 tensor([[9.0000, 6.0000, 1.2000],
        [6.0000, 5.0000, 1.3000],
        [1.2000, 1.3000, 0.6600]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{model\PYGZus{}mvn\PYGZus{}true}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{sample\PYGZus{}shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{model\PYGZus{}mvn\PYGZus{}true}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{negative likelihood value is}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{loss\PYGZus{}value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
negative likelihood value is tensor(4.6588)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{lr} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}
    \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sigma\PYGZus{}tril} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}
    \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{0.}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{[}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma\PYGZus{}tril}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n}{lr}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{model\PYGZus{}mvn} \PYG{o}{=} \PYG{n}{MultivariateNormal}\PYG{p}{(}
    \PYG{n}{loc}\PYG{o}{=}\PYG{n}{mu}\PYG{p}{,}
    \PYG{n}{scale\PYGZus{}tril}\PYG{o}{=}\PYG{n}{sigma\PYGZus{}tril}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{model\PYGZus{}mvn}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}value}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute the gradient}
    \PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by gradient descent: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{mu}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML covariance by gradient descent: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sigma\PYGZus{}tril} \PYG{o}{@} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{sigma\PYGZus{}tril}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by gradient descent: 
 tensor([\PYGZhy{}1.0622, \PYGZhy{}0.0872,  0.9892], requires\PYGZus{}grad=True)
ML covariance by gradient descent: 
 tensor([[8.4839, 5.6784, 1.1295],
        [5.6784, 4.8110, 1.2631],
        [1.1295, 1.2631, 0.6638]], grad\PYGZus{}fn=\PYGZlt{}MmBackward\PYGZgt{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}mean} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dim} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}moment2} \PYG{o}{=} \PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{@} \PYG{n}{x}\PYG{p}{)} \PYG{o}{/} \PYG{n}{sample\PYGZus{}size}
\PYG{n}{sample\PYGZus{}cov} \PYG{o}{=} \PYG{n}{sample\PYGZus{}moment2} \PYG{o}{\PYGZhy{}} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{ger}\PYG{p}{(}\PYG{n}{sample\PYGZus{}mean}\PYG{p}{,} \PYG{n}{sample\PYGZus{}mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML mean by formula: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sample\PYGZus{}mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ML covariance by formula: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{sample\PYGZus{}cov}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ML mean by formula: 
 tensor([\PYGZhy{}1.0622, \PYGZhy{}0.0872,  0.9892])
ML covariance by formula: 
 tensor([[8.4839, 5.6784, 1.1295],
        [5.6784, 4.8110, 1.2631],
        [1.1295, 1.2631, 0.6638]])
\end{sphinxVerbatim}


\section{實徵範例}
\label{\detokenize{notebook/lab-torch-mle:id2}}

\subsection{產生邏吉斯迴歸資料}
\label{\detokenize{notebook/lab-torch-mle:id3}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{torch}\PYG{o}{.}\PYG{n}{manual\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{48}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}torch.\PYGZus{}C.Generator at 0x7ff620fe6fb0\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{distributions} \PYG{k+kn}{import} \PYG{n}{Bernoulli}
\PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,}
                  \PYG{n}{weight}\PYG{p}{,}
                  \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}
                  \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
                  \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{weight}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}feature} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{=} \PYG{n}{mean\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{std} \PYG{o}{=} \PYG{n}{std\PYGZus{}feature}\PYG{p}{,}
                     \PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}sample}\PYG{p}{,} \PYG{n}{n\PYGZus{}feature}\PYG{p}{)}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}\PYG{p}{)}
    \PYG{n}{weight} \PYG{o}{=} \PYG{n}{weight}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{logit} \PYG{o}{=} \PYG{n}{bias} \PYG{o}{+} \PYG{n}{x} \PYG{o}{@} \PYG{n}{weight}
    \PYG{n}{bernoulli} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits} \PYG{o}{=} \PYG{n}{logit}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{bernoulli}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} run generate\PYGZus{}data}
\PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}sample} \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{,}
                     \PYG{n}{weight} \PYG{o}{=} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{bias} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{,}
                     \PYG{n}{mean\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}
                     \PYG{n}{std\PYGZus{}feature} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{,}
                     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{建立一進行邏吉斯迴歸分析之物件}
\label{\detokenize{notebook/lab-torch-mle:id4}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} define a class to fit logistic regression}
\PYG{k}{class} \PYG{n+nc}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype} \PYG{o}{=} \PYG{n}{dtype}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight} \PYG{o}{=} \PYG{k+kc}{None}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{k}{def} \PYG{n+nf}{log\PYGZus{}lik}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{logit} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias} \PYG{o}{+} \PYG{n}{x} \PYG{o}{@} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight}
        \PYG{n}{bernoulli} \PYG{o}{=} \PYG{n}{Bernoulli}\PYG{p}{(}\PYG{n}{logits} \PYG{o}{=} \PYG{n}{logit}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bernoulli}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{def} \PYG{n+nf}{fit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{lr} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{y}\PYG{o}{.}\PYG{n}{dtype} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{type}\PYG{p}{(}\PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}
        \PYG{n}{n\PYGZus{}feature} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,}
                                \PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{,}
                                \PYG{n}{requires\PYGZus{}grad} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{size} \PYG{o}{=} \PYG{p}{(}\PYG{n}{n\PYGZus{}feature}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                                  \PYG{n}{dtype} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{,}
                                  \PYG{n}{requires\PYGZus{}grad} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{opt} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{bias}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weight}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n}{lr}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{loss\PYGZus{}value} \PYG{o}{=} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}lik}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
            \PYG{n}{opt}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
            \PYG{n}{loss\PYGZus{}value}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} compute the gradient}
            \PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}
\end{sphinxVerbatim}


\subsection{計算模型參數}
\label{\detokenize{notebook/lab-torch-mle:id5}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} fit logistic model}
\PYG{n}{model\PYGZus{}lr} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{n}{lr} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{bias}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model\PYGZus{}lr}\PYG{o}{.}\PYG{n}{weight}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tensor([3.8176], dtype=torch.float64, requires\PYGZus{}grad=True)
tensor([[\PYGZhy{}4.6982],
        [ 2.6668],
        [\PYGZhy{}0.0054]], dtype=torch.float64, requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} fit logistic model via sklearn}
\PYG{c+c1}{\PYGZsh{} please install sklearn first}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{linear\PYGZus{}model}
\PYG{n}{model\PYGZus{}lr\PYGZus{}sklearn} \PYG{o}{=} \PYG{n}{linear\PYGZus{}model}\PYG{o}{.}\PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{n}{C}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{)}
\PYG{n}{model\PYGZus{}lr\PYGZus{}sklearn}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model\PYGZus{}lr\PYGZus{}sklearn}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model\PYGZus{}lr\PYGZus{}sklearn}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[3.81736449]
[[\PYGZhy{}4.69795962  2.66667074 \PYGZhy{}0.00543114]]
\end{sphinxVerbatim}
\begin{sphinxalltt}
/Users/phhaung/Documents/PycharmProject/tism/venv/lib/python3.8/site\sphinxhyphen{}packages/sklearn/utils/validation.py:72: DataConversionWarning: A column\sphinxhyphen{}vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  return f(**kwargs)

\end{sphinxalltt}


\chapter{概似推論}
\label{\detokenize{notebook/likelihood-inference:id1}}\label{\detokenize{notebook/likelihood-inference::doc}}

\chapter{真實分數模型}
\label{\detokenize{notebook/true-score-model:id1}}\label{\detokenize{notebook/true-score-model::doc}}

\chapter{因素分析}
\label{\detokenize{notebook/factor-analysis:id1}}\label{\detokenize{notebook/factor-analysis::doc}}

\chapter{試題反應理論}
\label{\detokenize{notebook/item-response-theory:id1}}\label{\detokenize{notebook/item-response-theory::doc}}

\chapter{混合建模}
\label{\detokenize{notebook/mixture-modeling:id1}}\label{\detokenize{notebook/mixture-modeling::doc}}






\renewcommand{\indexname}{Index}
\printindex
\end{document}