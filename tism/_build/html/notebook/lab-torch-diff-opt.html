

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>5. Lab: 數值微分與優化 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. 機率分佈" href="probability-distribution.html" />
    <link rel="prev" title="4. 邏輯斯迴歸" href="logistic-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-tensor.html">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic-regression.html">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="maximum-likelihood.html">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-mle.html">
   8. Lab: 最大概似估計
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood-inference.html">
   9. 概似推論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="true-score-model.html">
   10. 真實分數模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="factor-analysis.html">
   11. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   12. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixture-modeling.html">
   13. 混合建模
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/lab-torch-diff-opt.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/lab-torch-diff-opt.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   5.1. 數值微分
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     5.1.1. 可獲得梯度張量之輸入
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     5.1.2. 數值微分之執行
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     5.1.3. 可獲得梯度張量之進階控制
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   5.2. 數值優化
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     5.2.1. 手動撰寫優化算則
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#torch-optim">
     5.2.2. 使用
     <code class="docutils literal notranslate">
      <span class="pre">
       torch.optim
      </span>
     </code>
     進行優化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lbfgs">
     5.2.3. 使用LBFGS法進行優化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   5.3. 實徵範例
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="lab">
<h1><span class="section-number">5. </span>Lab: 數值微分與優化<a class="headerlink" href="#lab" title="Permalink to this headline">¶</a></h1>
<p>在此 lab 中，我們將介紹</p>
<ol class="simple">
<li><p>如何使用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行數值微分。</p></li>
<li><p>如何使用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行數值優化。</p></li>
<li><p>利用前述知識，撰寫一採用梯度下降（gradient descent）獲得迴歸參數估計之類型。</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">5.1. </span>數值微分<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3><span class="section-number">5.1.1. </span>可獲得梯度張量之輸入<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>而在統計模型中，模型之參數常需透過一優化（optimization）方法獲得，而許多的優化方法皆仰賴目標函數（objective function）的一階導數（first-order derivative），或稱梯度（gradient），因此，如何獲得目標函數對於模型參數的梯度，即為一重要的工作。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 中，張量不僅用於儲存資料，其亦用於儲存模型之參數。然而，誠如先前所述，我們很可能會需要用到對應於該參數之梯度訊息，因此，為了追朔該參數的歷史建立計算圖（computation graph），輸入該參數張量時需要加入 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 此指令：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([1., 2., 3.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>這裏，我們建立了一尺寸為 <span class="math notranslate nohighlight">\(3\)</span> 的張量，由於此張量具有 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 此標記，因此，接下來對此張量進行任何的運算，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 皆會將此計算過程記錄下來。舉例來說：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor y: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor z: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor y: 
 tensor([-2.,  0.,  2.], grad_fn=&lt;SubBackward0&gt;)
tensor z: 
 tensor(8., grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>我們可以看到，無論是 <code class="docutils literal notranslate"><span class="pre">y</span></code> 或是 <code class="docutils literal notranslate"><span class="pre">z</span></code>，其都具有 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 的標記。要特別注意的是，<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 僅適用於資料類型為浮點數之張量。</p>
</div>
<div class="section" id="id3">
<h3><span class="section-number">5.1.2. </span>數值微分之執行<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>針對已追朔之運算過程，想要獲得與該運算有關的梯度時，可以使用 <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>此方法。在前一小節的例子中，<span class="math notranslate nohighlight">\(z = \sum_{i=1}^3 (2x_{i} - 4)^2\)</span>，若想要獲得 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial x}\)</span> 在當下 <span class="math notranslate nohighlight">\(x\)</span> 的數值的話，可使用以下的程式碼：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tensor([-8.,  0.,  8.])
</pre></div>
</div>
</div>
</div>
<p>接著，由於 <span class="math notranslate nohighlight">\(z\)</span> 也可以寫為 <span class="math notranslate nohighlight">\(z = \sum_{i=1}^3 y_{i}^2\)</span>，因此，我們是否也可以透過類似的程式碼獲得 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial y}\)</span> 呢？</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dy: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dy:  None
</pre></div>
</div>
<div class="stderr docutils container">
<pre class="stderr literal-block">/Users/phhaung/Documents/PycharmProject/tism/venv/lib/python3.8/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn(&quot;The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad &quot;
</pre>
</div>
</div>
</div>
<p>結果是不行，主因在於，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 為了節省記憶體的使用，因此，僅可提供位於計算圖葉子（leaf）張量之一次微分。如果希望能夠獲得 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial y}\)</span> 的話，可以對 <code class="docutils literal notranslate"><span class="pre">y</span></code> 使用 <code class="docutils literal notranslate"><span class="pre">.retain_grad()</span></code> 此方法：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dy: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dy:  tensor([-4.,  0.,  4.])
</pre></div>
</div>
</div>
</div>
<p>在評估完 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial y}\)</span> 後，讓我們重新檢視一下 <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> 的數值：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tensor([-16.,   0.,  16.])
</pre></div>
</div>
</div>
</div>
<p>我們會發現，這時的 <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> 數值，變成了原先的兩倍，其背後的原因在於，<code class="docutils literal notranslate"><span class="pre">.backward()</span></code>此方法，會持續地將計算結果累積在變數所對應之 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 當中。若想要避免持續累積，可以使用 <code class="docutils literal notranslate"><span class="pre">.grad.zero_()</span></code> 方法將 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 中的數值歸零：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tensor([0., 0., 0.])
dz/dx:  tensor([0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>接著，就可以使用原先的程式碼，計算 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial x}\)</span> 與 <span class="math notranslate nohighlight">\(\frac{\partial z}{\partial y}\)</span>：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dx: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dz/dy: &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>dz/dx:  tensor([-8.,  0.,  8.])
dz/dy:  tensor([-4.,  0.,  4.])
</pre></div>
</div>
</div>
</div>
<p>不過要特別注意的是，如果計算圖沒有重新建立，連續進行兩次 <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> 會引發錯誤的訊息。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">5.1.3. </span>可獲得梯度張量之進階控制<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>一個張量是否有被追朔以計算梯度，除了直接列印外，亦可透過 <code class="docutils literal notranslate"><span class="pre">.requires_grad</span></code> 此屬性來觀看</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>如果想將一原先沒有要求梯度之張量，改為需要梯度時，可以使用 <code class="docutils literal notranslate"><span class="pre">.requires_grad_()</span></code> 此方法原地修改該向量的 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 類型：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>如果想將張量 <code class="docutils literal notranslate"><span class="pre">x</span></code> 拷貝到另一變量 <code class="docutils literal notranslate"><span class="pre">x_ng</span></code>，卻不希望 <code class="docutils literal notranslate"><span class="pre">x_ng</span></code> 的計算會被追朔時，可以使用以下的程式碼：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x_ng</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_ng</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>最後，如果希望可獲得梯度之向量後續的計算歷程不被追朔的話，可以將計算程式碼置於 <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad():</span></code> 此環境中，即</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>False
False
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">5.2. </span>數值優化<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3><span class="section-number">5.2.1. </span>手動撰寫優化算則<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>前一小節所使用的範例，其計算過程可以寫為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z &amp;= f(x)\\
 &amp;= \sum_{i=1}^3\left[2(x_i-2)\right]^2 \\
 &amp;= \sum_{i=1}^3 y^2
\end{aligned}
\end{split}\]</div>
<p>若想要找到一 <span class="math notranslate nohighlight">\(\widehat{x}\)</span>，其使得 <span class="math notranslate nohighlight">\(f(\widehat{x})\)</span> 達到最小值的話，由於 <span class="math notranslate nohighlight">\(z\)</span> 為 <span class="math notranslate nohighlight">\(y_1, y_2, y_3\)</span> 的平方和，因此，其會在 <span class="math notranslate nohighlight">\(\widehat{y} = (0, 0, 0)\)</span>的地方達到最小值，也意味著 <span class="math notranslate nohighlight">\(\widehat{x} = (2,2,2)\)</span>。</p>
<p>那麼，我們應該如何使用數值方法，對目標函數進行優化呢？令 <span class="math notranslate nohighlight">\(\theta\)</span> 表示模型參數（其扮演範例中<span class="math notranslate nohighlight">\(x\)</span>的角色），<span class="math notranslate nohighlight">\(\mathcal{D}(\theta)\)</span> 表示度量模型好壞的目標函數（其扮演<span class="math notranslate nohighlight">\(f(x)\)</span>的角色）。根據梯度下降（gradient descent）法，極小元（minimizer）<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 的更新規則為</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta} \leftarrow \widehat{\theta} - s \times \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta}
\]</div>
<p>這裏，<span class="math notranslate nohighlight">\(s\)</span> 表示一步伐大小（step size），或稱學習速率（learning rate）。一般來說，當 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 足夠圓滑（smooth），且 <span class="math notranslate nohighlight">\(s\)</span> 的數值大小適切時，梯度下降法能夠找到一臨界點（critical points），其可能為 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 最小值的發生位置。</p>
<p>在開始進行梯度下降前，我們先定義一函數 <code class="docutils literal notranslate"><span class="pre">f</span></code> 使得<code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">f(x)</span></code>，並了解起始狀態時，<code class="docutils literal notranslate"><span class="pre">f(x)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">x</span></code> 的數值為何：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 8.000, x = tensor([1., 2., 3.])
</pre></div>
</div>
</div>
</div>
<p>使用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行梯度下降，需先計算在當下 <code class="docutils literal notranslate"><span class="pre">x</span></code> 數值下的梯度，接著，根據該梯度的訊息與設定的步伐大小對 <code class="docutils literal notranslate"><span class="pre">x</span></code> 進行更新，即</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">x</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
</pre></div>
</div>
</div>
</div>
<p>這裡，我們將學習速率 <code class="docutils literal notranslate"><span class="pre">lr</span></code> 設為0.1，而張量的 <code class="docutils literal notranslate"><span class="pre">.sub_()</span></code> 方法則是就地減去括號內的數值直接更新。透過 <code class="docutils literal notranslate"><span class="pre">f(x)</span></code> 的數值，可觀察到梯度下降的確導致 <code class="docutils literal notranslate"><span class="pre">z</span></code> 數值的下降，而 <code class="docutils literal notranslate"><span class="pre">x</span></code> 也與 <span class="math notranslate nohighlight">\(\widehat{x}=(2,2,2)\)</span> 更加地靠近。</p>
<p>梯度下降的算則，需重複前述的程序多次，才可獲得一收斂的解。最簡單的方法，即使用 <code class="docutils literal notranslate"><span class="pre">for</span></code> 迴圈，重複更新<span class="math notranslate nohighlight">\(I\)</span>次：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">):</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter </span><span class="si">{:2.0f}</span><span class="s2">, f(x) = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>iter  1, f(x) = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, f(x) = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, f(x) = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, f(x) = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, f(x) = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, f(x) = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, f(x) = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 11, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 12, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 13, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 14, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 15, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 16, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 17, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 18, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 19, f(x) = 0.000, x = tensor([2., 2., 2.])
iter 20, f(x) = 0.000, x = tensor([2., 2., 2.])
</pre></div>
</div>
</div>
</div>
<p>然而，從列印出來的結果來看，20次迭代可能太多了，因此，我們可以進一步要求當梯度絕對值小於某收斂標準 <code class="docutils literal notranslate"><span class="pre">tol</span></code> 時，算則就停止，其所對應之程式碼為：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">x</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter </span><span class="si">{:2.0f}</span><span class="s2">, z = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">):</span>
        <span class="k">break</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="torch-optim">
<h3><span class="section-number">5.2.2. </span>使用<code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>進行優化<a class="headerlink" href="#torch-optim" title="Permalink to this headline">¶</a></h3>
<p>由於 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 已內建了進行優化的方法，因此，在絕大多數的情況下，可直接利用 <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> 的類型來求得函數的最小值。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> 為進行梯度下降法之物件，由於 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 主要用於進行深度學習，在該領域種主要使用的是隨機梯度下降（stochastic gradient descent）或是迷你批次梯度下降（mini-batch gradient descent）來強化優化的效能，因此，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 使用 <code class="docutils literal notranslate"><span class="pre">SGD</span></code> 一詞。事實上，除了計算一階導數時資料量的差異外，<code class="docutils literal notranslate"><span class="pre">SGD</span></code> 與傳統的梯度下降並無差異。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> 可透過以下的程式碼來使用：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">((</span><span class="n">x</span><span class="p">,),</span> <span class="n">lr</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter </span><span class="si">{:2.0f}</span><span class="s2">, z = </span><span class="si">{:2.3f}</span><span class="s2">, x = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">):</span>
        <span class="k">break</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>iter  1, z = 0.320, x = tensor([1.8000, 2.0000, 2.2000])
iter  2, z = 0.013, x = tensor([1.9600, 2.0000, 2.0400])
iter  3, z = 0.001, x = tensor([1.9920, 2.0000, 2.0080])
iter  4, z = 0.000, x = tensor([1.9984, 2.0000, 2.0016])
iter  5, z = 0.000, x = tensor([1.9997, 2.0000, 2.0003])
iter  6, z = 0.000, x = tensor([1.9999, 2.0000, 2.0001])
iter  7, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  8, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter  9, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
iter 10, z = 0.000, x = tensor([2.0000, 2.0000, 2.0000])
</pre></div>
</div>
</div>
</div>
<p>這裡，我們使用 <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> 來生成優化器（optimizer）物件 <code class="docutils literal notranslate"><span class="pre">opt</span></code>，其在生成時，需要指定其追朔的變量，並以一可迭代的物件（iterable object）作為輸入，因此，在該程式碼中，我們將 <code class="docutils literal notranslate"><span class="pre">x</span></code> 以一元組（tuple）的方式來輸入，並指定學習速率 <code class="docutils literal notranslate"><span class="pre">lr=.1</span></code>。使用內建優化器時，仍須手動對目標函數執行 <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>，但更新估計值的步驟，可使用優化器的 <code class="docutils literal notranslate"><span class="pre">.step()</span></code> 來進行，而消除變量的 <code class="docutils literal notranslate"><span class="pre">.grad</span></code>，則可使用優化器的 <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code> 方法。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> 容許使用者加入動能（momentum）<span class="math notranslate nohighlight">\(m\)</span>（其預設為 0），此時，優化算則會改為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\delta &amp; \leftarrow m \times  \delta + \frac{\partial \mathcal{D}(\widehat{\theta})}{\partial \theta} \\
\widehat{\theta} &amp;\leftarrow \widehat{\theta} - s \times \delta
\end{aligned}
\end{split}\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\delta\)</span> 表示更新的方向。此算則中，更新的方向不單單倚賴當下目標函數的梯度，其亦考慮到先前的梯度方向，因此，引入動能會使得求解的路徑更為平滑。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> 中，有許多不同的優化器（見<a class="reference external" href="https://pytorch.org/docs/stable/optim.html"><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>頁面</a>），如</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Adadelta</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adagrad</span></code>（見<a class="reference external" href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adam</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LBFGS</span></code>（見<a class="reference external" href="https://doi.org/10.1007/BF01589116">On the limited memory BFGS method for large scale optimization</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adadelta</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RMSprop</span></code>（見<a class="reference external" href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a>）</p></li>
</ul>
<p>讀者可自行深入了解這些方法。</p>
</div>
<div class="section" id="lbfgs">
<h3><span class="section-number">5.2.3. </span>使用LBFGS法進行優化<a class="headerlink" href="#lbfgs" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">((</span><span class="n">x</span><span class="p">,),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
                        <span class="n">tolerance_grad</span> <span class="o">=</span> <span class="n">tol</span><span class="p">,</span>
                        <span class="n">line_search_fn</span> <span class="o">=</span> <span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;descent!!!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>
<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>descent!!!
descent!!!
descent!!!
</pre></div>
</div>
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(8., grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="c1">### 計算黑塞矩陣</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd.functional</span> <span class="kn">import</span> <span class="n">hessian</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                 <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">z</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 6.,  0.,  0.],
        [ 0., 12.,  0.],
        [ 0.,  0., 18.]])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">5.3. </span>實徵範例<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="logistic-regression.html" title="previous page"><span class="section-number">4. </span>邏輯斯迴歸</a>
    <a class='right-next' id="next-link" href="probability-distribution.html" title="next page"><span class="section-number">6. </span>機率分佈</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>