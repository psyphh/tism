

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>7. 最大概似法 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Lab: 最大概似估計" href="lab-torch-mle.html" />
    <link rel="prev" title="6. 機率分佈" href="probability-distribution.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-tensor.html">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic-regression.html">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-diff-opt.html">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-mle.html">
   8. Lab: 最大概似估計
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="latent-variable-modeling.html">
   9. 潛在變項建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="factor-analysis.html">
   10. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   11. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixture-modeling.html">
   12. 混合建模
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-pyro.html">
   13. Lab:
   <code class="docutils literal notranslate">
    <span class="pre">
     pyro
    </span>
   </code>
   簡介
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-tf-example.html">
   14. Lab:
   <code class="docutils literal notranslate">
    <span class="pre">
     tensoflow
    </span>
   </code>
   範例
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/maximum-likelihood.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/maximum-likelihood.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   7.1. 最大概似估計
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     7.1.1. 估計的基本概念
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     7.1.2. 概似函數與最大概似估計式
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   7.2. 最大概似估計範例
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     7.2.1. 伯努利分配成功機率之估計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     7.2.2. 常態分配之平均數與變異數估計
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     7.2.3. 線性迴歸之最大概似估計
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   7.3. 最大概似估計式之理論性質
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     7.3.1. 一致性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     7.3.2. 大樣本常態性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     7.3.3. 不變性
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1><span class="section-number">7. </span>最大概似法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>在前一章節，我們提到所有隨機的現象，儘管我們無法事先得知其實現值為何，但我們可以透過其分配函數來了解行為表現。而許多的統計建模問題，我們試圖對變項的行為表現提出一參數化的機率模型，接著，收集樣本資料以對模型之參數進行估計。這裡，參數（parameter）指的是決定機率分佈形狀之量，像是二項式分配的成功機率、常態分配的平均數與變異數，皆為模型之參數。在此章節中，若一隨機變數 <span class="math notranslate nohighlight">\(X\)</span> 的PMF/PDF仰賴某參數 <span class="math notranslate nohighlight">\(\theta\)</span> 的話，我們將其 PMF/PDF 寫為 <span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span>。</p>
<p>估計模型參數為統計的核心議題，在諸多參數估計方法中，最大概似（maximum likelihood，簡稱ML）法可說是最為重要的一種方法，在本章節，我們將學習如何進行最大概似法進行參數估計。</p>
<div class="section" id="id2">
<h2><span class="section-number">7.1. </span>最大概似估計<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">7.1.1. </span>估計的基本概念<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>在初等統計學當中，同學們應該已具有基本的估計概念，舉例來說，樣本平均數 <span class="math notranslate nohighlight">\(m\)</span> 乃作為母群體平均數 <span class="math notranslate nohighlight">\(\mu\)</span> 之估計，樣本變異數 <span class="math notranslate nohighlight">\(s^2\)</span> 則是作為母群體變異數 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 之估計。我們可以看見，無論是 <span class="math notranslate nohighlight">\(m\)</span> 或是 <span class="math notranslate nohighlight">\(s^2\)</span>，其都是樣本資料的函數，因此，很寬鬆的來說，任何樣本資料的函數都被稱作為估計式（estimator，可以理解為公式），而在特定資料數值下所算出來的結果，則被稱作估計量或估計值（estimate，可以理解為數值）。</p>
<p>在此章節中，我們使用 <span class="math notranslate nohighlight">\(\theta\)</span> 來表示參數，而 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 則用於表示對 <span class="math notranslate nohighlight">\(\theta\)</span> 的估計式或估計值（適情況而定。）</p>
<p>一個估計式是不是夠好，是需要進一步說明的。舉例來說，<span class="math notranslate nohighlight">\(m\)</span> 或是 <span class="math notranslate nohighlight">\(s^2\)</span> 分別是 <span class="math notranslate nohighlight">\(\mu\)</span> 與 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的不偏估計式（unbiased estimator），這意味著 <span class="math notranslate nohighlight">\(\mathbb{E}(m) = \mu\)</span> 與 <span class="math notranslate nohighlight">\(\mathbb{E}(s^2) = \sigma^2\)</span>。因此，當 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 滿足以下條件時，我們說 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 為 <span class="math notranslate nohighlight">\(\theta\)</span> 的不偏估計式：</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(\widehat{\theta}) = \theta
\]</div>
<p>然而，不偏性（unbiasedness）只是評估估計式好壞的其中一標準而已，事實上，隨便挑選資料中的一筆出來，其也能作為平均數的不偏估計（<span class="math notranslate nohighlight">\(\mathbb{E}(X_n) = \mu\)</span>），因此，不偏性在實務上並非是個重要的標準，相較之下，統計學家更常使用均方誤（mean squared error，簡稱MSE）來評估一估計式的好壞：</p>
<div class="math notranslate nohighlight">
\[
MSE(\widehat{\theta}) = \mathbb{E}(|\widehat{\theta} - \theta|^2)
\]</div>
<p>一般來說，MSE越小則意味該估計式的品質越好，不過在解讀 MSE 的意涵時要特別小心，事實上，MSE 包含了變異與偏誤兩個成分</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
MSE(\widehat{\theta}) &amp;= \mathbb{E}(|\widehat{\theta} - \theta|^2) \\
&amp;=\mathbb{E}\left\{ | [\widehat{\theta}- \mathbb{E}(\widehat{\theta})] + [\mathbb{E}(\widehat{\theta})  - \theta]|^2 \right\} \\
&amp;=\mathbb{E}\left\{ [\widehat{\theta}- \mathbb{E}(\widehat{\theta})]^2 + 2 [\widehat{\theta}- \mathbb{E}(\widehat{\theta})][\mathbb{E}(\widehat{\theta})  - \theta] + [\mathbb{E}(\widehat{\theta})  - \theta]^2 \right\}\\
&amp;=\mathbb{E}\left\{ [\widehat{\theta}- \mathbb{E}(\widehat{\theta})]^2 \right\} +
\mathbb{E}\left\{  2 [\widehat{\theta}- \mathbb{E}(\widehat{\theta})][\mathbb{E}(\widehat{\theta})  - \theta]\right\}+
\mathbb{E}\left\{ [\mathbb{E}(\widehat{\theta})  - \theta]^2 \right\} \\
&amp; = \mathbb{V}\text{ar} (\widehat{\theta}) + [\mathbb{E}(\widehat{\theta})  - \theta]^2
\end{aligned}
\end{split}\]</div>
<p>因此，在評估一估計式的表現時，建議可以將變異與偏誤兩部分分開來評估。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">7.1.2. </span>概似函數與最大概似估計式<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math notranslate nohighlight">\(X_1, X_2,...,X_N\)</span> 表示一來自 <span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 此分配之隨機樣本（random sample），這意味著：</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_n\)</span> 與 <span class="math notranslate nohighlight">\(X_{n'}\)</span> 彼此相互獨立（<span class="math notranslate nohighlight">\(n \neq n'\)</span>）；</p></li>
<li><p>對於每筆觀測值 <span class="math notranslate nohighlight">\(X_n\)</span> 來說，其分配皆為 <span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span>，此分配的特性由模型參數 <span class="math notranslate nohighlight">\(\theta\)</span> 所控制。</p></li>
</ol>
<p>若此樣本的實現值為為 <span class="math notranslate nohighlight">\(X_1=x_1,X_2=x_2,...,X_N=x_N\)</span>，則奠基於<span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 此分配家族與前述資料之概似函數（likelihood function）為</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta; x_1,x_2,...,x_N)= \prod_{n=1}^N f_X(x_n|\theta)
\]</div>
<p>此概似函數反映了在當下的參數數值 <span class="math notranslate nohighlight">\(\theta\)</span> 之下，我們觀察到 <span class="math notranslate nohighlight">\(X_1=x_1,X_2=x_2,...,X_N=x_N\)</span> 這筆資料的可能性，因此，一種合理的估計策略為，試圖找到一參數估計式 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 使得觀察到此樣本資料之可能性達到最大，意即</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\widehat{\theta}; x_1,x_2,...,x_N) = \max_{\theta} \mathcal{L}(\theta; x_1,x_2,...,x_N)
\]</div>
<p>而此 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 則被稱作最大概似估計式（maximum likelihood estimator，簡稱MLE）。</p>
<p>由於 <span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 數值常小於 1，因此，<span class="math notranslate nohighlight">\(\prod_{n=1}^N f_X(x_n|\theta)\)</span> 會令概似函數的數值變得很靠近 0，此外，連續相乘在數學上處理難度較高，因此，在實務上研究者主要處理對數概似函數，即</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\theta) &amp;=\log \mathcal{L}(\theta; x_1,x_2,...,x_N) \\
&amp;= \sum_{n=1}^N \log f_X(x_n|\theta)
\end{aligned}
\end{split}\]</div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">7.2. </span>最大概似估計範例<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3><span class="section-number">7.2.1. </span>伯努利分配成功機率之估計<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math notranslate nohighlight">\(X_1, X_2,...,X_N\)</span> 表示一來自 <span class="math notranslate nohighlight">\(f_X(x|\pi) = \pi^x (1-\pi)^{(1-x)}\)</span> 之隨機樣本，則其對應之對數概似函數為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\pi) &amp; = \sum_{n=1}^N \log f_X(x_n|\pi) \\
&amp; = \sum_{n=1}^N \log\left[ \pi^{x_n} (1-\pi)^{(1-x_n)} \right] \\
&amp; = \sum_{n=1}^N \left[ x_n \log\pi + (1-x_n)\log(1-\pi) \right] \\
\end{aligned}
\end{split}\]</div>
<p>若要求得該概似函數的極大元（maximizer），等同於求得該函數取負號的極小元（minimizer），我們可計算其一階導數</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{d}{d \pi} \ell(\pi)
&amp;=  \sum_{n=1}^N \frac{d}{d \pi} \left[ -x_n \log\pi - (1-x_n)\log(1-\pi) \right] \\
&amp;=  \sum_{n=1}^N  \left[ -x_n \frac{d}{d \pi} \log\pi - (1-x_n) \frac{d}{d \pi} \log(1-\pi) \right] \\
&amp;=  \sum_{n=1}^N  \left[ -x_n \frac{1}{\pi} - (1-x_n)\frac{1}{1-\pi}  \frac{d}{d \pi} \log(1-\pi) \right] \\
&amp;=  \sum_{n=1}^N  \left[  \frac{-x_n}{\pi} + \frac{1-x_n}{1-\pi}   \right] \\
&amp;=  \frac{-1}{\pi}\sum_{n=1}^N x_n  + \frac{1}{1-\pi} \sum_{n=1}^N (1-x_n)    \\
\end{aligned}
\end{split}\]</div>
<p>因此，<span class="math notranslate nohighlight">\(\widehat{\pi}\)</span> 須滿足</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
 (1-\widehat{\pi})\sum_{n=1}^N x_n  = \widehat{\pi} (N - \sum_{n=1}^N x_n) \iff
 \sum_{n=1}^N x_n  =  \widehat{\pi} N  \\
\end{aligned}
\end{split}\]</div>
<p>我們可得 <span class="math notranslate nohighlight">\(\widehat{\pi} = \frac{1}{N} \sum_{n=1}^N x_n\)</span>，意即，<span class="math notranslate nohighlight">\(\widehat{\pi}\)</span> 為樣本比率（sample proportion）。</p>
<p>若我們進一步計算 <span class="math notranslate nohighlight">\(-\ell(\widehat{\pi})\)</span> 的二階微分，則可得</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{d^2}{d \pi^2} \ell(\widehat{\pi})
&amp;=  \frac{d}{d \pi}\frac{-1}{\widehat{\pi}}\sum_{n=1}^N x_n  + \frac{d}{d \pi} \frac{1}{1-\widehat{\pi}} \sum_{n=1}^N (1-x_n)    \\
&amp;=  \frac{N}{\widehat{\pi}}  + \frac{N}{1-\widehat{\pi}}    \\
&amp;=  \frac{N(1-\widehat{\pi}) + N \widehat{\pi}}{\widehat{\pi}(1-\widehat{\pi})}  \\
&amp;=  \frac{N}{\widehat{\pi}(1-\widehat{\pi})} \geq 0 \\
\end{aligned}
\end{split}\]</div>
<p>由於此二階導數在 <span class="math notranslate nohighlight">\(\pi = \widehat{\pi}\)</span> 大於 0，因此，<span class="math notranslate nohighlight">\(\widehat{\pi}\)</span> 確實最小化了<span class="math notranslate nohighlight">\(-\ell(\widehat{\pi})\)</span> 此取負號的對數概似函數。</p>
</div>
<div class="section" id="id7">
<h3><span class="section-number">7.2.2. </span>常態分配之平均數與變異數估計<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math notranslate nohighlight">\(X_1, X_2,...,X_N\)</span> 表示一來自 <span class="math notranslate nohighlight">\(f_X(x|\mu, \sigma^2)  = \frac{1}{\sqrt{2 \pi} \sigma_X} e^{-(x - \mu)^2/2 \sigma^2}\)</span> 之隨機樣本，則其對應之對數概似函數為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\mu, \sigma^2) &amp; = \sum_{n=1}^N \log f_X(x_n|\mu, \sigma^2) \\
&amp; = \sum_{n=1}^N \log\left[ \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x_n - \mu)^2/2 \sigma^2} \right] \\
&amp; = \sum_{n=1}^N \left[ -\frac{1}{2}\log{2 \pi} -\frac{1}{2}\log{\sigma^2} -\frac{(x_n - \mu)^2}{2 \sigma^2} \right] \\
&amp; =  -\frac{N}{2}\log{2 \pi} - \frac{N}{2}\log{\sigma^2} - \frac{1}{2 \sigma^2} \sum_{n=1}^N (x_n - \mu)^2  \\
\end{aligned}
\end{split}\]</div>
<p>為了計算其MLE，我們分別對 <span class="math notranslate nohighlight">\(\mu\)</span> 與 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 進行偏微分</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{\partial}{\partial \mu }\ell(\mu, \sigma^2)
&amp; = \frac{1}{2 \sigma^2} \sum_{n=1}^N  \frac{\partial}{\partial \mu } (x_n - \mu)^2  \\
&amp; =  \frac{1}{2 \sigma^2} \sum_{n=1}^N 2(x_n - \mu)\frac{\partial}{\partial \mu } (x_n - \mu) \\
&amp; =  -\frac{1}{ \sigma^2} \sum_{n=1}^N (x_n - \mu) \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{\partial}{\partial \sigma^2 }\ell(\mu, \sigma^2)
&amp; =  \frac{\partial}{\partial \sigma^2 } \frac{N}{2}\log{\sigma^2} + \frac{\partial}{\partial \sigma^2 } \frac{1}{2 \sigma^2} \sum_{n=1}^N (x_n - \mu)^2  \\
&amp; =  \frac{N}{2\sigma^2} + \frac{-1}{2 \sigma^4} \sum_{n=1}^N (x_n - \mu)^2  \\
\end{aligned}
\end{split}\]</div>
<p>因此，我們有</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
 -\frac{1}{\sigma^2} \sum_{n=1}^N (x_n - \widehat{\mu}) = 0
 \iff
\widehat{\mu} = \frac{1}{N}\sum_{n=1}^Nx_n
\end{aligned}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
 \frac{N}{2\widehat{\sigma}^2} = \frac{1}{2 \widehat{\sigma}^4} \sum_{n=1}^N (x_n - \widehat{\mu})^2
 \iff
 \widehat{\sigma}^2 = \frac{1}{N} \sum_{n=1}^N (x_n - \widehat{\mu})^2  \\
\end{aligned}
\end{split}\]</div>
<p>而此負對數概似函數的二階導數為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{\partial^2}{\partial \mu^2 }\ell(\widehat{\mu}, \sigma^2)
&amp; =  -\frac{1}{\widehat{\sigma}^2}\frac{\partial}{\partial \widehat{\mu} } \sum_{n=1}^N (x_n - \widehat{\mu}) \\
&amp; =  \frac{N}{\widehat{\sigma}^2}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{\partial^2}{\partial \sigma^4 }\ell(\widehat{\mu}, \widehat{\sigma}^2)
&amp; =  \frac{\partial}{\partial \sigma^2 }\frac{N}{2\widehat{\sigma}^2} + \frac{\partial}{\partial \sigma^2 }\frac{-1}{2 \widehat{\sigma}^4} \sum_{n=1}^N (x_n - \widehat{\mu})^2  \\
&amp; = \frac{-N}{2\widehat{\sigma}^4} + \frac{1}{\widehat{\sigma}^6} \sum_{n=1}^N (x_n - \widehat{\mu})^2  \\
&amp; =  \frac{N}{2\widehat{\sigma}^4}  \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{\partial^2}{\partial \mu \partial \sigma^2}\ell(\widehat{\mu}, \widehat{\sigma}^2)
&amp; =  \frac{\partial}{\partial \widehat{\sigma}^2}-\frac{1}{ \sigma^2} \sum_{n=1}^N (x_n - \widehat{\mu}) \\
&amp; = \frac{1}{ \widehat{\sigma}} \sum_{n=1}^N (x_n - \widehat{\mu}) \\
&amp; = 0
\end{aligned}
\end{split}\]</div>
<p>因此，負對數概似函數的黑塞矩陣為</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\nabla^2 \ell(\widehat{\mu}, \widehat{\sigma}^2)
&amp;= -
\begin{pmatrix}
\frac{\partial^2}{\partial \mu^4}\ell(\widehat{\mu}, \widehat{\sigma}^2)  &amp; \frac{\partial^2}{\partial \mu \partial \sigma^2}\ell(\widehat{\mu}, \widehat{\sigma}^2) \\
\frac{\partial^2}{\partial \sigma^2 \partial \mu }\ell(\widehat{\mu}, \widehat{\sigma}^2) &amp;  \frac{\partial^2}{\partial \sigma^4}\ell(\widehat{\mu}, \widehat{\sigma}^2)
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
 \frac{N}{\widehat{\sigma}^2} &amp; 0 \\
 0 &amp;  \frac{N}{2\widehat{\sigma}^4}
\end{pmatrix}
\end{aligned}
\end{split}\]</div>
</div>
<div class="section" id="id8">
<h3><span class="section-number">7.2.3. </span>線性迴歸之最大概似估計<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>在線性廻歸模型下，我們使用 <span class="math notranslate nohighlight">\(x\)</span> 對於 <span class="math notranslate nohighlight">\(y\)</span> 進行預測。假設在給定 <span class="math notranslate nohighlight">\(x\)</span> 之下，<span class="math notranslate nohighlight">\(y\)</span> 的條件分佈為</p>
<div class="math notranslate nohighlight">
\[
y|x \sim \text{Normal}(w_0 + \sum_{p=1}^P w_p x_p, \sigma_{\epsilon}^2)
\]</div>
<p>則給定一樣本資料 <span class="math notranslate nohighlight">\(\{(y_n,x_n)\}_{n=1}^N\)</span>，我們可以使用前述之條件分佈來建立參數之概似函數</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(w, \sigma_{\epsilon}^2)
&amp;= \sum_{n=1}^N \log f_{Y|X}(y_n| x_n, w, \sigma_{\epsilon}^2)\\
&amp;= \sum_{n=1}^N \log\left[ \frac{1}{\sqrt{2 \pi} \sigma_{\epsilon}} e^{-\frac{1}{2 \sigma_{\epsilon}^2}\left (y_n - w_0 - \sum_{p=1}^P w_p x_{np} \right)^2} \right] \\
&amp;= \sum_{n=1}^N \left[-\frac{1}{2}\log{2 \pi} -\frac{1}{2}\log{\sigma_{\epsilon}} - {\frac{1}{2 \sigma_{\epsilon}^2}\left (y_n - w_0 - \sum_{p=1}^P w_p x_{np} \right)^2} \right] \\
&amp;=  -\frac{N}{2}\log{2 \pi} -\frac{N}{2}\log{\sigma_{\epsilon}} - \frac{1}{2 \sigma_{\epsilon}^2} \sum_{n=1}^N \left (y_n - w_0 - \sum_{p=1}^P w_p x_{np} \right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>事實上，先前所使用之最小平方法適配函數，其可以寫為對負號對數概述函數標準化後之結果</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}(w) \propto - \frac{1}{N} \ell(w, \sigma_{\epsilon}^2)
\]</div>
</div>
</div>
<div class="section" id="id9">
<h2><span class="section-number">7.3. </span>最大概似估計式之理論性質<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>在滿足以下的條件時，MLE具有一致性、大樣本常態性、與不變性等性質</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1,X_2,...,X_n\)</span> 為來自 <span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 之獨立且相同分配之樣本。</p></li>
<li><p>參數為可辨識（identifiable），意即，當 <span class="math notranslate nohighlight">\(\theta \neq \theta'\)</span> 時，<span class="math notranslate nohighlight">\(f_X(x|\theta) \neq f_X(x|\theta')\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 此函數具有相同的支撐（support，意即，<span class="math notranslate nohighlight">\(x\)</span> 可數值範圍與 <span class="math notranslate nohighlight">\(\theta\)</span> 無關）。</p></li>
<li><p>參數真實數值 <span class="math notranslate nohighlight">\(\theta^*\)</span> 位於參數空間之內點（interior point）。</p></li>
<li><p><span class="math notranslate nohighlight">\(f_X(x|\theta)\)</span> 為三次連續可微分，<span class="math notranslate nohighlight">\(\int f_X(x|\theta)dx\)</span> 為三次可微。</p></li>
<li><p>對於任一的 <span class="math notranslate nohighlight">\(\theta^*\)</span>，存在一<span class="math notranslate nohighlight">\(\delta&gt;0\)</span>與函數 <span class="math notranslate nohighlight">\(M(x)\)</span>，使得對於所有的 <span class="math notranslate nohighlight">\(x\)</span> 與 <span class="math notranslate nohighlight">\(\theta \in [\theta^* - \delta, \theta^* + \delta]\)</span>，滿足</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
|\frac{\partial^3}{\partial \theta^3} \log f_X(x|\theta)| \leq M(x)
\]</div>
<p>且 <span class="math notranslate nohighlight">\(\mathbb{E}(M(X)) &lt; \infty\)</span>。</p>
<div class="section" id="id10">
<h3><span class="section-number">7.3.1. </span>一致性<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>MLE具有一致性（consistency），意即，當樣本數夠大時，<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 會跟參數的真實數值 <span class="math notranslate nohighlight">\(\theta^*\)</span> 很靠近。若用數學概念來表示的話，即給定任何的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span>，我們有</p>
<div class="math notranslate nohighlight">
\[
\lim_{N \to \infty } \mathbb{P}(|\widehat{\theta} - \theta^*| &gt; \epsilon) = 0
\]</div>
<p>這邊須要特別注意的是，<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 事實上是樣本數的函數。前述的數學條件，可以簡單地寫為 <span class="math notranslate nohighlight">\(\widehat{\theta} \to_{P} \theta^*\)</span>，表示 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 於機率收斂於 <span class="math notranslate nohighlight">\(\theta^*\)</span>（<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> converges to <span class="math notranslate nohighlight">\(\theta^*\)</span> in probability）。</p>
</div>
<div class="section" id="id11">
<h3><span class="section-number">7.3.2. </span>大樣本常態性<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>MLE具有大樣本常態性（consistency），意即，當樣本數夠大時，<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 會呈現多元常態分配，其平均數與共變異數矩陣為 <span class="math notranslate nohighlight">\(\theta^*\)</span> 與 <span class="math notranslate nohighlight">\(\mathcal{I}(\theta^*)^{-1}\)</span>，意即</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta} \sim \text{Normal}\left[\theta^*, \frac{1}{N} \mathcal{I}(\theta^*)^{-1} \right]
\]</div>
<p>這裡，<span class="math notranslate nohighlight">\(\mathcal{I}(\theta^*)= \mathbb{E}\left[-\nabla^2 \frac{1}{N} \ell(\widehat{\theta}) \right]\)</span>，其被稱作費雪期望訊息矩陣（Fisher’s expected information matrix）。</p>
<p>此外，由於 <span class="math notranslate nohighlight">\(\frac{1}{N} \mathcal{I}(\theta^*)^{-1}\)</span> 此共變異數矩陣為理論上變異性最小的，其達到了所謂的Cramer-Rao下界，因此，MLE具有所謂的有效性（efficiency），意即，其大樣本時的MSE，是所有估計式中最小的。</p>
</div>
<div class="section" id="id12">
<h3><span class="section-number">7.3.3. </span>不變性<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>如果 <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> 為 <span class="math notranslate nohighlight">\(\theta^*\)</span> 之MLE，令 <span class="math notranslate nohighlight">\(g\)</span> 表示一函數，其將 <span class="math notranslate nohighlight">\(\theta\)</span> 轉換為 <span class="math notranslate nohighlight">\(\vartheta\)</span>，即<span class="math notranslate nohighlight">\(\vartheta = g(\theta)\)</span>，則<span class="math notranslate nohighlight">\(g(\widehat{\theta})\)</span> 即為 <span class="math notranslate nohighlight">\(g(\theta^*)\)</span> 之MLE。</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="probability-distribution.html" title="previous page"><span class="section-number">6. </span>機率分佈</a>
    <a class='right-next' id="next-link" href="lab-torch-mle.html" title="next page"><span class="section-number">8. </span>Lab: 最大概似估計</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>