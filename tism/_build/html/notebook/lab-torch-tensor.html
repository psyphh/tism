

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>3. Lab: 張量與線性代數 &#8212; 統計建模技法</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. 邏輯斯迴歸" href="logistic-regression.html" />
    <link rel="prev" title="2. 線性迴歸" href="linear-regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">統計建模技法</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math-prerequisite.html">
   1. 先備數學知識
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-regression.html">
   2. 線性迴歸
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Lab: 張量與線性代數
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic-regression.html">
   4. 邏輯斯迴歸
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-diff-opt.html">
   5. Lab: 數值微分與優化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distribution.html">
   6. 機率分佈
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="maximum-likelihood.html">
   7. 最大概似法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab-torch-mle.html">
   8. Lab: 最大概似估計
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="true-score-model.html">
   9. 真實分數模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="factor-analysis.html">
   10. 因素分析
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="item-response-theory.html">
   11. 試題反應理論
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixture-modeling.html">
   12. 混合建模
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebook/lab-torch-tensor.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/psyphh/tism/blob/master/tism/notebook/lab-torch-tensor.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.1. 張量之基礎
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     3.1.1. 張量之輸入
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     3.1.2. 張量之數值
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     3.1.3. 張量之形狀
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     3.1.4. 張量之資料類型
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   3.2. 張量之操弄
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     3.2.1. 張量之切片
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     3.2.2. 張量之串接
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   3.3. 張量之運算
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     3.3.1. 張量元素對元素之運算
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     3.3.2. 張量線性代數之運算
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     3.3.3. 對張量之數值進行摘要
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   3.4. 張量運算之進階議題
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     3.4.1. 廣播
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpu">
     3.4.2. GPU運算
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id15">
   3.5. 實作範例與練習
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     3.5.1. 產生線性迴歸資料
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     3.5.2. 計算模型參數
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     3.5.3. 建立迴歸分析物件
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     3.5.4. 練習
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="lab">
<h1><span class="section-number">3. </span>Lab: 張量與線性代數<a class="headerlink" href="#lab" title="Permalink to this headline">¶</a></h1>
<p>此 lab 中，我們將會透過 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 此套件，學習以下的主題。</p>
<ol class="simple">
<li><p>認識 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 的張量（tensor）之基礎。</p></li>
<li><p>了解如何對 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 張量進行操弄。</p></li>
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行線性代數之運算。</p></li>
<li><p>應用前述之知識，建立一可進行線性迴歸分析之類型（class）。</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code>之安裝與基礎教學，可參考 <a class="reference external" href="https://pytorch.org/get-started/locally">PyTorch官方網頁</a>，如果讀者使用的是Google的Colab服務，則不需要另外安裝 <code class="docutils literal notranslate"><span class="pre">torch</span></code>。在安裝完 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 後，可透過以下的指令載入</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">3.1. </span>張量之基礎<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3><span class="section-number">3.1.1. </span>張量之輸入<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 最基本的物件是張量（tensor），其與 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 <code class="docutils literal notranslate"><span class="pre">python</span></code> 的 <code class="docutils literal notranslate"><span class="pre">list</span></code> 或是 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 的 <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>），置於<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>函數中</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span>
<span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Tensor
</pre></div>
</div>
</div>
</div>
<p>透過 <code class="docutils literal notranslate"><span class="pre">type()</span></code>，可看見其屬於 <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 此一類型（class），若欲了解 <code class="docutils literal notranslate"><span class="pre">a</span></code> 的樣貌，我們可使用 <code class="docutils literal notranslate"><span class="pre">print</span></code> 指令來列印其主要的內容</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
</pre></div>
</div>
</div>
</div>
<p>透過對 <code class="docutils literal notranslate"><span class="pre">a</span></code> 列印的結果，我們可觀察到：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 內部的資料數值（value）為 <code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4],</span> <span class="pre">[5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">8],</span> <span class="pre">[9,</span> <span class="pre">10,</span> <span class="pre">11,</span> <span class="pre">12]]</span></code>。</p></li>
</ul>
<p>除此之外，<code class="docutils literal notranslate"><span class="pre">a</span></code> 還有兩個重要的屬性並未顯示在列印的結果中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 的尺寸（size）為 <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code>，表示 <code class="docutils literal notranslate"><span class="pre">a</span></code> 為一 <span class="math notranslate nohighlight">\(3 \times 4\)</span> 之 2d 張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 的資料類型（data type）為 <code class="docutils literal notranslate"><span class="pre">int64</span></code>，表示64位元的整數。在進行運算時，張量間的類型須相同。</p></li>
</ul>
<p>稍後，我們會討論如何獲得 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 張量的尺寸與資料類型。</p>
</div>
<div class="section" id="id3">
<h3><span class="section-number">3.1.2. </span>張量之數值<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>若要擷取 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 張量的資料數值（value），則可透過 <code class="docutils literal notranslate"><span class="pre">.numpy()</span></code>獲得，其回傳該張量對應之 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 陣列</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;data of tensor is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>data of tensor is: 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">numpy</span></code> 陣列是 <code class="docutils literal notranslate"><span class="pre">python</span></code> 進行科學運算時，幾乎都會仰賴的資料格式。因此，<code class="docutils literal notranslate"><span class="pre">.numpy()</span></code> 此指令主要用於不同套件間的資料交換，或是希望列印出來的結果比較簡單使用。</p>
<p>在形成張量時，要記得張量的變數名稱，僅為其表徵資料的一個標籤，而相同的資料，可以有許多不同的標籤指稱其。舉例來說，考慮一下的程式碼</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor c is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor d is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">d</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor c is: 
 [[1 1 1]
 [2 2 2]]
tensor d is: 
 [[1 1 1]
 [2 2 2]]
</pre></div>
</div>
</div>
</div>
<p>沒有意外的，<code class="docutils literal notranslate"><span class="pre">c</span></code> 和 <code class="docutils literal notranslate"><span class="pre">d</span></code> 內部的數值是一樣的。然而，若我們利用 <code class="docutils literal notranslate"><span class="pre">.fill_()</span></code> 方法，將 <code class="docutils literal notranslate"><span class="pre">c</span></code> 的內部全部填入 0 的話，則我們可以看到不僅是 <code class="docutils literal notranslate"><span class="pre">c</span></code>，<code class="docutils literal notranslate"><span class="pre">d</span></code> 內部的資料亦改變了。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor c is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor d is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">d</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor c is: 
 [[0 0 0]
 [0 0 0]]
tensor d is: 
 [[0 0 0]
 [0 0 0]]
</pre></div>
</div>
</div>
</div>
<p>在此，讀者需特別注意的是，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 中若有方法的尾端是底線 <code class="docutils literal notranslate"><span class="pre">_</span></code> 的話，則意味著該方法會取代原有物件中的資料，如 <code class="docutils literal notranslate"><span class="pre">c</span></code> 此張量對應的資料直接被取代掉了，不需要另外寫 <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">c.fill_(0)</span></code>。</p>
<p>前述的設計，主要是為了避免資料的拷貝，以減少記憶體的使用。如果說希望 <code class="docutils literal notranslate"><span class="pre">d</span></code> 表徵的資料為 <code class="docutils literal notranslate"><span class="pre">c</span></code> 對應資料的拷貝的話，可以使用 <code class="docutils literal notranslate"><span class="pre">.clone()</span></code>此方法：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>如此，就不會出現更動 <code class="docutils literal notranslate"><span class="pre">c</span></code> 的資料，<code class="docutils literal notranslate"><span class="pre">d</span></code> 也跟著動的狀況發生。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 內建了多種函數，以協助產生具有特別數值結構之張量：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with all elements being ones </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with all elements being zeros </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;identity-like tensor </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;diagonal matrix </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor with all elements being ones 
 [[1.]
 [1.]
 [1.]
 [1.]]
tensor with all elements being zeros 
 [[0. 0. 0.]
 [0. 0. 0.]]
identity-like tensor 
 [[1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]]
diagonal matrix 
 [[1 0 0 0]
 [0 2 0 0]
 [0 0 3 0]
 [0 0 0 4]]
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 亦可隨機地產生資料，或是直接使用尚未起始化的資料：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with random elements from uniform(0, 1) </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with uninitialized data </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor with random elements from uniform(0, 1) 
 [[0.19392657 0.45533425 0.37045574 0.64610296 0.30947196 0.9488975 ]
 [0.5414999  0.36794567 0.339603   0.77709156 0.4973094  0.08453757]]
tensor with uninitialized data 
 [[ 0.000000e+00  3.689349e+19  1.330479e+24 -4.657748e-10  9.809089e-45
   0.000000e+00]
 [ 0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00
   0.000000e+00]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">3.1.3. </span>張量之形狀<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>張量之形狀與形狀維度之數量，可透過張量物件的 <code class="docutils literal notranslate"><span class="pre">.size()</span></code>（或 <code class="docutils literal notranslate"><span class="pre">.shape</span></code>） 與 <code class="docutils literal notranslate"><span class="pre">dim</span></code> 方法來獲得</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;size of tensor is&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;size of tensor is&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dim of tensor is&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>size of tensor is torch.Size([3, 4])
size of tensor is torch.Size([3, 4])
dim of tensor is 2
</pre></div>
</div>
</div>
</div>
<p>如果要對張量的形狀進行改變的話，可透過 <code class="docutils literal notranslate"><span class="pre">.view()</span></code> 此方法獲得</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with shape (4, 3): </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with shape (2, 2, 3): </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with shape (12, 1): </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with shape (12, 1) by (-1, 1): </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor with shape (12,): </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor with shape (4, 3): 
 [[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
tensor with shape (2, 2, 3): 
 [[[ 1  2  3]
  [ 4  5  6]]

 [[ 7  8  9]
  [10 11 12]]]
tensor with shape (12, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12, 1) by (-1, 1): 
 [[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]
tensor with shape (12,): 
 [ 1  2  3  4  5  6  7  8  9 10 11 12]
</pre></div>
</div>
</div>
</div>
<p>注意，<code class="docutils literal notranslate"><span class="pre">(12,</span> <span class="pre">1)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">(12,)</span></code> 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。另外，-1表示該面向對應之尺寸，由其它面向決定。</p>
<p><code class="docutils literal notranslate"><span class="pre">.view()</span></code> 所回傳的張量，即為原本張量之資料在不同尺寸下對應之張量，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 並未對資料進行拷貝（copy）。考慮以下的程式碼</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor c is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor d is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">d</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor c is: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
tensor d is: 
 [0. 0. 0. 0. 0. 0. 0. 0.]
</pre></div>
</div>
</div>
</div>
<p>如預期地，<code class="docutils literal notranslate"><span class="pre">c</span></code> 與 <code class="docutils literal notranslate"><span class="pre">d</span></code> 內部有著相同的數值，僅差在尺寸有所不同。接著，我們將 <code class="docutils literal notranslate"><span class="pre">c</span></code> 的內部全部填入1，我們會觀察到 <code class="docutils literal notranslate"><span class="pre">d</span></code> 的數值也跟著改變了：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor c is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor d is: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">d</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor c is: 
 [[1. 1.]
 [1. 1.]
 [1. 1.]
 [1. 1.]]
tensor d is: 
 [1. 1. 1. 1. 1. 1. 1. 1.]
</pre></div>
</div>
</div>
</div>
<p>這顯示 <code class="docutils literal notranslate"><span class="pre">c</span></code> 與 <code class="docutils literal notranslate"><span class="pre">d</span></code> 背後有著共享的資料內容。</p>
<p>在0.4版之後，<code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> 此方法亦可改變張量的尺寸，但其有可能會對資料進行拷貝，不過，當資料本身的排列不具有連續性時，僅 <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> 能夠使用。因此，在 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 的<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor.view">官方文件</a>中，建議使用 <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code>此指令。</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">3.1.4. </span>張量之資料類型<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>張量的資料類型，可透過 <code class="docutils literal notranslate"><span class="pre">.dtype</span></code> 方法獲得</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;data type of tensor is&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>data type of tensor is torch.int64
</pre></div>
</div>
</div>
</div>
<p>若是要調整資料類型的話，則可透過 <code class="docutils literal notranslate"><span class="pre">.type()</span></code> 此方法：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 內建多種資料類型，包含整數類型（如 <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> 與 <code class="docutils literal notranslate"><span class="pre">torch.int64</span></code>）與浮點數類型（如 <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> 與 <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>），完整的資料類型請見 <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>文件</a>之 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 欄位。</p>
<p>在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 常用之資料類型為 <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> 與 <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>，前者所需的記憶體較小，但運算結果的數值誤差較大。</p>
<p>除了利用 <code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> 搭配 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 產生張量外，另一種產生張量的方法為，直接利用特定資料型態張量的建構式，再利用特定的方法填入數值。例如，以下的程式碼在 CPU 先產稱了一資料類型為 <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> 之張量，再利用均勻分配隨機生成資料填入：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>前述之張量建構風格，在後續討論到 GPU 計算時會很有幫助。不同資料類型之建構式，可同樣參考 <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor文件</a> 之 <code class="docutils literal notranslate"><span class="pre">CPU</span> <span class="pre">tensor</span></code> 欄位（若要在 GPU 上建構張量，則是參考 <code class="docutils literal notranslate"><span class="pre">GPU</span> <span class="pre">tensor</span></code> 之欄位）。</p>
</div>
</div>
<div class="section" id="id6">
<h2><span class="section-number">3.2. </span>張量之操弄<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3><span class="section-number">3.2.1. </span>張量之切片<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。<code class="docutils literal notranslate"><span class="pre">torch</span></code> 張量的切片方式，與 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 類似，皆使用中括號 <code class="docutils literal notranslate"><span class="pre">[]</span></code>，再搭配所欲擷取資料行列的索引（index）獲得。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;extract 1st row: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;extract 1st and 2nd rows: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;extract 2nd column: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;extract 2nd and 3rd columns: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>extract 1st row: 
 [1 2 3 4]
extract 1st and 2nd rows: 
 [[1 2 3 4]
 [5 6 7 8]]
extract 2nd column: 
 [ 2  6 10]
extract 2nd and 3rd columns: 
 [[ 2  3]
 [ 6  7]
 [10 11]]
</pre></div>
</div>
</div>
</div>
<p>進行切片時，有幾項重點需要注意。</p>
<ul class="simple">
<li><p>各面向之索引從0開始。</p></li>
<li><p>負號表示從結尾數回來，如 <code class="docutils literal notranslate"><span class="pre">-1</span></code> 表示最後一個位置。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">:</span></code>表示該面向所有元素皆挑選。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">start:stop</span></code> 表示從 <code class="docutils literal notranslate"><span class="pre">start</span></code> 開始挑選到 <code class="docutils literal notranslate"><span class="pre">stop-1</span></code>。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code> 表示從 <code class="docutils literal notranslate"><span class="pre">start</span></code> 開始到 <code class="docutils literal notranslate"><span class="pre">stop-1</span></code>，間隔 <code class="docutils literal notranslate"><span class="pre">step</span></code> 挑選。</p></li>
</ul>
</div>
<div class="section" id="id8">
<h3><span class="section-number">3.2.2. </span>張量之串接<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>多個張量在維度可對應之前提下，可透過 <code class="docutils literal notranslate"><span class="pre">torch.cat</span></code> 串接</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;vertical concatenation </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;horizontal concatenation </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>vertical concatenation 
 [[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
horizontal concatenation 
 [[ 1  2  3  4  1  2  3  4]
 [ 5  6  7  8  5  6  7  8]
 [ 9 10 11 12  9 10 11 12]]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id9">
<h2><span class="section-number">3.3. </span>張量之運算<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>考慮以下 <code class="docutils literal notranslate"><span class="pre">a</span></code> 與 <code class="docutils literal notranslate"><span class="pre">b</span></code> 兩張量</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor a is 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
tensor b is 
 [[1. 2.]
 [1. 2.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p>我們將使用 <code class="docutils literal notranslate"><span class="pre">a</span></code> 與 <code class="docutils literal notranslate"><span class="pre">b</span></code> 來展示如何使用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行張量間的計算。</p>
<div class="section" id="id10">
<h3><span class="section-number">3.3.1. </span>張量元素對元素之運算<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>透過 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 的數學函數，可進行張量元素對元素的四則運算</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise add </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise subtract </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise multiply </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise divide </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>element-wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element-wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element-wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element-wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>前述採用的函數，皆可取代為其所對應之運算子計算</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise add </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise subtract </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise multiply </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;element-wise divide </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>element-wise add 
 tensor([[2., 4.],
        [4., 6.],
        [6., 8.]], dtype=torch.float64)
element-wise subtract 
 tensor([[0., 0.],
        [2., 2.],
        [4., 4.]], dtype=torch.float64)
element-wise multiply 
 tensor([[ 1.,  4.],
        [ 3.,  8.],
        [ 5., 12.]], dtype=torch.float64)
element-wise divide 
 tensor([[1., 1.],
        [3., 2.],
        [5., 3.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#math-operations">troch官方文件</a> 此模組中尋找對應的數學函數。</p>
</div>
<div class="section" id="id11">
<h3><span class="section-number">3.3.2. </span>張量線性代數之運算<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>除了簡單的四則運算外，當張量的 <code class="docutils literal notranslate"><span class="pre">dim</span></code> 為2時，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 提供了進行線性代數（linear algebra）相關的函數，如</p>
<ul class="simple">
<li><p>矩陣轉置（matrix transpose）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transpose of a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a_t</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transpose of a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">a</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>transpose of a is 
 [[1. 3. 5.]
 [2. 4. 6.]]
transpose of a is 
 [[1. 3. 5.]
 [2. 4. 6.]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>矩陣乘法（matrix multiplication）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">a_t</span> <span class="o">@</span> <span class="n">a</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c = a_t @ a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>c = a_t @ a is 
 [[35. 44.]
 [44. 56.]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>反矩陣（matrix inverse）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inverse of c is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c_inv</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c1"># c @ c_inv should be identity matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;check for inverse (left) </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="p">(</span><span class="n">c_inv</span> <span class="o">@</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;check for inverse (right) </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="p">(</span><span class="n">c</span> <span class="o">@</span> <span class="n">c_inv</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>inverse of c is 
 [[ 2.33333333 -1.83333333]
 [-1.83333333  1.45833333]]
check for inverse (left) 
 [[ 1.00000000e+00 -1.42108547e-14]
 [ 0.00000000e+00  1.00000000e+00]]
check for inverse (right) 
 [[1. 0.]
 [0. 1.]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Cholesky 拆解（Cholesky decomposition）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c_chol</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cholesky factor of c is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">c_chol</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;check for Cholesky decomposition </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="p">(</span><span class="n">c_chol</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c_chol</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Cholesky factor of c is 
 [[5.91607978 0.        ]
 [7.43735744 0.82807867]]
check for Cholesky decomposition 
 [[35. 44.]
 [44. 56.]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>特徵拆解（eigen-decomposition）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">symeig</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;eigenvalue of c is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">e</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;eigenvector of c is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;check for eigen-decomposition </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="p">(</span><span class="n">v</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">@</span>
       <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>eigenvalue of c is 
 [ 0.26450509 90.73549491]
eigenvector of c is 
 [[-0.78489445  0.61962948]
 [ 0.61962948  0.78489445]]
check for eigen-decomposition 
 [[35. 44.]
 [44. 56.]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>奇異值拆解（singular value decomposition）</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;singular value of a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">s</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;left singular vector of a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">u</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;right singular vector of a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;check for singular value decomposition </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="p">(</span><span class="n">u</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">@</span>
       <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>singular value of a is 
 [9.52551809 0.51430058]
left singular vector of a is 
 [[-0.2298477   0.88346102]
 [-0.52474482  0.24078249]
 [-0.81964194 -0.40189603]]
right singular vector of a is 
 [[-0.61962948 -0.78489445]
 [-0.78489445  0.61962948]]
check for singular value decomposition 
 [[1. 2.]
 [3. 4.]
 [5. 6.]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id12">
<h3><span class="section-number">3.3.3. </span>對張量之數值進行摘要<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 提供了一些化約（reduction）的函數，對張量內的數值進行摘要</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate mean </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate standard deviation </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate max </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate min </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>calculate mean 
 3.5
calculate standard deviation 
 1.8708286933869707
calculate max 
 6.0
calculate min 
 1.0
</pre></div>
</div>
</div>
</div>
<p>我們亦可對張量的各面向，進行前述的摘要。以平均數為例：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate mean for each column </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;calculate mean for each row </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>calculate mean for each column 
 [3. 4.]
calculate mean for each row 
 [1.5 3.5 5.5]
</pre></div>
</div>
</div>
</div>
<p>其它的化約函數，可以參考<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">官方文件</a>。</p>
</div>
</div>
<div class="section" id="id13">
<h2><span class="section-number">3.4. </span>張量運算之進階議題<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id14">
<h3><span class="section-number">3.4.1. </span>廣播<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>在先前討論到兩張量進行元素對元素的加減乘除時，有一個條件是兩張量的尺寸必須相等。然而，此條件並非必要的。考慮以下的範例：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a * b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>a * b is 
 tensor([[2., 4., 6.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>我們可以看到，雖然 <code class="docutils literal notranslate"><span class="pre">a</span></code> 和 <code class="docutils literal notranslate"><span class="pre">b</span></code> 的尺寸並不相同，但 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 仍然會根據某種規則，將 <code class="docutils literal notranslate"><span class="pre">b</span></code> 的數值分配給 <code class="docutils literal notranslate"><span class="pre">a</span></code> 進行元素對元素的運算，這樣的特性被稱作廣播（broadcasting）。廣播的優點在於，其一方面可以處理不同尺寸張量的運算，二方面則是提供了高效率的計算。</p>
<p>廣播的概念承襲自 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 套件，其詳細的運作機制可以參考 <a class="reference external" href="https://numpy.org/doc/stable/user/theory.broadcasting.html">Array Broadcasting in Numpy</a> 一文。當以下條件滿足時，則 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 可進行廣播：</p>
<ul class="simple">
<li><p>兩張量從尾端軸（trailing axes）往前算回來的尺寸相同，或是其中一張量之維度必須是 1。</p></li>
</ul>
<p>以下之範例為尺寸 <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">)</span></code> 兩張量之乘法，由於兩張量從尾端軸算回來的維度都是 <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">)</span></code>，故符合前述條件。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor a * b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor a is 
 tensor([[1., 2., 3.],
        [4., 5., 6.]], dtype=torch.float64)
tensor b is 
 tensor([0., 1., 2.], dtype=torch.float64)
tensor a * b is 
 tensor([[ 0.,  2.,  6.],
        [ 0.,  5., 12.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>以下之範例為尺寸 <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">1)</span></code> 兩張量之乘法，其從尾端軸算回來的維度為相等或是其中一張量等於1，亦符合廣播條件</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                 <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor a is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor a * b is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>tensor a is 
 tensor([[1., 2., 3.],
        [4., 5., 6.]], dtype=torch.float64)
tensor b is 
 tensor([[0.],
        [1.]], dtype=torch.float64)
tensor a * b is 
 tensor([[0., 0., 0.],
        [4., 5., 6.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>然而，若是尺寸為 <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> 與 <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">)</span></code> 之張量相乘的，則不符合廣播條件，會產生錯誤訊息。</p>
</div>
<div class="section" id="gpu">
<h3><span class="section-number">3.4.2. </span>GPU運算<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h3>
<p>當執行程式碼之機器支援GPU運算時，<code class="docutils literal notranslate"><span class="pre">torch</span></code> 可利用GPU獲得高效能之運算。而機器是否具有GPU 支援，可透過 <code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code> 此指令進行判定。若使用 Google 的 Colab 服務，進行 GPU 運算需要點選 <code class="docutils literal notranslate"><span class="pre">Runtime\Change</span> <span class="pre">runtime</span> <span class="pre">type</span></code>，在 <code class="docutils literal notranslate"><span class="pre">Hardware</span> <span class="pre">accelerator</span></code> 處點選 <code class="docutils literal notranslate"><span class="pre">GPU</span></code>，才會轉移到具有 GPU 之機器進行計算。</p>
<p>為了瞭解 GPU 在計算上的幫助，我們撰寫了一個函數 <code class="docutils literal notranslate"><span class="pre">math_speed_test</span></code>，其內部生成一 <code class="docutils literal notranslate"><span class="pre">(n,</span> <span class="pre">n)</span></code> 之方陣 <code class="docutils literal notranslate"><span class="pre">a</span></code>，接著對其連續進行 <code class="docutils literal notranslate"><span class="pre">iter_max</span></code> 次的連加，並記錄完成整個工作所需的時間。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="k">def</span> <span class="nf">math_speed_test</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">iter_max</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iter_max</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">+=</span> <span class="n">a</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="s1">&#39;time =&#39;</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 進行 GPU 運算的關鍵步驟在於，利用 <code class="docutils literal notranslate"><span class="pre">.to(&quot;cuda&quot;)</span></code> 方法將資料送到 GPU 上，而如果將指令改為 <code class="docutils literal notranslate"><span class="pre">.to(&quot;cpu&quot;)</span></code>，則意味著我們將使用一般的 CPU 進行運算。</p>
<p>接下來的程式碼，我們將 <code class="docutils literal notranslate"><span class="pre">iter_max</span></code> 設為 100，<code class="docutils literal notranslate"><span class="pre">n</span></code> 設為 <code class="docutils literal notranslate"><span class="pre">10</span></code>、<code class="docutils literal notranslate"><span class="pre">100</span></code>、以及<code class="docutils literal notranslate"><span class="pre">1000</span></code>，觀察計算時間的變化（注意，下述程式碼得在支援 GPU 之機器上執行，才可以看見 GPU 之計算時間，否則僅會呈現 CPU 的結果）：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">iter_max</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n =&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">math_speed_test</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="p">,</span> <span class="n">iter_max</span> <span class="o">=</span> <span class="n">iter_max</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">math_speed_test</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="p">,</span> <span class="n">iter_max</span> <span class="o">=</span> <span class="n">iter_max</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>n =
</pre></div>
</div>
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span> 10
CPU time = 0.0008530616760253906
n = 100
CPU time = 0.0010561943054199219
n = 1000
</pre></div>
</div>
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>CPU time = 0.04702186584472656
</pre></div>
</div>
</div>
</div>
<p>大體上，我們可以觀察到在 <code class="docutils literal notranslate"><span class="pre">n</span></code> 數值為 <code class="docutils literal notranslate"><span class="pre">10</span></code> 或 <code class="docutils literal notranslate"><span class="pre">100</span></code> 時，GPU並沒有顯著的幫助，甚至可能表現得更差。然而，在 <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">1000</span></code>，GPU 可大幅提升計算的速度。</p>
<p>除了數學運算外，隨機亂數之生成亦可透過 GPU 顯著的加速，可參考以下之程式碼：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>4.74 ms ± 71.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>在這裡，我們使用了 <code class="docutils literal notranslate"><span class="pre">IPython</span></code> 的魔術指令 <code class="docutils literal notranslate"><span class="pre">%timeit</span></code> ，其可用於評估該行程式碼之計算時間。如果要評估的是整個程式塊（code block）的時間的話，可以改為使用 <code class="docutils literal notranslate"><span class="pre">%%timeit</span></code> 置於程式塊的開頭。</p>
<p>另外，線性代數亦可獲得 GPU 之幫助</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>19.7 ms ± 3.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>然而，部分線性代數的計算則未必可以獲得 GPU 的強力幫助，如計算反矩陣</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>21.4 ms ± 7.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="o">%</span><span class="n">timeit</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>一個數學運算是否能夠獲得 GPU 幫助的關鍵在於，該運算是否能夠被平行化。如果讀者曾經有使用過高斯消去法（Gaussian elimination）來解反矩陣的話，就知道高斯消去法是個得序列求解的方法，故 GPU 幫助不大。</p>
<p>在實務上，我們可以透過以下的程式碼來檢驗機器是否有可供使用的GPU，以動態決定 <code class="docutils literal notranslate"><span class="pre">device</span></code> 應為何：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>cpu
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id15">
<h2><span class="section-number">3.5. </span>實作範例與練習<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id16">
<h3><span class="section-number">3.5.1. </span>產生線性迴歸資料<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>在開始之前，我們先設定一種子，以讓後續的亂數生成都能夠獲得相同的結果（不過，這裡的 <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> 僅適用於 CPU 之張量，若使用GPU，請改為 <code class="docutils literal notranslate"><span class="pre">torch.cuda.manual_seed</span></code>）。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">48</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7fd6b06c6fb0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a function to generate x and y</span>
<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span>
                  <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="n">std_residual</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="n">mean_feature</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="n">std_feature</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">n_feature</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean_feature</span><span class="p">,</span>
                     <span class="n">std</span> <span class="o">=</span> <span class="n">std_feature</span><span class="p">,</span>
                     <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">n_feature</span><span class="p">),</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                     <span class="n">std</span> <span class="o">=</span> <span class="n">std_residual</span><span class="p">,</span>
                     <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">+</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">e</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run generate_data</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_sample</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="n">bias</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                     <span class="n">std_residual</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="n">mean_feature</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="n">std_feature</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;feature matrix x is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;response vector y is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>feature matrix x is 
 [[10.86425232  2.85930239 12.99748957]
 [ 7.98709525  9.79137811 12.98202577]
 [ 8.40585824 13.31645195  4.51125723]
 [10.52843635 10.28388618  8.43102705]
 [11.25624976 12.04005459 12.59301479]
 [12.31145327  9.15153143  7.96220487]
 [11.49807312 14.63609048  6.82995236]
 [11.7234386   7.16098399  9.8980089 ]
 [ 7.24735628  7.5570991   5.9154331 ]
 [11.05035931 10.47430089  4.86376669]]
response vector y is 
 [[-42.98404441]
 [ -4.94309144]
 [  0.31840093]
 [-16.21407503]
 [-13.77391657]
 [-31.18360077]
 [ -8.4871242 ]
 [-31.53590804]
 [ -7.8563037 ]
 [-19.27007283]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id17">
<h3><span class="section-number">3.5.2. </span>計算模型參數<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a function to calculate model parameter</span>
<span class="k">def</span> <span class="nf">calculate_parameter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">x_design</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">u</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_design</span><span class="p">,</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">x_design</span><span class="p">)</span> <span class="o">@</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_design</span><span class="p">,</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">y</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">parameter</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">parameter</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">weight</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run calculate_parameter</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="n">bias</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                     <span class="n">std_residual</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="n">mean_feature</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="n">std_feature</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">bias</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">calculate_parameter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bias estimate is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weight estimate is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>bias estimate is 
 4.996612423766793
weight estimate is 
 [-4.98003037  2.98762589 -0.00737968]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id18">
<h3><span class="section-number">3.5.3. </span>建立迴歸分析物件<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a class to fit linear regression</span>
<span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x_design</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">u</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_design</span><span class="p">,</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">x_design</span><span class="p">)</span> <span class="o">@</span> \
                    <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_design</span><span class="p">,</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">parameter</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">parameter</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bias estimate is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model_lr</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weight estimate is </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model_lr</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>bias estimate is 
 4.996612423766793
weight estimate is 
 [-4.98003037  2.98762589 -0.00737968]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id19">
<h3><span class="section-number">3.5.4. </span>練習<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>請根據本章節的內容，進行一系列的實驗，以了解在哪些運算與張量尺寸上，GPU 計算才具有其優勢。</p></li>
<li><p>請在 <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> 此類型中，加入以下新的功能：</p></li>
</ol>
<ul class="simple">
<li><p>新增一個選項，以決定 <code class="docutils literal notranslate"><span class="pre">x</span></code> 是否需要進行標準化（每個變項的平均數為0，變異數為1，不准使用其它的套件）</p></li>
<li><p>新增一個選項，以決定是否使用 GPU 進行運算。</p></li>
<li><p>新增一方法 <code class="docutils literal notranslate"><span class="pre">predict()</span></code>，其輸入為一新的 <code class="docutils literal notranslate"><span class="pre">x</span></code>，輸出為在該 <code class="docutils literal notranslate"><span class="pre">x</span></code> 下，<code class="docutils literal notranslate"><span class="pre">y</span></code> 的預測值。</p></li>
<li><p>新增一方法 <code class="docutils literal notranslate"><span class="pre">gradient()</span></code>，其可獲得在當前係數估計下，最小平方估計準則的梯度數值。</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="linear-regression.html" title="previous page"><span class="section-number">2. </span>線性迴歸</a>
    <a class='right-next' id="next-link" href="logistic-regression.html" title="next page"><span class="section-number">4. </span>邏輯斯迴歸</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Po-Hsien Huang<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>