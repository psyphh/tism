{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "lab-probability.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "dw_zvvIwcvxG",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "Lab: 最大概似估計法\n",
    "================\n",
    "此 lab 中，我們將會透過 `tensorflow-probability`（TFP）此套件，學習以下的主題。\n",
    "\n",
    "1. 認識 TFP 的分配（distribution）物件。\n",
    "\n",
    "2. 利用變項之分配，搭配自動微分獲與優化器獲得最大概似估計值。\n",
    "\n",
    "\n",
    "TFP 之安裝與基礎教學，可參考 [TFP官方網頁](https://www.tensorflow.org/probability/install)。在安裝完成後，可透過以下的指令載入其與 `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7P6EpAcIcvxI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "16pj-OeWcvxM",
    "colab_type": "text"
   },
   "source": [
    "## 分配物件\n",
    "TFP 最為核心的物件為分配物件，其用於表徵一機率分配。TFP 涵蓋了許多不同的機率分配，其名單可至 [官方網頁](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions) 查看。\n",
    "\n",
    "在實務上，我們常將 TFP 的分配模組儲存為 `tfd`，以便於使用，即："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "soPSsBtfcvxN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tfd = tfp.distributions"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "hDHBLSrhcvxQ",
    "colab_type": "text"
   },
   "source": [
    "### 分配物件之基本操作\n",
    "\n",
    "以常態分配為例，我們可透過以下程式碼產生一表徵常態分配之物件，其平均數為0，標準差為1（尾端的小數點表示浮點數，而非整數）："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wB8BZgYwcvxQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "normal = tfd.Normal(loc=0., scale=1.)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bDJn1AtRcvxT",
    "colab_type": "text"
   },
   "source": [
    "透過此分配物件，我們可以產生對應之隨機樣本"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sCxas-XwcvxT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "outputId": "7998aef4-86a1-4742-e8eb-3f16a4c620ef"
   },
   "source": [
    "print(\"random sample with shape ():\\n\",\n",
    "      normal.sample())\n",
    "print(\"random sample with shape (3,):\\n\",\n",
    "      normal.sample(sample_shape=3))\n",
    "print(\"random sample with shape (2,3):\\n\",\n",
    "      normal.sample(sample_shape=(2, 3)))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with shape ():\n",
      " tf.Tensor(1.8022155, shape=(), dtype=float32)\n",
      "random sample with shape (3,):\n",
      " tf.Tensor([-0.40080532 -0.7313163   0.5339465 ], shape=(3,), dtype=float32)\n",
      "random sample with shape (2,3):\n",
      " tf.Tensor(\n",
      "[[ 2.4187677   0.5399068   0.29920924]\n",
      " [-1.2940803  -0.23391153  0.5884562 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uvFzPPERcvxX",
    "colab_type": "text"
   },
   "source": [
    "我們亦可給定實現值來評估其在該分配下之累積機率值："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LfdFM8i7cvxX",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "outputId": "e9f5e48c-db6b-4ee1-b06f-97f35319be30"
   },
   "source": [
    "print(\"cumulative probability given value with shape ():\\n\",\n",
    "      normal.cdf(value=0), \"\\n\")\n",
    "print(\"cumulative probability given value with (3,):\\n\",\n",
    "      normal.cdf(value=[-1, 0, .5]), \"\\n\")\n",
    "print(\"cumulative probability given value with (2,3):\\n\",\n",
    "      normal.cdf(value=[[-1, 0, .5], [-2, 1, 3]]))\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative probability given value with shape ():\n",
      " tf.Tensor(0.5, shape=(), dtype=float32) \n",
      "\n",
      "cumulative probability given value with (3,):\n",
      " tf.Tensor([0.15865526 0.5        0.69146246], shape=(3,), dtype=float32) \n",
      "\n",
      "cumulative probability given value with (2,3):\n",
      " tf.Tensor(\n",
      "[[0.15865526 0.5        0.69146246]\n",
      " [0.02275013 0.8413447  0.9986501 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "A7lMewP6cvxa",
    "colab_type": "text"
   },
   "source": [
    "或是對數機率值："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eUx3vyN7cvxb",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "outputId": "a988b1e4-fe00-4cd6-d7ec-efcf8d41a692"
   },
   "source": [
    "print(\"log-probability given value with shape ():\\n\",\n",
    "      normal.log_prob(value=0), \"\\n\")\n",
    "print(\"log-probability given value with (3,):\\n\",\n",
    "      normal.log_prob(value=[-1, 0, .5]), \"\\n\")\n",
    "print(\"log-probability given value with (2,3):\\n\",\n",
    "      normal.log_prob(value=[[-1, 0, .5], [-2, 1, 3]]))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-probability given value with shape ():\n",
      " tf.Tensor(-0.9189385, shape=(), dtype=float32) \n",
      "\n",
      "log-probability given value with (3,):\n",
      " tf.Tensor([-1.4189385 -0.9189385 -1.0439385], shape=(3,), dtype=float32) \n",
      "\n",
      "log-probability given value with (2,3):\n",
      " tf.Tensor(\n",
      "[[-1.4189385 -0.9189385 -1.0439385]\n",
      " [-2.9189386 -1.4189385 -5.4189386]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WgQ5DRsMcvxe",
    "colab_type": "text"
   },
   "source": [
    "### 分配物件之形狀\n",
    "分配物件的形狀比起張量的形狀較為複雜些，其產生的樣本共牽涉到三種形狀：\n",
    "\n",
    "1. 樣本形狀（sample shape），為在使用分配物件產生樣本時之形狀（即為前一小節使用 `.sample()` 時所設定的 `sample_shape`），其產生的資料彼此間為獨立且相同分配的（independent and identically distributed）。\n",
    "\n",
    "2. 批次形狀（batch shape），為建立分配物件時所設定的形狀（其透過參數的形狀決定），其可用於產生批次的樣本，批次樣本間彼此獨立，但其邊際分配之參數可以不同。\n",
    "\n",
    "3. 事件形狀（event shape），即多變量分配之變數形狀，如 $P$ 維多元常態分配的形狀即為 `(P,)`，在同一事件下產生的資料其變數間可為相依，且各邊際分配之參數也未必相同。\n",
    "\n",
    "而分配物件的形狀則牽涉到批次與事件兩種，可透過直接列印分配物件查看"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lbz_r5_6cvxe",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "dcb4e7d3-19dd-440f-8a6a-a406339a1eb0"
   },
   "source": [
    "print(normal)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Normal(\"Normal\", batch_shape=[], event_shape=[], dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7k5_5Whkcvxh",
    "colab_type": "text"
   },
   "source": [
    "或是使用 `.batch_shape` 與 `.event_shape` 獲得："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zi9Xi80Bcvxh",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "2431a2fb-1086-4150-fc71-b82e6f764327"
   },
   "source": [
    "print(normal.batch_shape)\n",
    "print(normal.event_shape)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "0H3ED4Iscvxk",
    "colab_type": "text"
   },
   "source": [
    "由於之前所創立的 `normal` 其用於產生純量之常態分配隨機變數，故 `.batch_shape` 與 `.event_shape` 兩者皆為 `()`。\n",
    "\n",
    "批次形狀之設定，乃經由對分配參數形狀之推論獲得，如"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zs7R4c4vcvxk",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "302adb10-46b7-4a89-c409-c0f4d4adc195"
   },
   "source": [
    "normal_batch = tfd.Normal(loc=[0., 1.], scale=[1., .5])\n",
    "print(normal_batch)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Normal(\"Normal\", batch_shape=[2], event_shape=[], dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "w2Rf56rdcvxn",
    "colab_type": "text"
   },
   "source": [
    "前述分配的 `batch_shape` 為 `(2,)`，其可用於產生一組兩個來自常態分配之變數，其中一個平均數為0，變異數為1，另一個平均數為1，變異數為.5，如"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DCDP4E6jcvxn",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "outputId": "62a52567-6e86-4314-ebd0-0fe5013efffb"
   },
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      normal_batch.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      normal_batch.sample(sample_shape=3), \"\\n\")\n",
    "print(\"random sample with sample_shape (2,3):\\n\",\n",
    "      normal_batch.sample(sample_shape=(2,3)))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tf.Tensor([-0.12045587  1.5716608 ], shape=(2,), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tf.Tensor(\n",
      "[[ 1.0107127   0.89146   ]\n",
      " [ 0.9506541   0.5228324 ]\n",
      " [-1.1188143   0.87866706]], shape=(3, 2), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (2,3):\n",
      " tf.Tensor(\n",
      "[[[ 0.15255086  1.3803482 ]\n",
      "  [-2.2631257   1.1712642 ]\n",
      "  [-2.8740003   0.12843525]]\n",
      "\n",
      " [[ 0.34314007  0.7685755 ]\n",
      "  [-0.64506155  0.9141298 ]\n",
      "  [-0.928601    1.6389561 ]]], shape=(2, 3, 2), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UAY45NsFcvxq",
    "colab_type": "text"
   },
   "source": [
    "我們亦可使用所創立的 `normal_batch` 來度量輸入數值的對數機率值："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Bu1mmQqqcvxr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "outputId": "dc1a0ef5-17b2-42e0-a8fb-1882003d80f0"
   },
   "source": [
    "print(\"log-probability given value with shape ():\\n\",\n",
    "      normal_batch.log_prob(0), \"\\n\")\n",
    "print(\"log-probability given value with shape (2,):\\n\",\n",
    "      normal_batch.log_prob([0, 0]), \"\\n\")\n",
    "print(\"log-probability given value with shape (2,1):\\n\",\n",
    "      normal_batch.log_prob([[0], [0]]))"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-probability given value with shape ():\n",
      " tf.Tensor([-0.9189385 -2.2257915], shape=(2,), dtype=float32) \n",
      "\n",
      "log-probability given value with shape (2,):\n",
      " tf.Tensor([-0.9189385 -2.2257915], shape=(2,), dtype=float32) \n",
      "\n",
      "log-probability given value with shape (2,1):\n",
      " tf.Tensor(\n",
      "[[-0.9189385 -2.2257915]\n",
      " [-0.9189385 -2.2257915]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bcdAX1H-cvxt",
    "colab_type": "text"
   },
   "source": [
    "這邊我們可以觀察到，前兩者種寫法獲得一樣的數值，皆表示 `[0, 0]` 此向量於 `normal_batch` 下的對數機率。對第一種寫法來說，其輸入為純量，因此，使用到了廣播（broadcasting）的概念，將0的數值轉為 `[0,0]` 後再進行評估，第二種寫法則是較為標準，其直接輸入了 `[0,0]`，與 `normal_batch` 的 `batch_size` 相同。而第三種寫法，可以想成輸入了一 `sample_shape` 為2的資料，而每筆觀測值的0皆會透過廣播拓展為 `[0, 0]`，故回傳了每筆觀測值於 `normal_batch` 之對數機率。\n",
    "\n",
    "\n",
    "事件形狀僅適用於多變量之分配，以多元常態（multivariate normal）分配為例，其需給定一平均數向量與共變異數矩陣（在這邊，我們採用的 `tfd.MultivariateNormalTriL` 需給定的是共變異數矩陣的 Cholesky 分解）："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QS0enb2Acvxu",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "3bd32b6c-4dff-4cb7-d006-590f8b8c1c74"
   },
   "source": [
    "mvn = tfd.MultivariateNormalTriL(\n",
    "    loc=[0, 1],\n",
    "    scale_tril=tf.linalg.cholesky([[1., 0.], [0., .5]]))\n",
    "print(mvn)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.MultivariateNormalTriL(\"MultivariateNormalTriL\", batch_shape=[], event_shape=[2], dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "9miRrz-jcvxx",
    "colab_type": "text"
   },
   "source": [
    "我們可看到此分配物件的 `event_shape` 是 `(2)`，與此多元常態分配的維度相同，其可用於產生服從多元常態分配之資料"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eq9IVXCTcvxx",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "outputId": "06354dc4-57ec-4b8d-90c4-ec5c0c41e00e"
   },
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      mvn.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      mvn.sample(sample_shape=3), \"\\n\")\n",
    "print(\"random sample with sample_shape (2, 3):\\n\",\n",
    "      mvn.sample(sample_shape=(2, 3)))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tf.Tensor([1.4775816  0.46959066], shape=(2,), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tf.Tensor(\n",
      "[[ 0.13093975  1.9413812 ]\n",
      " [-0.3281478   0.3186245 ]\n",
      " [ 0.637075    0.13808978]], shape=(3, 2), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (2, 3):\n",
      " tf.Tensor(\n",
      "[[[ 0.35966963  2.3257236 ]\n",
      "  [-0.39306912  1.5216911 ]\n",
      "  [ 0.45905554  0.5633739 ]]\n",
      "\n",
      " [[-2.2071261   1.432902  ]\n",
      "  [-1.2735064   0.2039342 ]\n",
      "  [-0.9986951   0.3985774 ]]], shape=(2, 3, 2), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "AWf4QF3tcvx0",
    "colab_type": "text"
   },
   "source": [
    "同樣的，該物件亦可用於評估給定數值下的對數機率："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KlKkABhXcvx1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "outputId": "c48d24e6-e621-4e06-802e-cb1a26486d25"
   },
   "source": [
    "print(\"log-probability given value with shape (2,):\\n\",\n",
    "      mvn.log_prob([0, 0]), \"\\n\")\n",
    "print(\"log-probability given value with shape (2,1):\\n\",\n",
    "      mvn.log_prob([[0, 0], [0, 0]]))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-probability given value with shape (2,):\n",
      " tf.Tensor(-2.4913032, shape=(), dtype=float32) \n",
      "\n",
      "log-probability given value with shape (2,1):\n",
      " tf.Tensor([-2.4913032 -2.4913032], shape=(2,), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WZkJAJXNcvx3",
    "colab_type": "text"
   },
   "source": [
    "儘管此 `mvn` 表徵的二維之多元常態分配，其平均數與共變異數矩陣之設定，與先前 `normal_batch`是等價的，但在使用 `mvn` 評估機率時，需注意：（1）先前針對 `batch_shape` 此面向的廣播，不再適用於 `event_shape`，故 `mvn.log_prob(0)` 會產生錯誤訊息；（2）針對每筆觀測值，其計算的是在此多元常態分配下的聯合機率，因此，只會獲得一個對數機率值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yrFwsZ3dcvx4",
    "colab_type": "text"
   },
   "source": [
    "分配物件的 `batch_shape` 可透過 `tfd.Independent` 此物件轉為 `event_shape`，如"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7opzhoUBcvx4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "58d8ff42-9ac9-43ed-ba9c-7bd1cd207210"
   },
   "source": [
    "tfd.Independent(normal_batch, reinterpreted_batch_ndims=1)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<tfp.distributions.Independent 'IndependentNormal' batch_shape=[] event_shape=[2] dtype=float32>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "lpeaHvHscvx-",
    "colab_type": "text"
   },
   "source": [
    "在多變量的分配之下，前述介紹的批次形狀與事件形狀，可以合併使用："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MuJW9ohAcvx-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "8bc7ebd1-ebd1-40e7-a3a0-7cc9be4cffb2"
   },
   "source": [
    "mvn_batch = tfd.MultivariateNormalTriL(\n",
    "    loc=[[0, 1],\n",
    "         [1, 2],\n",
    "         [2, 3]],\n",
    "    scale_tril=tf.linalg.cholesky([[1., 0.], [0., .5]]))\n",
    "mvn_batch"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<tfp.distributions.MultivariateNormalTriL 'MultivariateNormalTriL' batch_shape=[3] event_shape=[2] dtype=float32>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "IvwYBO60cvyA",
    "colab_type": "text"
   },
   "source": [
    "這裡，其 `batch_shape` 為 `(3)`，值得注意的是，這邊我們僅設定了一個共變異數矩陣，因此，其會透過廣播的機制，與三個平均數向量做對應。同樣的，我們可以用此 `mvn_batch` 來產生樣本資料\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "N8qw6R_pcvyB",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "outputId": "7064662a-5f23-4bdc-acaf-3f8e3bc8f481"
   },
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      mvn_batch.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      mvn_batch.sample(sample_shape=3), \"\\n\")\n",
    "print(\"random sample with sample_shape (2, 3):\\n\",\n",
    "      mvn_batch.sample(sample_shape=(2, 3)))"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tf.Tensor(\n",
      "[[-0.52711165  1.2977471 ]\n",
      " [ 0.71927595  2.362156  ]\n",
      " [ 1.6059546   3.198034  ]], shape=(3, 2), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tf.Tensor(\n",
      "[[[-0.97376215  1.3276844 ]\n",
      "  [ 0.09181923  2.0983415 ]\n",
      "  [ 4.26184     3.9088588 ]]\n",
      "\n",
      " [[ 1.0735731   1.734164  ]\n",
      "  [ 2.305106    2.6296315 ]\n",
      "  [ 2.1507843   4.2726173 ]]\n",
      "\n",
      " [[-0.8849281   1.8740932 ]\n",
      "  [-0.09303105  2.105943  ]\n",
      "  [ 2.8704734   4.3114862 ]]], shape=(3, 3, 2), dtype=float32) \n",
      "\n",
      "random sample with sample_shape (2, 3):\n",
      " tf.Tensor(\n",
      "[[[[ 0.41628233  0.9831396 ]\n",
      "   [ 1.9586751   1.1809969 ]\n",
      "   [ 2.1355686   3.8793414 ]]\n",
      "\n",
      "  [[-0.0234903   0.4839651 ]\n",
      "   [ 2.3488755   1.340109  ]\n",
      "   [ 1.8641006   2.3110018 ]]\n",
      "\n",
      "  [[ 0.21153425  0.23206025]\n",
      "   [ 0.45886844  1.9729831 ]\n",
      "   [ 3.0200868   3.752419  ]]]\n",
      "\n",
      "\n",
      " [[[-0.08479002  1.8746097 ]\n",
      "   [ 0.79663867  2.250805  ]\n",
      "   [ 0.13800907  3.6238537 ]]\n",
      "\n",
      "  [[ 0.22511227  1.7854975 ]\n",
      "   [ 2.3729033   1.0953634 ]\n",
      "   [ 2.412556    3.1825552 ]]\n",
      "\n",
      "  [[ 0.3246875   0.5076373 ]\n",
      "   [ 2.0186892   1.3338784 ]\n",
      "   [ 1.2854546   3.107246  ]]]], shape=(2, 3, 3, 2), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "J7Ax86-xcvyD",
    "colab_type": "text"
   },
   "source": [
    "## 最大概似估計法\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "f8eZ-35hcvyE",
    "colab_type": "text"
   },
   "source": [
    "### 可學習的分配與求解\n",
    "使用 `tensorflow` 來進行最大概似法，有許多種做法，其中，最重要的關鍵就在於如何建構概似函數。事實上，在前一小節中，我們已經可以計算在給定參數下，某個隨機樣本實現值之可能性，因此，關鍵就在於如何讓前述之可能性，轉為參數數值之函數，而最簡單的做法，就是將參數設為 `tf.Variable`，搭配自動微分與優化器對其進行更新。\n",
    "\n",
    "以下的程式碼建立了一 `batch_size` 為2的常態分配模型，我們可透過 `.trainable_varialbes` 來查看哪些變數是可以透過訓練更新其數值的。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MP-7H725cvyE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "outputId": "aa7ae444-4a30-4c1a-80b9-4934458b50ee"
   },
   "source": [
    "batch_size = 2\n",
    "normal_model = tfd.Normal(\n",
    "        loc=tf.Variable(tf.zeros(batch_size), name='loc'),\n",
    "        scale=tf.Variable(tf.ones(batch_size), name='scale'))\n",
    "print(\"normal model:\\n\", normal_model, \"\\n\")\n",
    "print(\"parameters in normal model:\\n\", normal_model.trainable_variables)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal model:\n",
      " tfp.distributions.Normal(\"Normal\", batch_shape=[2], event_shape=[], dtype=float32) \n",
      "\n",
      "parameters in normal model:\n",
      " (<tf.Variable 'loc:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Variable 'scale:0' shape=(2,) dtype=float32, numpy=array([1., 1.], dtype=float32)>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "oceFlpMpdHSC",
    "colab_type": "text"
   },
   "source": [
    "TFP 的官方教學中，將前述的分配稱作可學習的分配（learnable distribution）。接著，我們在目前給定的參數數值下，產生一隨機樣本，此樣本在目前參數數值下的可能性，可簡單地使用 `.log_prob()` 方法與加總平均的計算獲得："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "i6ZFZp1qcvyH",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "89e7aa0a-6063-4749-a39f-416c91befa22"
   },
   "source": [
    "sample_size = 1000\n",
    "x = normal_model.sample(sample_shape=sample_size)\n",
    "loss_value = -tf.reduce_mean(tf.reduce_sum(normal_model.log_prob(x), axis = 1))\n",
    "print(\"negative likelihood value is \", loss_value.numpy())"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative likelihood value is  2.8675694\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fErvUwgcdHSF",
    "colab_type": "text"
   },
   "source": [
    "最後，我們就可以透過優化器來求最大概似解了。在這邊需特別注意的是，由於 `loc` 與 `scale` 原本的數值為真實參數的數值，為了要展示優化器的正確運作，我們將其起始值設為一個較差的數值。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dVVB6J9VdHSG",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "3e1bb5cb-7465-4584-878a-d073a6902c23"
   },
   "source": [
    "epochs = 400\n",
    "tol = 10**(-5)\n",
    "learning_rate = 1.0\n",
    "normal_model.loc.assign([1., 1.])\n",
    "normal_model.scale.assign([.5, .5])\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "for epoch in tf.range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = -tf.reduce_mean(tf.reduce_sum(normal_model.log_prob(x), axis = 1))\n",
    "    gradients = tape.gradient(loss_value,\n",
    "                              normal_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,\n",
    "                                  normal_model.trainable_variables))\n",
    "    if (tf.reduce_max(\n",
    "            [tf.reduce_mean(\n",
    "                tf.math.abs(x)) for x in gradients]).numpy()) < tol:\n",
    "        print(\"{n} Optimizer Converges After {i} Iterations\".format(\n",
    "            n=optimizer.__class__.__name__, i=epoch))\n",
    "        break"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer Converges After 252 Iterations\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GfY2994EdHSI",
    "colab_type": "text"
   },
   "source": [
    "接著，我們比較優化器求得的解，以及分析解之間的差異（常態分配平均數與標準差的分析解即為樣本平均數與除上 $N$ 的標準差）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9uAr62N9dHSJ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "70d56207-f12d-4dbe-8cb0-e74579d26f92"
   },
   "source": [
    "print(\"ML mean estimate: \\n\", \n",
    "      normal_model.loc.numpy())\n",
    "print(\"ML standard deviation estimate: \\n\", \n",
    "      normal_model.scale.numpy())"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML mean estimate: \n",
      " [ 0.02060857 -0.02839172]\n",
      "ML standard deviation estimate: \n",
      " [1.0028995 1.0258194]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ja4QA3_GezuW",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "b40dc325-85e8-453d-f463-cd589f92eed1"
   },
   "source": [
    "print(\"sample mean: \\n\", \n",
    "      tf.reduce_mean(x, axis = 0).numpy())\n",
    "print(\"sample standard deviation: \\n\", \n",
    "      tfp.stats.stddev(x, sample_axis = 0).numpy())"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mean: \n",
      " [ 0.02060501 -0.02839323]\n",
      "sample standard deviation: \n",
      " [1.0029054 1.0258336]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PWe4hHu-cvyK",
    "colab_type": "text"
   },
   "source": [
    "我們可看到兩組解的數值幾乎相同，顯示優化器在這邊有確實地找到最大概似解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rae_6zMedHSR",
    "colab_type": "text"
   },
   "source": [
    "### 利用對射進行變數轉換\n",
    "前述的最大概似估計過程，並未對於參數估計之數值進行限制，其考慮的是非限制的優化問題（unconstrained optimization problem），然而，在實際進行優化時，若未對於參數數值進行限制的話，可能會獲得不合理之估計值（如負的變異數等）。\n",
    "\n",
    "在 TFP 的架構中，主要是透過對於參數進行對射（bijection），將原始受限制的參數轉為非限制的參數後進行估計。TFP 所內建的對射函數，可參考其 [官方網頁](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors)。\n",
    "\n",
    "在下面的範例中，我們利用 `tfp.util.TransformedVariable` 與 `tfb.Exp()` ，將常態分配的變異數 $\\sigma$ 參數化為 $\\exp(\\gamma)$，將原本需進行估計 $\\mu$ 與 $\\sigma$ 的優化問題，轉為估計 $\\mu$ 與 $\\gamma$，此時，我們不需要對 $\\gamma$ 的數值範圍進行限制，其在透過 $\\exp$ 函數的轉換後，會自動符合模型隱含的限制式。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5xEX0NuadHSR",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "outputId": "8e22e1bb-0d12-4135-855a-ee2a561c49fd"
   },
   "source": [
    "tfb = tfp.bijectors\n",
    "batch_size = 2\n",
    "normal_model_tr = tfd.Normal(\n",
    "    loc=tf.Variable(tf.zeros(batch_size), name='loc'),\n",
    "    scale=tfp.util.TransformedVariable(\n",
    "        tf.ones(batch_size),\n",
    "        bijector=tfb.Exp(), name=\"scale\"))\n",
    "print(\"normal model:\\n\", normal_model_tr, \"\\n\")\n",
    "print(\"parameters in normal model:\\n\", normal_model_tr.trainable_variables)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal model:\n",
      " tfp.distributions.Normal(\"Normal\", batch_shape=[2], event_shape=[], dtype=float32) \n",
      "\n",
      "parameters in normal model:\n",
      " (<tf.Variable 'loc:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Variable 'scale:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "oVvtaThrdHST",
    "colab_type": "text"
   },
   "source": [
    "我們可以直接使用前述的優化程式碼來獲得重新參數化後的參數估計"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZvEhEahkdHSU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "outputId": "3716b593-4a66-4dfe-b62d-de1a21b3b062"
   },
   "source": [
    "epochs = 400\n",
    "tol = 10**(-5)\n",
    "learning_rate = 1.0\n",
    "normal_model_tr.loc.assign([1., 1.])\n",
    "normal_model_tr.scale.assign([.5, .5])\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "for epoch in tf.range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = -tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                normal_model_tr.log_prob(x), axis = 1))\n",
    "    gradients = tape.gradient(loss_value,\n",
    "                              normal_model_tr.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,\n",
    "                                  normal_model_tr.trainable_variables))\n",
    "    if (tf.reduce_max(\n",
    "            [tf.reduce_mean(\n",
    "                tf.math.abs(x)) for x in gradients]).numpy()) < tol:\n",
    "        print(\"{n} Optimizer Converges After {i} Iterations\".format(\n",
    "            n=optimizer.__class__.__name__, i=epoch))\n",
    "        break\n",
    "\n",
    "print(\"ML mean estimate: \\n\",\n",
    "      normal_model_tr.loc.numpy())\n",
    "print(\"ML standard deviation estimate: \\n\",\n",
    "      normal_model_tr.scale.numpy())"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer Converges After 240 Iterations\n",
      "ML mean estimate: \n",
      " [ 0.02059323 -0.02839942]\n",
      "ML standard deviation estimate: \n",
      " [1.0029178 1.0258318]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "CTZysd6scvyK",
    "colab_type": "text"
   },
   "source": [
    "### 多元常態分配之參數估計"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OWGvSlwUdHSY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "outputId": "eb09d2ca-772c-4997-afbe-47718088e881"
   },
   "source": [
    "loc_true = tf.constant([0., 1., -1.])\n",
    "scale_tril_true = tf.linalg.cholesky(\n",
    "    tf.constant([[1, .3, .6], [.3, .5, .1], [.6, .1, 1.5]]))\n",
    "mvn_model_true = tfd.MultivariateNormalTriL(\n",
    "    loc = loc_true,\n",
    "    scale_tril = scale_tril_true)\n",
    "print(mvn_model_true)"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.MultivariateNormalTriL(\"MultivariateNormalTriL\", batch_shape=[], event_shape=[3], dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3ipFEemRdHSa",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "outputId": "cd06e69d-ee79-4eb6-ac48-0f9932127d5c"
   },
   "source": [
    "#a bug here\n",
    "x=mvn_model_true.sample([10000,1]).numpy()\n",
    "\n",
    "optimizer=tf.optimizers.Adam(learning_rate=.5)\n",
    "epochs=500\n",
    "tol=10**(-3)\n",
    "\n",
    "loc = tf.Variable(tf.constant([0., 0., 0.]), name='loc')\n",
    "scale_tril = tf.Variable(tf.linalg.cholesky(\n",
    "    tf.constant([[1, .0, .0], [.0, 1, .0], [.0, .0, 1]])),\n",
    "    name = \"scale_tril\")\n",
    "mvn_model = tfd.MultivariateNormalTriL(\n",
    "    loc=loc, scale_tril=scale_tril)\n",
    "for epoch in tf.range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = -tf.reduce_mean(\n",
    "            tf.reduce_sum(mvn_model.log_prob(x), axis = 1))\n",
    "    gradients = tape.gradient(loss_value,\n",
    "                              mvn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,\n",
    "                                  mvn_model.trainable_variables))\n",
    "    if (tf.reduce_max(\n",
    "            [tf.reduce_max(\n",
    "                tf.math.abs(x)) for x in gradients]).numpy()) < tol:\n",
    "        print(\"{n} Optimizer Converges After {i} Iterations\".format(\n",
    "            n=optimizer.__class__.__name__, i=epoch))\n",
    "        break"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer Converges After 139 Iterations\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.00456796  0.9913664  -0.9916235 ], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.0066237  0.29171604 0.61250305]\n",
      " [0.29171604 0.49913228 0.08988991]\n",
      " [0.61250305 0.08988991 1.5196214 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(mvn_model.mean())\n",
    "print(mvn_model.covariance())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}