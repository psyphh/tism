{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lab: 張量與線性代數\n",
    "================\n",
    "\n",
    "此 lab 中，我們將會透過 `torch` 此套件，學習以下的主題。\n",
    "\n",
    "1. 認識 `torch` 的張量（tensor）之基礎。\n",
    "\n",
    "2. 了解如何對 `torch` 張量進行操弄。\n",
    "\n",
    "3. 使用 `torch` 進行線性代數之運算。\n",
    "\n",
    "4. 應用前述之知識，建立一可進行線性迴歸分析之類型（class）。\n",
    "\n",
    "`torch`之安裝與基礎教學，可參考 [PyTorch官方網頁](https://pytorch.org/get-started/locally)。在安裝完成後，可透過以下的指令載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 張量之基礎\n",
    "\n",
    "### 張量之輸入\n",
    "`torch` 最基本的物件是張量（tensor），其與 `numpy` 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 `python` 的 `list` 或是 `numpy` 的 `ndarray`），置於`torch.tensor`函數中\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(data = [[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12]])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過 `type()`，可看見其屬於 `torch.Tensor` 此一類型（class），若欲了解 `a` 的樣貌，我們可使用 `print` 指令來列印其主要的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過對 `a` 列印的結果，我們可觀察到：\n",
    "\n",
    "+ `a` 內部的資料數值（value）為 `[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]`。\n",
    "\n",
    "除此之外，`a` 還有兩個重要的屬性並未顯示在列印的結果中：\n",
    "\n",
    "+ `a` 的尺寸（size）為 `(3, 4)`，表示 `a` 為一 $3 \\times 4$ 之張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則。\n",
    "+ `a` 的資料類型（data type）為 `int64`，表示64位元的整數。在進行運算時，張量間的類型須相同。\n",
    "\n",
    "稍後，我們會討論如何獲得 `torch` 張量的尺寸與資料類型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 張量之數值\n",
    "若要獲得張量的資料數值（value），可透過 `.numpy()`獲得，其回傳該張量對應之 `numpy` 陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data of tensor is: \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"data of tensor is: \\n\", a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `numpy` 陣列是 `python` 進行科學運算時，幾乎都會仰賴的資料格式。`torch` 內建了多種函數，以協助產生具有特別數值結構之張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with all elements being ones \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "tensor with all elements being zeros \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "identity-like tensor \n",
      " [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "diagonal matrix \n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with all elements being ones \\n\",\n",
    "      torch.ones(size = (4, 1)).numpy())\n",
    "print(\"tensor with all elements being zeros \\n\",\n",
    "      torch.zeros(size = (2, 3)).numpy())\n",
    "print(\"identity-like tensor \\n\",\n",
    "      torch.eye(n = 3, m = 5).numpy())\n",
    "print(\"diagonal matrix \\n\",\n",
    "      torch.diag(input = torch.tensor([1, 2, 3, 4])).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch` 亦可指定分配產生隨機的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with random elements from uniform(0, 1) \n",
      " [[0.28970462 0.9929803  0.0085113  0.57995695 0.75530434 0.07996726]\n",
      " [0.2685395  0.9141003  0.91774434 0.9897176  0.4272172  0.27501744]]\n",
      "tensor with uninitialized data \n",
      " [[ 2.7745710e-43  0.0000000e+00  1.6983938e+31  4.5773414e-41\n",
      "   0.0000000e+00 -3.6893488e+19]\n",
      " [ 0.0000000e+00 -3.6893488e+19  1.2611686e-44  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with random elements from uniform(0, 1) \\n\",\n",
    "      torch.rand(size = (2, 6)).numpy())\n",
    "print(\"tensor with uninitialized data \\n\",\n",
    "      torch.empty(size = (2, 6)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之形狀\n",
    "\n",
    "張量之形狀與形狀維度之數量，可透過張量物件的 `.size()`（或 `.shape`） 與 `dim` 方法來獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of tensor is torch.Size([3, 4])\n",
      "size of tensor is torch.Size([3, 4])\n",
      "dim of tensor is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"size of tensor is\", a.size())\n",
    "print(\"size of tensor is\", a.shape)\n",
    "print(\"dim of tensor is\", a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要對張量的形狀進行改變的話，可透過 `.view()` 此方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with shape (4, 3): \n",
      " [[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "tensor with shape (2, 2, 3): \n",
      " [[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]]\n",
      "tensor with shape (12, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12, 1) by (-1, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12,): \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with shape (4, 3): \\n\",\n",
    "      a.view(size = (4, 3)).numpy())\n",
    "print(\"tensor with shape (2, 2, 3): \\n\",\n",
    "      a.view(size = (2, 2, 3)).numpy())\n",
    "print(\"tensor with shape (12, 1): \\n\",\n",
    "      a.view(size = (12, 1)).numpy())\n",
    "print(\"tensor with shape (12, 1) by (-1, 1): \\n\",\n",
    "      a.view(size = (-1, 1)).numpy())\n",
    "print(\"tensor with shape (12,): \\n\",\n",
    "      a.view(size = (12, )).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意，`(12, 1)` 與 `(12,)` 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。另外，-1表示該面向對應之尺寸，由其它面向決定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之資料類型\n",
    "張量的資料類型，可透過 `.dtype` 方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of tensor is torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"data type of tensor is\", a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若是要調整資料類型的話，則可透過 `.type()` 此方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a.type(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`torch` 內建多種資料類型，包含整數類型（如 `torch.int32` 與 `torch.int64`）與浮點數類型（如 `torch.float32` 與 `torch.float64`），完整的資料類型請見 [torch.Tensor文件](https://pytorch.org/docs/stable/tensors.html)。\n",
    "\n",
    "在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 `torch` 常用之資料類型為 `torch.float32` 與 `torch.float64`，前者所需的記憶體較小，但運算結果的數值誤差較大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 張量之操弄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之切片\n",
    "\n",
    "若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。`torch` 張量的切片方式，與 `numpy` 類似，皆使用中括號 `[]`，再搭配所欲擷取資料行列的索引（index）獲得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract 1st row: \n",
      " [1 2 3 4]\n",
      "extract 1st and 2nd rows: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "extract 2nd column: \n",
      " [ 2  6 10]\n",
      "extract 2nd and 3rd columns: \n",
      " [[ 2  3]\n",
      " [ 6  7]\n",
      " [10 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"extract 1st row: \\n\",\n",
    "      a[0, :].numpy())\n",
    "print(\"extract 1st and 2nd rows: \\n\",\n",
    "      a[:2, :].numpy())\n",
    "print(\"extract 2nd column: \\n\",\n",
    "      a[:, 1].numpy())\n",
    "print(\"extract 2nd and 3rd columns: \\n\",\n",
    "      a[:, 1:3].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "進行切片時，有幾項重點需要注意。\n",
    "\n",
    "+ 各面向之索引從0開始。\n",
    "+ 負號表示從結尾數回來，如 `-1` 表示最後一個位置。\n",
    "+ `:`表示該面向所有元素皆挑選。\n",
    "+ `start:stop` 表示從 `start` 開始挑選到 `stop-1`。\n",
    "+ `start:stop:step` 表示從 `start` 開始到 `stop-1`，間隔 `step` 挑選。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之串接\n",
    "多個張量在維度可對應之前提下，可透過 `torch.cat` 串接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertical concatenation \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "horizontal concatenation \n",
      " [[ 1  2  3  4  1  2  3  4]\n",
      " [ 5  6  7  8  5  6  7  8]\n",
      " [ 9 10 11 12  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"vertical concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 0).numpy())\n",
    "print(\"horizontal concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量之運算\n",
    "考慮以下 `a` 與 `b` 兩張量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a is \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "tensor b is \n",
      " [[1. 2.]\n",
      " [1. 2.]\n",
      " [1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(data = [[1, 2], [3, 4], [5, 6]],\n",
    "                dtype = torch.float64)\n",
    "b = torch.tensor(data = [[1, 2], [1, 2], [1, 2]],\n",
    "                dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a.numpy())\n",
    "print(\"tensor b is \\n\", b.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們將使用 `a` 與 `b` 來展示如何使用 `torch` 進行張量間的計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量元素對元素之運算\n",
    "透過 `torch` 的數學函數，可進行張量元素對元素的四則運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"element-wise add \\n\",\n",
    "      torch.add(a, b))\n",
    "print(\"element-wise subtract \\n\",\n",
    "      torch.sub(a, b))\n",
    "print(\"element-wise multiply \\n\",\n",
    "      torch.mul(a, b))\n",
    "print(\"element-wise divide \\n\",\n",
    "      torch.div(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "前述採用的函數，皆可取代為其所對應之運算子計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"element-wise add \\n\", a + b)\n",
    "print(\"element-wise subtract \\n\", a - b)\n",
    "print(\"element-wise multiply \\n\", a * b)\n",
    "print(\"element-wise divide \\n\", a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 [troch官方文件](https://pytorch.org/docs/stable/torch.html#math-operations) 此模組中尋找對應的數學函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量線性代數之運算\n",
    "除了簡單的四則運算外，當張量的 `dim` 為2時，`torch` 提供了進行線性代數（linear algebra）相關的函數，如\n",
    "\n",
    "+ 矩陣轉置（matrix transpose）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transpose of a is \n",
      " [[1. 3. 5.]\n",
      " [2. 4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "a_t = torch.transpose(input=a, dim0=0, dim1=1)\n",
    "print(\"transpose of a is \\n\",\n",
    "      a_t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ 矩陣乘法（matrix multiplication）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = a_t @ a is \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "c = a_t @ a\n",
    "print(\"c = a_t @ a is \\n\",\n",
    "      c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 反矩陣（matrix inverse）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse of c is \n",
      " [[ 2.33333333 -1.83333333]\n",
      " [-1.83333333  1.45833333]]\n",
      "check for inverse (left) \n",
      " [[ 1.00000000e+00 -1.42108547e-14]\n",
      " [ 0.00000000e+00  1.00000000e+00]]\n",
      "check for inverse (right) \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "c_inv = torch.inverse(input = c)\n",
    "print(\"inverse of c is \\n\",\n",
    "      c_inv.numpy()) # c @ c_inv should be identity matrix\n",
    "print(\"check for inverse (left) \\n\",\n",
    "      (c_inv @ c).numpy())\n",
    "print(\"check for inverse (right) \\n\",\n",
    "      (c @ c_inv).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ Cholesky 拆解（Cholesky decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky factor of c is \n",
      " [[5.91607978 0.        ]\n",
      " [7.43735744 0.82807867]]\n",
      "check for Cholesky decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "c_chol = torch.cholesky(input = c)\n",
    "print(\"Cholesky factor of c is \\n\",\n",
    "      c_chol.numpy())\n",
    "print(\"check for Cholesky decomposition \\n\",\n",
    "      (c_chol @ torch.transpose(c_chol, 0 , 1)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 特徵拆解（eigen-decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue of c is \n",
      " [ 0.26450509 90.73549491]\n",
      "eigenvector of c is \n",
      " [[-0.78489445  0.61962948]\n",
      " [ 0.61962948  0.78489445]]\n",
      "check for eigen-decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "e, v = torch.symeig(input = c, eigenvectors=True)\n",
    "print(\"eigenvalue of c is \\n\",\n",
    "      e.numpy())\n",
    "print(\"eigenvector of c is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for eigen-decomposition \\n\",\n",
    "      (v @ torch.diag(e) @\n",
    "       torch.transpose(v, 0 , 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 奇異值拆解（singular value decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singular value of a is \n",
      " [9.52551809 0.51430058]\n",
      "left singular vector of a is \n",
      " [[-0.2298477   0.88346102]\n",
      " [-0.52474482  0.24078249]\n",
      " [-0.81964194 -0.40189603]]\n",
      "right singular vector of a is \n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "check for singular value decomposition \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "u, s, v = torch.svd(input = a)\n",
    "print(\"singular value of a is \\n\",\n",
    "      s.numpy())\n",
    "print(\"left singular vector of a is \\n\",\n",
    "      u.numpy())\n",
    "print(\"right singular vector of a is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for singular value decomposition \\n\",\n",
    "      (u @ torch.diag(s) @\n",
    "       torch.transpose(v, 0, 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 對張量之數值進行摘要\n",
    "`torch` 提供了一些化約（reduction）的函數，對張量內的數值進行摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean \n",
      " 3.5\n",
      "calculate standard deviation \n",
      " 1.8708286933869707\n",
      "calculate max \n",
      " 6.0\n",
      "calculate min \n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate mean \\n\",\n",
    "      torch.mean(input = a).numpy())\n",
    "print(\"calculate standard deviation \\n\",\n",
    "      torch.std(input = a).numpy())\n",
    "print(\"calculate max \\n\",\n",
    "      torch.max(input = a).numpy())\n",
    "print(\"calculate min \\n\",\n",
    "      torch.min(input = a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們亦可對張量的各面向，進行前述的摘要。以平均數為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean for each column \n",
      " [3. 4.]\n",
      "calculate mean for each row \n",
      " [1.5 3.5 5.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate mean for each column \\n\",\n",
    "      torch.mean(input = a, dim=0).numpy())\n",
    "print(\"calculate mean for each row \\n\",\n",
    "      torch.mean(input = a, dim=1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "其它的化約函數，可以參考[官方文件](https://pytorch.org/docs/stable/torch.html#reduction-ops)。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 實徵範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生線性迴歸資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# define a function to generate x and y\n",
    "def generate_data(n_sample, coef,\n",
    "                  intercept = 0,\n",
    "                  std_residual = 1,\n",
    "                  mean_feature = 0,\n",
    "                  std_feature = 1,\n",
    "                  dtype = torch.float64):\n",
    "    coef = torch.tensor(coef, dtype = dtype)\n",
    "    n_feature = coef.shape[0]\n",
    "    x = torch.normal(mean = mean_feature,\n",
    "                     std = std_feature,\n",
    "                     size = (n_sample, n_feature),\n",
    "                     dtype = dtype)\n",
    "    e = torch.normal(mean = 0,\n",
    "                     std = std_residual,\n",
    "                     size = (n_sample, 1),\n",
    "                     dtype = dtype)\n",
    "    coef = coef.view(size = (-1, 1))\n",
    "    y = intercept + x @ coef + e\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix x is \n",
      " [[12.23193227 16.1875451   8.47948037]\n",
      " [11.3377909  11.99897033 13.32103276]\n",
      " [ 9.39078596 12.26502356 12.56462157]\n",
      " [ 6.41933653  9.50310267  8.70375246]\n",
      " [11.99496626  8.57390059  5.15994537]\n",
      " [ 6.17447106 13.71870529 10.43966373]\n",
      " [11.07554904 10.61907646  5.95899556]\n",
      " [10.83895842  8.88731214 10.1729874 ]\n",
      " [ 9.37841615 10.4375999  12.1586268 ]\n",
      " [ 9.93521764  5.02822629  9.63586145]]\n",
      "response vector y is \n",
      " [[ -9.71187125]\n",
      " [-16.26963111]\n",
      " [ -4.88137753]\n",
      " [  0.81863219]\n",
      " [-28.64362011]\n",
      " [ 14.56850054]\n",
      " [-18.93265397]\n",
      " [-23.57492726]\n",
      " [ -9.93121492]\n",
      " [-28.85149536]]\n"
     ]
    }
   ],
   "source": [
    "# run generate_data\n",
    "x, y = generate_data(n_sample = 10,\n",
    "                     coef = [-5, 3, 0],\n",
    "                     intercept = 5,\n",
    "                     std_residual = 1,\n",
    "                     mean_feature = 10,\n",
    "                     std_feature = 3,\n",
    "                     dtype = torch.float64)\n",
    "print(\"feature matrix x is \\n\", x.numpy())\n",
    "print(\"response vector y is \\n\", y.numpy())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 計算模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# define a function to calculate model parameter\n",
    "def calculate_parameter(x, y, dtype = torch.float64):\n",
    "    if x.dtype is not dtype:\n",
    "        x = x.type(dtype = dtype)\n",
    "    if y.dtype is not dtype:\n",
    "        y = y.type(dtype = dtype)\n",
    "    u = torch.ones(size = (x.size()[0], 1), dtype = dtype)\n",
    "    x_design = torch.cat([u, x], dim = 1)\n",
    "    parameter = torch.inverse(\n",
    "        torch.transpose(x_design, dim0=0, dim1=1) @ x_design) @ \\\n",
    "                torch.transpose(x_design, dim0=0, dim1=1) @ y\n",
    "    intercept = parameter[0, 0]\n",
    "    coef = parameter[1:, 0]\n",
    "    return intercept, coef"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept estimate is \n",
      " 4.9529711517514095\n",
      "coefficient estimate is \n",
      " [-4.989112    2.99049829  0.00698989]\n"
     ]
    }
   ],
   "source": [
    "# run calculate_parameter\n",
    "x, y = generate_data(n_sample = 1000,\n",
    "                     coef = [-5, 3, 0],\n",
    "                     intercept = 5,\n",
    "                     std_residual = 1,\n",
    "                     mean_feature = 10,\n",
    "                     std_feature = 3,\n",
    "                     dtype = torch.float64)\n",
    "intercept, coef = calculate_parameter(x, y)\n",
    "print(\"intercept estimate is \\n\", intercept.numpy())\n",
    "print(\"coefficient estimate is \\n\", coef.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 建立一進行迴歸分析之物件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# define a class to fit linear regression\n",
    "class LinearRegression():\n",
    "    def __init__(self, dtype = torch.float64):\n",
    "        self.dtype = dtype\n",
    "        self.intercept = None\n",
    "        self.coef = None\n",
    "    def fit(self, x, y):\n",
    "        if x.dtype is not self.dtype:\n",
    "            x = x.type(dtype = self.dtype)\n",
    "        if y.dtype is not self.dtype:\n",
    "            y = y.type(dtype = self.dtype)\n",
    "        u = torch.ones(size = (x.size()[0], 1), dtype = self.dtype)\n",
    "        x_design = torch.cat([u, x], dim = 1)\n",
    "        parameter = torch.inverse(\n",
    "            torch.transpose(x_design, dim0=0, dim1=1) @ x_design) @ \\\n",
    "                    torch.transpose(x_design, dim0=0, dim1=1) @ y\n",
    "        self.intercept = parameter[0, 0]\n",
    "        self.coef = parameter[1:, 0]\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9529711517514095\n",
      "[-4.989112    2.99049829  0.00698989]\n"
     ]
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x, y)\n",
    "print(linear_regression.intercept.numpy())\n",
    "print(linear_regression.coef.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}