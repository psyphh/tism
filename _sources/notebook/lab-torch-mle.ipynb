{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "lab-torch-mle.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "-RbLeUMpv_yA"
   ],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Lab: 最大概似估計\n",
    "================"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `torch` 分配物件\n",
    "\n",
    "### 分配物件之基礎\n",
    "`torch.distribution` 內建了許多機率分配物件（見[官方網頁](https://pytorch.org/docs/stable/distributions.html)），而分配物件可供使用者\n",
    "\n",
    "1. 產生隨機樣本。\n",
    "2. 給定實現值計算可能性或機率值。\n",
    "2. 給定上界計算累積機率值（並非每個分配都可以）。\n",
    "\n",
    "在產生一分配物件時，我們需給定該分配的參數。以常態分配為例，其參數包括了平均數與變異數，此兩參數亦稱作位置（location）參數與尺度（scale）參數"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "normal = Normal(loc=0., scale=1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "再以Binomial分配為例，其參數為嘗試次數與成功之機率（也可以使用對數勝率來設定）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch.distributions import Binomial\n",
    "binomial = Binomial(total_count = 10, probs = 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我們可以透過對分配物件的列印，以了解其內部之參數設定："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal(loc: 0.0, scale: 1.0)\n",
      "Binomial(total_count: 10.0, probs: 0.5, logits: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(normal)\n",
    "print(binomial)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "對於已建立之分配物件，我們可以利用其`.sample()`方法來產生隨機變數"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with shape ():\n",
      " tensor(-1.2133)\n",
      "random sample with shape (3,):\n",
      " tensor([1.2034, 1.0338, 0.8819])\n",
      "random sample with shape (2,3):\n",
      " tensor([[ 0.0807,  0.7612,  1.1109],\n",
      "        [ 0.9675, -2.3148, -0.8792]])\n"
     ]
    }
   ],
   "source": [
    "print(\"random sample with shape ():\\n\",\n",
    "      normal.sample())\n",
    "print(\"random sample with shape (3,):\\n\",\n",
    "      normal.sample(sample_shape=(3,)))\n",
    "print(\"random sample with shape (2,3):\\n\",\n",
    "      normal.sample(sample_shape=(2, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with shape ():\n",
      " tensor(3.)\n",
      "random sample with shape (3,):\n",
      " tensor([5., 5., 3.])\n",
      "random sample with shape (2,3):\n",
      " tensor([[5., 3., 3.],\n",
      "        [5., 4., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"random sample with shape ():\\n\",\n",
    "      binomial.sample())\n",
    "print(\"random sample with shape (3,):\\n\",\n",
    "      binomial.sample(sample_shape=(3,)))\n",
    "print(\"random sample with shape (2,3):\\n\",\n",
    "      binomial.sample(sample_shape=(2, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "從前述的例子，我們可以看到 `sample_shape` 可用於設定產生樣本之個數與樣本張量之排列形狀。\n",
    "\n",
    "給定一組實現值，`log_prob()`可用於計算該實現之對數可能性或機率"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood given value with shape ():\n",
      " tensor([-0.9189]) \n",
      "\n",
      "log-likelihood given value with (3,):\n",
      " tensor([-1.4189, -0.9189, -1.0439]) \n",
      "\n",
      "log-likelihood given value with (2,3):\n",
      " tensor([[-1.4189, -0.9189, -1.0439],\n",
      "        [-2.9189, -1.4189, -5.4189]])\n"
     ]
    }
   ],
   "source": [
    "print(\"log-likelihood given value with shape ():\\n\",\n",
    "      normal.log_prob(value=torch.Tensor([0])), \"\\n\")\n",
    "print(\"log-likelihood given value with (3,):\\n\",\n",
    "      normal.log_prob(value=torch.Tensor([-1, 0, .5])), \"\\n\")\n",
    "print(\"log-likelihood given value with (2,3):\\n\",\n",
    "      normal.log_prob(value=torch.Tensor([[-1, 0, .5], [-2, 1, 3]])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-probability given value with shape ():\n",
      " tensor([-1.4020]) \n",
      "\n",
      "log-probability given value with (3,):\n",
      " tensor([-1.4020, -2.1440, -2.1440]) \n",
      "\n",
      "log-probability given value with (2,3):\n",
      " tensor([[-1.4020, -2.1440, -2.1440],\n",
      "        [-3.1248, -6.9315, -6.9315]])\n"
     ]
    }
   ],
   "source": [
    "print(\"log-probability given value with shape ():\\n\",\n",
    "      binomial.log_prob(value=torch.Tensor([5])), \"\\n\")\n",
    "print(\"log-probability given value with (3,):\\n\",\n",
    "      binomial.log_prob(value=torch.Tensor([5, 3, 7])), \"\\n\")\n",
    "print(\"log-probability given value with (2,3):\\n\",\n",
    "      binomial.log_prob(value=torch.Tensor([[5, 3, 7], [2, 0, 10]])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在給定上界之數值，常態分配之`.cdf()` 可用於計算該上界數值所對應之累積機率數值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative probability given value with shape ():\n",
      " tensor([0.5000]) \n",
      "\n",
      "cumulative probability given value with (3,):\n",
      " tensor([0.1587, 0.5000, 0.6915]) \n",
      "\n",
      "cumulative probability given value with (2,3):\n",
      " tensor([[0.1587, 0.5000, 0.6915],\n",
      "        [0.0228, 0.8413, 0.9987]])\n"
     ]
    }
   ],
   "source": [
    "print(\"cumulative probability given value with shape ():\\n\",\n",
    "      normal.cdf(value=torch.Tensor([0])), \"\\n\")\n",
    "print(\"cumulative probability given value with (3,):\\n\",\n",
    "      normal.cdf(value=torch.Tensor([-1, 0, .5])), \"\\n\")\n",
    "print(\"cumulative probability given value with (2,3):\\n\",\n",
    "      normal.cdf(value=torch.Tensor([[-1, 0, .5], [-2, 1, 3]])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "不過，binomial分配並無 `cdf()` 方法可評估累積機率值。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分配物件之形狀\n",
    "`pytorch` 分配物件之設計，乃參考 `tensorflow_probability`此套件，而分配物件在形狀上，牽涉到三類型之形狀：\n",
    "\n",
    "1. 樣本形狀（sample shape）：為用於描述獨立且具有相同分配隨機樣本之形狀，先前產生隨機樣本時，所設定的 `sample_shape` 即為樣本形狀。\n",
    "2. 批次形狀（batch shape）：為用於描述獨立，但不具有相同分配隨機樣本之形狀，其可以透過模型參數之形狀進行設定。\n",
    "3. 事件形狀（event shape）：為用於描述多變量分配之形狀，各變數間可能不具有統計獨立之特性。\n",
    "\n",
    "先前產生的常態分配，其在 `batch_shape` 與 `event_shape` 上，皆為純量，故其數值為0-d之張量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Normal\n",
    "normal = Normal(loc=0., scale=1.)\n",
    "print(normal.batch_shape)\n",
    "print(normal.event_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下來，我們設定一批次形狀為 `[2]` 之常態分配物件："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "normal_batch = Normal(loc=torch.Tensor([0., 1.]),\n",
    "                      scale=torch.Tensor([1., 1.5]))\n",
    "print(normal_batch.batch_shape)\n",
    "print(normal_batch.event_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "該分配可產生一形狀為 `[2]` 之常態隨機變數，第一個元素的平均數為0，變異數為1，第二個元素的平均數為1，變異數為1.5。接著，我們從該分配中產生不同樣本形狀之隨機樣本"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tensor([-0.4645,  2.5817]) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tensor([[ 1.2168,  1.3941],\n",
      "        [-0.2435, -0.6497],\n",
      "        [ 1.9588,  3.7498]]) \n",
      "\n",
      "random sample with sample_shape (2,3):\n",
      " tensor([[[-1.6750, -0.1854],\n",
      "         [-0.5363,  3.0050],\n",
      "         [ 1.9453,  0.0485]],\n",
      "\n",
      "        [[-0.7708,  0.6906],\n",
      "         [ 0.7839,  2.1603],\n",
      "         [ 0.3667,  2.7540]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      normal_batch.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      normal_batch.sample(sample_shape=(3,)), \"\\n\")\n",
    "print(\"random sample with sample_shape (2,3):\\n\",\n",
    "      normal_batch.sample(sample_shape=(2,3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我們可以看見，產生樣本的張量尺寸為 `sample_size + batch_size`，尺寸的最後一個維度皆為2。\n",
    "\n",
    "\n",
    "當分配物件的 `batch_shape` 為 `[2]` 時，則在評估其對數機率時若僅輸入 `[0]`，則 `[0]` 會被廣播為 `[0, 0]` 評估，而 `[[0], [0]]` 會被廣播為 `[[0, 0], [0, 0]]`。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-probability given value with shape ():\n",
      " tensor([-0.9189, -1.5466]) \n",
      "\n",
      "log-probability given value with shape (2,):\n",
      " tensor([-0.9189, -1.5466]) \n",
      "\n",
      "log-probability given value with shape (2,1):\n",
      " tensor([[-0.9189, -1.5466],\n",
      "        [-0.9189, -1.5466]])\n"
     ]
    }
   ],
   "source": [
    "print(\"log-probability given value with shape ():\\n\",\n",
    "      normal_batch.log_prob(torch.Tensor([0])), \"\\n\")\n",
    "print(\"log-probability given value with shape (2,):\\n\",\n",
    "      normal_batch.log_prob(torch.Tensor([0, 0])), \"\\n\")\n",
    "print(\"log-probability given value with shape (2,1):\\n\",\n",
    "      normal_batch.log_prob(torch.Tensor([[0], [0]])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分配物件的 `event_shape`，可透過多變量分配之參數設定。以多元常態分配為例，我們可以透過其平均數向量與共變異數矩陣設定 `event_shape`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "mvn = MultivariateNormal(\n",
    "    loc=torch.Tensor([0, 1]),\n",
    "    scale_tril=torch.cholesky(torch.Tensor([[1., 0.], [0., .5]])))\n",
    "print(mvn.batch_shape)\n",
    "print(mvn.event_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由於我們給定的平均數向量與共變異數矩陣適用於二維之多變量常態分配，因此，其 `event_shape` 為 `[2]`。這邊需特別注意的是，我們並非直接給定共變異數矩陣，取而代之的是，給定共變異數矩陣之 `cholesky` 拆解。\n",
    "\n",
    "我們可以使用該多元常態分配來產生資料，以及評估其對數可能性數值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tensor([0.0992, 1.2793]) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tensor([[-0.8873,  1.7927],\n",
      "        [ 2.0441,  1.3142],\n",
      "        [-1.4461,  0.3472]]) \n",
      "\n",
      "random sample with sample_shape (2, 3):\n",
      " tensor([[[-0.3771,  0.5297],\n",
      "         [-0.0492,  0.4329],\n",
      "         [ 0.0083,  0.5295]],\n",
      "\n",
      "        [[ 0.5970,  0.9445],\n",
      "         [ 0.4963,  2.1334],\n",
      "         [-0.6908,  0.9495]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      mvn.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      mvn.sample(sample_shape=(3,)), \"\\n\")\n",
    "print(\"random sample with sample_shape (2, 3):\\n\",\n",
    "      mvn.sample(sample_shape=(2, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood given value with shape (2,):\n",
      " tensor(-2.4913) \n",
      "\n",
      "log-likelihood given value with shape (2,1):\n",
      " tensor([-2.4913, -2.4913])\n"
     ]
    }
   ],
   "source": [
    "print(\"log-likelihood given value with shape (2,):\\n\",\n",
    "      mvn.log_prob(torch.Tensor([0, 0])), \"\\n\")\n",
    "print(\"log-likelihood given value with shape (2,1):\\n\",\n",
    "      mvn.log_prob(torch.Tensor([[0, 0], [0, 0]])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "這邊需要別注意的是，屬於同一事件之觀測值，僅會給予一對數機率值，方便用於建立概似函數。\n",
    "\n",
    "\n",
    "另外，也可以透過 `Independent` 此函數，將分配之 `batch_size` 重新解釋為 `event_size`，`reinterpreted_batch_ndims` 用於設定有多少個面向要從 `batch_shape` 轉為 `event_shape`（從右至左）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Independent\n",
    "normal_event = Independent(normal_batch,\n",
    "                           reinterpreted_batch_ndims = 1)\n",
    "print(normal_event.batch_shape)\n",
    "print(normal_event.event_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最後，我們也可以對多元常態分配設定 `batch_shape`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "mvn_batch = MultivariateNormal(\n",
    "    loc=torch.Tensor([[0, 1],[1, 2],[2, 3]]),\n",
    "    scale_tril=torch.cholesky(torch.Tensor([[1., .2], [.2, .5]])))\n",
    "print(mvn_batch.batch_shape)\n",
    "print(mvn_batch.event_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "此分配每次產生一形狀為 `(3, 2)` 之樣本，若進一步設定 `sample_shape`，則其產生之樣本張量形狀為 `smaple_shape + (3, 2)`："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sample with sample_shape ():\n",
      " tensor([[ 1.5312,  1.8764],\n",
      "        [-0.3711,  1.3287],\n",
      "        [ 1.1071,  3.1308]]) \n",
      "\n",
      "random sample with sample_shape (3,):\n",
      " tensor([[[-1.2689,  0.7526],\n",
      "         [ 0.6875,  1.2836],\n",
      "         [ 1.3875,  3.8003]],\n",
      "\n",
      "        [[-0.1016,  1.8281],\n",
      "         [ 0.4350,  1.3775],\n",
      "         [ 0.7873,  1.8207]],\n",
      "\n",
      "        [[-0.3644,  1.5479],\n",
      "         [ 1.2765,  3.1208],\n",
      "         [ 1.2598,  2.6793]]]) \n",
      "\n",
      "random sample with sample_shape (2, 3):\n",
      " tensor([[[[-0.8765,  1.0153],\n",
      "          [ 0.4457,  0.3573],\n",
      "          [ 0.3373,  2.6802]],\n",
      "\n",
      "         [[ 1.1544,  0.2747],\n",
      "          [ 2.6681,  2.1695],\n",
      "          [ 1.9862,  2.5194]],\n",
      "\n",
      "         [[-0.6112,  1.6778],\n",
      "          [ 0.7522,  2.6097],\n",
      "          [ 1.7715,  3.4655]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8301,  0.1247],\n",
      "          [ 1.7661,  1.7685],\n",
      "          [ 3.3757,  2.4665]],\n",
      "\n",
      "         [[-0.2240,  0.0623],\n",
      "          [ 1.1988,  1.7303],\n",
      "          [ 3.4870,  2.8650]],\n",
      "\n",
      "         [[ 0.8595,  0.4615],\n",
      "          [-0.1280,  2.1399],\n",
      "          [ 3.2188,  2.7153]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"random sample with sample_shape ():\\n\",\n",
    "      mvn_batch.sample(), \"\\n\")\n",
    "print(\"random sample with sample_shape (3,):\\n\",\n",
    "      mvn_batch.sample(sample_shape=(3,)), \"\\n\")\n",
    "print(\"random sample with sample_shape (2, 3):\\n\",\n",
    "      mvn_batch.sample(sample_shape=(2, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "關於前述三種形狀之說明，讀者亦可參考此[網誌](https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/)。\n",
    "\n",
    "## 最大概似估計法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 建立概似函數\n",
    "利用 `torch` 的分配物件，我們可以很容易地來建立概似函數。\n",
    "\n",
    "首先，我們先以常態分配進行說明。為了產生樣本資料，我們先設定一平均數為5，標準差為4之常態分配"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal model:\n",
      " Normal(loc: tensor([5.]), scale: tensor([2.])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_true = torch.tensor([5.])\n",
    "sigma_true = torch.tensor([2.])\n",
    "model_normal_true = Normal(\n",
    "    loc=mu_true,\n",
    "    scale=sigma_true)\n",
    "print(\"normal model:\\n\", model_normal_true, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接著，我們可以利用此常態分配來產生一樣本數為1000之資料，並評估該資料在平均數為5，標準差為4此常態分配下之負對數可能性（negative log-likelihood）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative likelihood value is tensor(2.0907)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "x = model_normal_true.sample(sample_shape=(sample_size,))\n",
    "loss_value = - model_normal_true.log_prob(x).mean()\n",
    "print(\"negative likelihood value is\", loss_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因此，只要在資料的形狀上可以匹配，我們即可使用分配物件的`log_prob()` 方法，再搭配 `sum()` 或是 `mean()` 來計算對數概似函數數值。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 進行優化\n",
    "\n",
    "在建立完概似函數後，我們就可以透過 `torch` 的優化器進行優化。在這邊需要特別注意的是，由於模型的參數會在優化過程中更新，因此，其必須使用一可為分之張量來儲存，並且，在每次更新完參數數值後，皆需再次產生一新的分配物件，以計算概述函數之數值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "lr = 1.0\n",
    "mu = torch.tensor([0.], requires_grad=True)\n",
    "sigma = torch.tensor([1.], requires_grad=True)\n",
    "opt = torch.optim.Adam([mu, sigma], lr=.5)\n",
    "for epoch in range(epochs):\n",
    "    model_normal = Normal(loc=mu, scale=sigma)\n",
    "    loss_value = - model_normal.log_prob(x).mean()\n",
    "    opt.zero_grad()\n",
    "    loss_value.backward() # compute the gradient\n",
    "    opt.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML mean by gradient descent: [5.1135783]\n",
      "ML std by gradient descent: [1.9526198]\n"
     ]
    }
   ],
   "source": [
    "print(\"ML mean by gradient descent:\", mu.data.numpy())\n",
    "print(\"ML std by gradient descent:\", sigma.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我們可以比較前述使用梯度下降法所得到的結果，與直接帶公式計算結果間的差異，可以發現兩者間的差異主要展現在小數點後3位，是絕大多數情況下可以忽略的誤差。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML mean by formula: 5.1134906\n",
      "ML std by formula: 1.9535666\n"
     ]
    }
   ],
   "source": [
    "print(\"ML mean by formula:\", torch.mean(x).numpy())\n",
    "print(\"ML std by formula:\", torch.std(x, unbiased=False).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 多元常態分配之最大概似估計\n",
    "\n",
    "多元常態分配為統計建模中常被使用之分配，因此，我們在此對該分配之參數進行最大概似法之估計。\n",
    "\n",
    "多元常態分配之最大概似估計最麻煩的部分在於，共變異數矩陣是對稱正定矩陣，因此，雖然共變異矩陣中有 $P \\times P$ 個元素，但事實上，其僅有 $P(P+1)/2$ 個能夠自由估計之參數，並且，其數值需滿足正定矩陣之要求。為了處理此困難，在進行多元常態分配之參數設定時，我們不直接設定共變異數矩陣，取而代之的是，設定該矩陣之 Cholesky 拆解，即 `scale_tril`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean vector: \n",
      " tensor([-1.,  0.,  1.])\n",
      "true covariance matrix: \n",
      " tensor([[9.0000, 6.0000, 1.2000],\n",
      "        [6.0000, 5.0000, 1.3000],\n",
      "        [1.2000, 1.3000, 0.6600]])\n"
     ]
    }
   ],
   "source": [
    "mu_true = torch.tensor([-1., 0., 1.])\n",
    "sigma_tril_true = torch.tensor([[3., 0., 0.], [2., 1., 0.], [.4, .5, .5]])\n",
    "model_mvn_true = MultivariateNormal(\n",
    "    loc=mu_true,\n",
    "    scale_tril=sigma_tril_true)\n",
    "print(\"true mean vector: \\n\", model_mvn_true.mean)\n",
    "print(\"true covariance matrix: \\n\", model_mvn_true.covariance_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "前一程式碼所展示的共變異數矩陣，可透過以下的公式獲得"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9.0000, 6.0000, 1.2000],\n        [6.0000, 5.0000, 1.3000],\n        [1.2000, 1.3000, 0.6600]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_tril_true @ sigma_tril_true.t()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "前式可以確保所得到的共變異數矩陣為對稱矩陣，另外，如果說給定的對角線元素為正的，則可以進一步確保該共變異數矩陣為對稱正定矩陣。\n",
    "\n",
    "接著，我們就可以利用該分配物件來產生資料、計算概似函數數值、以及計算最大概似估計值。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative likelihood value is tensor(4.7281)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "x = model_mvn_true.sample(sample_shape=(sample_size,))\n",
    "loss_value = -model_mvn_true.log_prob(x).mean()\n",
    "print(\"negative likelihood value is\", loss_value)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "lr = .1\n",
    "mu = torch.tensor(\n",
    "    [0., 0., 0.], requires_grad=True)\n",
    "sigma_tril = torch.tensor(\n",
    "    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\n",
    "    requires_grad=True)\n",
    "opt = torch.optim.Adam([mu, sigma_tril], lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_mvn = MultivariateNormal(\n",
    "    loc=mu,\n",
    "    scale_tril=sigma_tril)\n",
    "    loss_value = -torch.mean(model_mvn.log_prob(x))\n",
    "    opt.zero_grad()\n",
    "    loss_value.backward() # compute the gradient\n",
    "    opt.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML mean by gradient descent: \n",
      " tensor([-0.9589,  0.0221,  1.0172], requires_grad=True)\n",
      "ML covariance by gradient descent: \n",
      " tensor([[8.9483, 5.9167, 1.1431],\n",
      "        [5.9167, 5.0637, 1.3194],\n",
      "        [1.1431, 1.3194, 0.6678]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"ML mean by gradient descent: \\n\", mu)\n",
    "print(\"ML covariance by gradient descent: \\n\", sigma_tril @ sigma_tril.t())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML mean by formula: \n",
      " tensor([-0.9589,  0.0221,  1.0172])\n",
      "ML covariance by formula: \n",
      " tensor([[8.9483, 5.9167, 1.1431],\n",
      "        [5.9167, 5.0637, 1.3194],\n",
      "        [1.1431, 1.3194, 0.6678]])\n"
     ]
    }
   ],
   "source": [
    "sample_mean = torch.mean(x, dim = 0)\n",
    "sample_moment2 = (x.t() @ x) / sample_size\n",
    "sample_cov = sample_moment2 - torch.ger(sample_mean, sample_mean)\n",
    "print(\"ML mean by formula: \\n\", sample_mean)\n",
    "print(\"ML covariance by formula: \\n\", sample_cov)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [2., 3., 0.],\n",
      "        [4., 5., 6.]], grad_fn=<IndexPutBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5., 6.], requires_grad=True)\n",
    "m = torch.zeros((3, 3))\n",
    "\n",
    "tril_indices = torch.tril_indices(row=3, col=3, offset=0)\n",
    "m[tril_indices[0], tril_indices[1]] = x\n",
    "print(m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 實徵範例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 產生邏吉斯迴歸資料"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fb70883e290>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(246437)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli\n",
    "def generate_data(n_sample,\n",
    "                  weight,\n",
    "                  bias = 0,\n",
    "                  mean_feature = 0,\n",
    "                  std_feature = 1,\n",
    "                  dtype = torch.float64):\n",
    "    weight = torch.tensor(weight, dtype = dtype)\n",
    "    n_feature = weight.shape[0]\n",
    "    x = torch.normal(mean = mean_feature,\n",
    "                     std = std_feature,\n",
    "                     size = (n_sample, n_feature),\n",
    "                     dtype = dtype)\n",
    "    weight = weight.view(size = (-1, 1))\n",
    "    logit = bias + x @ weight\n",
    "    bernoulli = Bernoulli(logits = logit)\n",
    "    y = bernoulli.sample()\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# run generate_data\n",
    "x, y = generate_data(n_sample = 1000,\n",
    "                     weight = [-5, 3, 0],\n",
    "                     bias = 2,\n",
    "                     mean_feature = 10,\n",
    "                     std_feature = 3,\n",
    "                     dtype = torch.float64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 建立一進行邏吉斯迴歸分析之物件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# define a class to fit logistic regression\n",
    "class LogisticRegression():\n",
    "    def __init__(self, dtype = torch.float64):\n",
    "        self.dtype = dtype\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "    def log_lik(self, bias, weight):\n",
    "        logit = bias + self.x @ weight\n",
    "        bernoulli = Bernoulli(logits = logit)\n",
    "        return bernoulli.log_prob(self.y).mean()\n",
    "    def fit(self, x, y, optimizer = \"LBFGS\",\n",
    "            epochs = 200, lr = .1, tol = 10**(-7)):\n",
    "        if x.dtype is not self.dtype:\n",
    "            x = x.type(dtype = self.dtype)\n",
    "        if y.dtype is not self.dtype:\n",
    "            y = y.type(dtype = self.dtype)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_sample = x.size()[0]\n",
    "        self.n_feature = x.size()[1]\n",
    "        bias = torch.zeros(size = (1,),\n",
    "                           dtype = self.dtype,\n",
    "                           requires_grad = True)\n",
    "        weight = torch.zeros(size = (self.n_feature, 1),\n",
    "                             dtype = self.dtype,\n",
    "                             requires_grad = True)\n",
    "        if optimizer == \"LBFGS\":\n",
    "            opt = torch.optim.LBFGS([bias, weight],\n",
    "                                    lr=lr, max_iter = epochs,\n",
    "                                    tolerance_grad = tol,\n",
    "                                    line_search_fn = \"strong_wolfe\")\n",
    "            def closure():\n",
    "                opt.zero_grad()\n",
    "                loss_value = - self.log_lik(bias, weight)\n",
    "                loss_value.backward()\n",
    "                return loss_value\n",
    "            opt.step(closure)\n",
    "        else:\n",
    "            opt = getattr(torch.optim, optimizer)([bias, weight], lr=lr)\n",
    "            for epoch in range(epochs):\n",
    "                opt.zero_grad()\n",
    "                loss_value = - self.log_lik(bias, weight)\n",
    "                loss_value.backward()\n",
    "                with torch.no_grad():\n",
    "                    grad_max = max(bias.grad.abs().max().item(),\n",
    "                                   weight.grad.abs().max().item())\n",
    "                if (grad_max < tol):\n",
    "                    break\n",
    "                opt.step()\n",
    "        self.bias = bias.data.numpy()\n",
    "        self.weight = weight.data.numpy()\n",
    "        # print(opt.state_dict())\n",
    "        return self\n",
    "    def vcov(self):\n",
    "        from torch.autograd.functional import hessian\n",
    "        bias, weight = torch.tensor(self.bias), torch.tensor(self.weight)\n",
    "        h = hessian(self.log_lik, (bias, weight))\n",
    "        fisher_obs = -torch.cat([torch.cat([h[0][0],h[0][1].squeeze(dim = 2)], dim = 1),\n",
    "                                torch.cat([h[1][0].squeeze(dim =0).squeeze(dim =1),\n",
    "                                           h[1][1].squeeze()], dim = 1)],\n",
    "                               dim = 0)\n",
    "\n",
    "        vcov = torch.inverse(fisher_obs)/self.n_sample\n",
    "        return vcov"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 計算模型參數"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.73765684]\n",
      "[[-6.32144761]\n",
      " [ 3.74170747]\n",
      " [-0.05765073]]\n"
     ]
    }
   ],
   "source": [
    "# fit logistic model\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(x, y, optimizer = \"LBFGS\", epochs = 2000, lr = 1)\n",
    "print(model_lr.bias)\n",
    "print(model_lr.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.736656]\n",
      "[[-6.31995124  3.74083793 -0.05764225]]\n"
     ]
    }
   ],
   "source": [
    "# fit logistic model via sklearn\n",
    "# please install sklearn first\n",
    "from sklearn import linear_model\n",
    "model_lr_sklearn = linear_model.LogisticRegression(C=10000)\n",
    "model_lr_sklearn.fit(x, y.flatten())\n",
    "print(model_lr_sklearn.intercept_)\n",
    "print(model_lr_sklearn.coef_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 計算參數估計標準誤"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "vcov = model_lr.vcov()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.0433, 1.1524, 0.6945, 0.1204], dtype=torch.float64)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcov.diagonal().sqrt()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.032809\n",
      "         Iterations 13\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 1000\n",
      "Model:                          Logit   Df Residuals:                      996\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 10 Nov 2020   Pseudo R-squ.:                  0.9211\n",
      "Time:                        22:13:39   Log-Likelihood:                -32.809\n",
      "converged:                       True   LL-Null:                       -415.71\n",
      "Covariance Type:            nonrobust   LLR p-value:                1.133e-165\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.7373      2.043      1.829      0.067      -0.267       7.742\n",
      "x1            -6.3211      1.152     -5.485      0.000      -8.580      -4.063\n",
      "x2             3.7415      0.694      5.388      0.000       2.380       5.103\n",
      "x3            -0.0577      0.120     -0.479      0.632      -0.294       0.178\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.81 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "# fit logistic model via statsmodels\n",
    "# please install statsmodels first\n",
    "import statsmodels.api as sm\n",
    "model_lr_sm = sm.Logit(y.numpy(), sm.add_constant(x.numpy()))\n",
    "print(model_lr_sm.fit().summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 練習\n",
    "1. 請建立一類型，其可以使用最大概似法，執行線性回歸分析。\n",
    "\n",
    "2. 請在前述的類型中，加入一摘要方法（summary()），該方法可以列印出參數估計的假設檢定與信賴區間。\n",
    "\n",
    "3. 請建立一類型，其可以估計多元常態分配的平均數與共變異數矩陣，並且提供各參數估計之標準誤。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}