{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lab: 線性代數\n",
    "================\n",
    "\n",
    "此 lab 中，我們將會透過 `tensorflow` 此套件，學習以下的主題。\n",
    "\n",
    "1. 認識 `tensorflow` 的張量（tensor）之基礎。\n",
    "\n",
    "2. 了解如何對 `tensorflow` 張量進行操弄。\n",
    "\n",
    "3. 使用 `tensorflow` 進行線性代數之運算。\n",
    "\n",
    "4. 應用前述之知識，建立一可進行線性迴歸分析之類型（class）。\n",
    "\n",
    "`tensorflow`之安裝與基礎教學，可參考 [tensorflow官方網頁](https://www.tensorflow.org)。在安裝完成後，可透過以下的指令載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 張量之基礎\n",
    "\n",
    "### 張量之輸入\n",
    "`tensorflow` 最基本的物件是張量（tensor），其與 `numpy` 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 `python` 的 `list` 或是 `numpy` 的 `ndarray`），置於`tf.constant`函數中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor(data = [[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12]])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過 `type()`，可看見其屬於 `EagerTensor` 此一類型（class），若欲了解 `a` 的樣貌，我們可使用 `print` 指令來列印其主要的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過對 `a` 列印的結果，我們可觀察到：\n",
    "\n",
    "+ `a` 內部的資料數值（value）為 `[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]`。\n",
    "+ `a` 的形狀（shape）為 `(3, 4)`，表示 `a` 為一 $3 \\times 4$ 之張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則\n",
    "+ `a` 的資料類型（data type）為 `int32`，表示32位元的整數。在進行運算時，張量間的類型須相同。一般來說，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之數值\n",
    "若要獲得張量的資料數值（value），可透過 `.numpy()`獲得，其回傳該張量對應之 `numpy` 陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data of tensor is: \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"data of tensor is: \\n\", a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` 內建了多種函數，以協助產生具有特別數值結構之張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with all elements being ones \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "tensor with all elements being zeros \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "identity-like tensor \n",
      " [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "diagonal matrix \n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with all elements being ones \\n\",\n",
    "      torch.ones(size = (4, 1)).numpy())\n",
    "print(\"tensor with all elements being zeros \\n\",\n",
    "      torch.zeros(size = (2, 3)).numpy())\n",
    "print(\"identity-like tensor \\n\",\n",
    "      torch.eye(n = 3, m = 5).numpy())\n",
    "print(\"diagonal matrix \\n\",\n",
    "      torch.diag(input = torch.tensor([1, 2, 3, 4])).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` 亦可指定分配產生隨機的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with random elements from uniform(0, 1) \n",
      " [[0.31774068 0.63178676 0.6246827  0.10044563 0.7976905  0.7206251 ]\n",
      " [0.34286612 0.14121383 0.9462809  0.57766175 0.7642721  0.33148348]]\n",
      "tensor with uninitialized data \n",
      " [[9.8090893e-45 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [7.7052459e+31 1.9446917e+31 5.0206850e+28 2.3328567e-18 4.2329797e+21\n",
      "  7.2250795e+28]]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with random elements from uniform(0, 1) \\n\",\n",
    "      torch.rand(size = (2, 6)).numpy())\n",
    "print(\"tensor with uninitialized data \\n\",\n",
    "      torch.empty(size = (2, 6)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之形狀\n",
    "\n",
    "張量之形狀與形狀之維度數量，可透過張量物件的 `.shape` 與 `ndim` 方法來獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of tensor is torch.Size([3, 4])\n",
      "dim of tensor is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"size of tensor is\", a.size())\n",
    "print(\"dim of tensor is\", a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要對張量的形狀進行改變的話，可透過 `tf.reshape()` 此函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor with shape (4, 3): \n",
      " [[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "tensor with shape (2, 2, 3): \n",
      " [[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]]\n",
      "tensor with shape (12, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12, 1) by (-1, 1): \n",
      " [[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "tensor with shape (12,): \n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensor with shape (4, 3): \\n\",\n",
    "      a.view(size = (4, 3)).numpy())\n",
    "print(\"tensor with shape (2, 2, 3): \\n\",\n",
    "      a.view(size = (2, 2, 3)).numpy())\n",
    "print(\"tensor with shape (12, 1): \\n\",\n",
    "      a.view(size = (12, 1)).numpy())\n",
    "print(\"tensor with shape (12, 1) by (-1, 1): \\n\",\n",
    "      a.view(size = (-1, 1)).numpy())\n",
    "print(\"tensor with shape (12,): \\n\",\n",
    "      a.view(size = (12, )).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意，`(12, 1)` 與 `(12,)` 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之資料類型\n",
    "張量的資料類型，可透過 `.dtype` 方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of tensor is torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"data type of tensor is\", a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若是要調整資料類型的話，則可透過 `tf.cast()`此函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a.type(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`tensorflow` 內建多種資料類型，包含整數類型（如 `tf.int32` 與 `tf.int64`）與浮點數類型（如 `tf.float32` 與 `tf.float64`），完整的資料類型請見 [tf.dtypes.DType](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)。\n",
    "\n",
    "在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 `tensorflow` 常用之資料類型為 `tf.float32` 與 `tf.float64`，前者所需的記憶體較小，但運算結果的數值誤差較大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 張量之操弄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之切片\n",
    "\n",
    "若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。`tensorflow` 張量的切片方式，與 `numpy` 類似，皆使用中括號 `[]`，再搭配所欲擷取資料行列的索引（index）獲得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract 1st row: \n",
      " [1 2 3 4]\n",
      "extract 1st and 2nd rows: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "extract 2nd column: \n",
      " [ 2  6 10]\n",
      "extract 2nd and 3rd columns: \n",
      " [[ 2  3]\n",
      " [ 6  7]\n",
      " [10 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"extract 1st row: \\n\",\n",
    "      a[0, :].numpy())\n",
    "print(\"extract 1st and 2nd rows: \\n\",\n",
    "      a[:2, :].numpy())\n",
    "print(\"extract 2nd column: \\n\",\n",
    "      a[:, 1].numpy())\n",
    "print(\"extract 2nd and 3rd columns: \\n\",\n",
    "      a[:, 1:3].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "進行切片時，有幾項重點需要注意。\n",
    "\n",
    "+ 各面向之索引從0開始。\n",
    "+ 負號表示從結尾數回來，如 `-1` 表示最後一個位置。\n",
    "+ `:`表示該面向所有元素皆挑選。\n",
    "+ `start:stop` 表示從 `start` 開始挑選到 `stop-1`。\n",
    "+ `start:stop:step` 表示從 `start` 開始到 `stop-1`，間隔 `step` 挑選。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之串接\n",
    "多個張量在維度可對應之前提下，可透過 `tf.concat` 串接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertical concatenation \n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "horizontal concatenation \n",
      " [[ 1  2  3  4  1  2  3  4]\n",
      " [ 5  6  7  8  5  6  7  8]\n",
      " [ 9 10 11 12  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"vertical concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 0).numpy())\n",
    "print(\"horizontal concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量之運算\n",
    "考慮以下 `a` 與 `b` 兩張量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a is \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "tensor b is \n",
      " [[1. 2.]\n",
      " [1. 2.]\n",
      " [1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(data = [[1, 2], [3, 4], [5, 6]],\n",
    "                dtype = torch.float64)\n",
    "b = torch.tensor(data = [[1, 2], [1, 2], [1, 2]],\n",
    "                dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a.numpy())\n",
    "print(\"tensor b is \\n\", b.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們將使用 `a` 與 `b` 來展示如何使用 `tensorflow` 進行張量間的計算。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量元素對元素之運算\n",
    "透過 `tensorflow` 的數學函數，可進行張量元素對元素的四則運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"element-wise add \\n\",\n",
    "      torch.add(a, b))\n",
    "print(\"element-wise subtract \\n\",\n",
    "      torch.sub(a, b))\n",
    "print(\"element-wise multiply \\n\",\n",
    "      torch.mul(a, b))\n",
    "print(\"element-wise divide \\n\",\n",
    "      torch.div(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "前述採用的函數，皆可取代為其所對應之運算子計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise add \n",
      " tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]], dtype=torch.float64)\n",
      "element-wise subtract \n",
      " tensor([[0., 0.],\n",
      "        [2., 2.],\n",
      "        [4., 4.]], dtype=torch.float64)\n",
      "element-wise multiply \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 3.,  8.],\n",
      "        [ 5., 12.]], dtype=torch.float64)\n",
      "element-wise divide \n",
      " tensor([[1., 1.],\n",
      "        [3., 2.],\n",
      "        [5., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"element-wise add \\n\", a + b)\n",
    "print(\"element-wise subtract \\n\", a - b)\n",
    "print(\"element-wise multiply \\n\", a * b)\n",
    "print(\"element-wise divide \\n\", a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 [tf.math](https://www.tensorflow.org/api_docs/python/tf/math) 此模組中尋找對應的數學函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量線性代數之運算\n",
    "除了簡單的四則運算外，當張量的 `ndim` 為2時，`tensorflow` 提供了進行線性代數（linear algebra）相關的函數，如\n",
    "\n",
    "+ 矩陣轉置（matrix transpose）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transpose of a is \n",
      " [[1. 3. 5.]\n",
      " [2. 4. 6.]]\n"
     ]
    }
   ],
   "source": [
    "a_t = torch.transpose(input=a, dim0=0, dim1=1)\n",
    "print(\"transpose of a is \\n\",\n",
    "      a_t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ 矩陣乘法（matrix multiplication）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = a_t @ a is \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "c = a_t @ a\n",
    "print(\"c = a_t @ a is \\n\",\n",
    "      c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 反矩陣（matrix inverse）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse of c is \n",
      " [[ 2.33333333 -1.83333333]\n",
      " [-1.83333333  1.45833333]]\n",
      "check for inverse (left) \n",
      " [[ 1.00000000e+00 -1.42108547e-14]\n",
      " [ 0.00000000e+00  1.00000000e+00]]\n",
      "check for inverse (right) \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "c_inv = torch.inverse(input = c)\n",
    "print(\"inverse of c is \\n\",\n",
    "      c_inv.numpy()) # c @ c_inv should be identity matrix\n",
    "print(\"check for inverse (left) \\n\",\n",
    "      (c_inv @ c).numpy())\n",
    "print(\"check for inverse (right) \\n\",\n",
    "      (c @ c_inv).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ Cholesky 拆解（Cholesky decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky factor of c is \n",
      " [[5.91607978 0.        ]\n",
      " [7.43735744 0.82807867]]\n",
      "check for Cholesky decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "c_chol = torch.cholesky(input = c)\n",
    "print(\"Cholesky factor of c is \\n\",\n",
    "      c_chol.numpy())\n",
    "print(\"check for Cholesky decomposition \\n\",\n",
    "      (c_chol @ torch.transpose(c_chol, 0 , 1)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 特徵拆解（eigen-decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue of c is \n",
      " [ 0.26450509 90.73549491]\n",
      "eigenvector of c is \n",
      " [[-0.78489445  0.61962948]\n",
      " [ 0.61962948  0.78489445]]\n",
      "check for eigen-decomposition \n",
      " [[35. 44.]\n",
      " [44. 56.]]\n"
     ]
    }
   ],
   "source": [
    "e, v = torch.symeig(input = c, eigenvectors=True)\n",
    "print(\"eigenvalue of c is \\n\",\n",
    "      e.numpy())\n",
    "print(\"eigenvector of c is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for eigen-decomposition \\n\",\n",
    "      (v @ torch.diag(e) @\n",
    "       torch.transpose(v, 0 , 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 奇異值拆解（singular value decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singular value of a is \n",
      " [9.52551809 0.51430058]\n",
      "left singular vector of a is \n",
      " [[-0.2298477   0.88346102]\n",
      " [-0.52474482  0.24078249]\n",
      " [-0.81964194 -0.40189603]]\n",
      "right singular vector of a is \n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "check for singular value decomposition \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "u, s, v = torch.svd(input = a)\n",
    "print(\"singular value of a is \\n\",\n",
    "      s.numpy())\n",
    "print(\"left singular vector of a is \\n\",\n",
    "      u.numpy())\n",
    "print(\"right singular vector of a is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for singular value decomposition \\n\",\n",
    "      (u @ torch.diag(s) @\n",
    "       torch.transpose(v, 0, 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 對張量之數值進行摘要\n",
    "`tf.math` 提供了一些化約（reduce）的函數，對張量內的數值進行摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean \n",
      " 3.5\n",
      "calculate standard deviation \n",
      " 1.8708286933869707\n",
      "calculate max \n",
      " 6.0\n",
      "calculate min \n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate mean \\n\",\n",
    "      torch.mean(input = a).numpy())\n",
    "print(\"calculate standard deviation \\n\",\n",
    "      torch.std(input = a).numpy())\n",
    "print(\"calculate max \\n\",\n",
    "      torch.max(input = a).numpy())\n",
    "print(\"calculate min \\n\",\n",
    "      torch.min(input = a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們亦可對張量的各面向，進行前述的摘要。以平均數為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate mean for each column \n",
      " [3. 4.]\n",
      "calculate mean for each row \n",
      " [1.5 3.5 5.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"calculate mean for each column \\n\",\n",
    "      torch.mean(input = a, dim=0).numpy())\n",
    "print(\"calculate mean for each row \\n\",\n",
    "      torch.mean(input = a, dim=1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 實徵範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生線性迴歸資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to generate x and y\n",
    "def generate_data(n_sample, coef, intercept = 0, sd_residual = 1,\n",
    "                  mean_feature = 0, sd_feature = 1,\n",
    "                  dtype = tf.float64, seed = None):\n",
    "    coef = tf.constant(coef, dtype = dtype)\n",
    "    n_feature = coef.shape[0]\n",
    "    x = tf.random.normal(shape = (n_sample, n_feature),\n",
    "                         mean = mean_feature,\n",
    "                         stddev = sd_feature,\n",
    "                         seed = seed, dtype = dtype)\n",
    "    e = tf.random.normal(shape = (n_sample, 1),\n",
    "                         mean = 0,\n",
    "                         stddev = sd_residual,\n",
    "                         seed = seed, dtype = dtype)\n",
    "    coef = tf.reshape(coef, shape = (-1, 1))\n",
    "    y = intercept + x @ coef + e\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix x is \n",
      " [[ 5.48848568 13.61840308 15.88021677]\n",
      " [11.37361141 10.45760477  7.29207417]\n",
      " [ 8.76283527  7.25985482  4.44062436]\n",
      " [ 6.44741895 12.51466253 11.91473433]\n",
      " [13.05764078 12.00758984  8.85894785]\n",
      " [16.00571498  6.44779053  6.17078322]\n",
      " [ 9.45319088 12.23745452  5.14101102]\n",
      " [10.99438762  3.68643527  8.68771621]\n",
      " [ 8.10937642  6.67353645  8.66068655]\n",
      " [12.13642167  7.61931554 10.45228741]]\n",
      "response vector y is \n",
      " [[ 19.08869209]\n",
      " [-21.47155787]\n",
      " [-18.28819253]\n",
      " [ 11.00760759]\n",
      " [-24.22280723]\n",
      " [-55.18004606]\n",
      " [ -5.9423839 ]\n",
      " [-39.84312934]\n",
      " [-15.45899857]\n",
      " [-31.83700397]]\n"
     ]
    }
   ],
   "source": [
    "# run generate_data\n",
    "x, y = generate_data(\n",
    "    n_sample = 10, coef = [-5, 3, 0],\n",
    "    intercept = 5, sd_residual = 1,\n",
    "    mean_feature = 10, sd_feature = 3,\n",
    "    dtype = tf.float64, seed = 48)\n",
    "print(\"feature matrix x is \\n\", x.numpy())\n",
    "print(\"response vector y is \\n\", y.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 計算模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to calculate model parameter\n",
    "def calculate_parameter(x, y, dtype = tf.float64):\n",
    "    if x.dtype is not dtype:\n",
    "        x = tf.cast(x, dtype = dtype)\n",
    "    if y.dtype is not dtype:\n",
    "        y = tf.cast(y, dtype = dtype)\n",
    "    u = tf.ones(shape = (x.shape[0], 1), dtype = dtype)\n",
    "    x_design = tf.concat([u, x], axis = 1)\n",
    "    parameter = tf.linalg.inv(tf.transpose(x_design) @ x_design) @ \\\n",
    "                tf.transpose(x_design) @ y\n",
    "    intercept = parameter[0, 0]\n",
    "    coef = parameter[1:, 0]\n",
    "    return intercept, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept estimate is \n",
      " 4.75108692695553\n",
      "coefficient estimate is \n",
      " [-5.01016455  3.0247444   0.00671568]\n"
     ]
    }
   ],
   "source": [
    "# run calculate_parameter\n",
    "x, y = generate_data(\n",
    "    n_sample = 1000, coef = [-5, 3, 0],\n",
    "    intercept = 5, sd_residual = 1,\n",
    "    mean_feature = 10, sd_feature = 3,\n",
    "    dtype = tf.float64, seed = 48)\n",
    "intercept, coef = calculate_parameter(x, y)\n",
    "print(\"intercept estimate is \\n\", intercept.numpy())\n",
    "print(\"coefficient estimate is \\n\", coef.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 建立一進行迴歸分析之物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a class to fit linear regression\n",
    "class LinearRegression():\n",
    "    def __init__(self, dtype = tf.float64):\n",
    "        self.dtype = dtype\n",
    "        self.intercept = None\n",
    "        self.coef = None\n",
    "    def fit(self, x, y):\n",
    "        if x.dtype is not self.dtype:\n",
    "            x = tf.cast(x, dtype = self.dtype)\n",
    "        if y.dtype is not self.dtype:\n",
    "            y = tf.cast(y, dtype = self.dtype)\n",
    "        u = tf.ones(shape = (x.shape[0], 1), dtype = self.dtype)\n",
    "        x_design = tf.concat([u, x], axis = 1)\n",
    "        parameter = tf.linalg.inv(tf.transpose(x_design) @ x_design) @ \\\n",
    "                tf.transpose(x_design) @ y\n",
    "        self.intercept = parameter[0, 0]\n",
    "        self.coef = parameter[1:, 0]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.75108692695553\n",
      "[-5.01016455  3.0247444   0.00671568]\n"
     ]
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x, y)\n",
    "print(linear_regression.intercept.numpy())\n",
    "print(linear_regression.coef.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}