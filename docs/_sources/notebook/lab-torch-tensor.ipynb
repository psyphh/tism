{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lab: 線性代數\n",
    "================\n",
    "\n",
    "此 lab 中，我們將會透過 `tensorflow` 此套件，學習以下的主題。\n",
    "\n",
    "1. 認識 `tensorflow` 的張量（tensor）之基礎。\n",
    "\n",
    "2. 了解如何對 `tensorflow` 張量進行操弄。\n",
    "\n",
    "3. 使用 `tensorflow` 進行線性代數之運算。\n",
    "\n",
    "4. 應用前述之知識，建立一可進行線性迴歸分析之類型（class）。\n",
    "\n",
    "`tensorflow`之安裝與基礎教學，可參考 [tensorflow官方網頁](https://www.tensorflow.org)。在安裝完成後，可透過以下的指令載入"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 張量之基礎\n",
    "\n",
    "### 張量之輸入\n",
    "`tensorflow` 最基本的物件是張量（tensor），其與 `numpy` 的陣列（array）相當的類似。產生一個張量最基本的方法為，將所欲形成張量的資料（其可為 `python` 的 `list` 或是 `numpy` 的 `ndarray`），置於`tf.constant`函數中\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-6c548cfeecad>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m a = torch.tensor(data = [[1, 2, 3, 4],\n\u001B[1;32m      3\u001B[0m                          \u001B[0;34m[\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                          [9, 10, 11, 12]])\n\u001B[1;32m      5\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor(data = [[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12]])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過 `type()`，可看見其屬於 `EagerTensor` 此一類型（class），若欲了解 `a` 的樣貌，我們可使用 `print` 指令來列印其主要的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-6ca8edb7b3ab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "透過對 `a` 列印的結果，我們可觀察到：\n",
    "\n",
    "+ `a` 內部的資料數值（value）為 `[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]`。\n",
    "+ `a` 的形狀（shape）為 `(3, 4)`，表示 `a` 為一 $3 \\times 4$ 之張量。在進行運算時，張量間的形狀需滿足某些條件，如相同，或是滿足某種廣播（broadcasting）的規則\n",
    "+ `a` 的資料類型（data type）為 `int32`，表示32位元的整數。在進行運算時，張量間的類型須相同。一般來說，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之數值\n",
    "若要獲得張量的資料數值（value），可透過 `.numpy()`獲得，其回傳該張量對應之 `numpy` 陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"data of tensor is: \\n\", a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` 內建了多種函數，以協助產生具有特別數值結構之張量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tensor with all elements being ones \\n\",\n",
    "      torch.ones(size = (4, 1)).numpy())\n",
    "print(\"tensor with all elements being zeros \\n\",\n",
    "      torch.zeros(size = (2, 3)).numpy())\n",
    "print(\"identity-like tensor \\n\",\n",
    "      torch.eye(n = 3, m = 5).numpy())\n",
    "print(\"diagonal matrix \\n\",\n",
    "      torch.diag(input = torch.tensor([1, 2, 3, 4])).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` 亦可指定分配產生隨機的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tensor with random elements from uniform(0, 1) \\n\",\n",
    "      torch.rand(size = (2, 6)).numpy())\n",
    "print(\"tensor with uninitialized data \\n\",\n",
    "      torch.empty(size = (2, 6)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之形狀\n",
    "\n",
    "張量之形狀與形狀之維度數量，可透過張量物件的 `.shape` 與 `ndim` 方法來獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"size of tensor is\", a.size())\n",
    "print(\"dim of tensor is\", a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要對張量的形狀進行改變的話，可透過 `tf.reshape()` 此函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tensor with shape (4, 3): \\n\",\n",
    "      a.view(size = (4, 3)).numpy())\n",
    "print(\"tensor with shape (2, 2, 3): \\n\",\n",
    "      a.view(size = (2, 2, 3)).numpy())\n",
    "print(\"tensor with shape (12, 1): \\n\",\n",
    "      a.view(size = (12, 1)).numpy())\n",
    "print(\"tensor with shape (12, 1) by (-1, 1): \\n\",\n",
    "      a.view(size = (-1, 1)).numpy())\n",
    "print(\"tensor with shape (12,): \\n\",\n",
    "      a.view(size = (12, )).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意，`(12, 1)` 與 `(12,)` 兩種形狀是不一樣的，前者為2d的張量，後者為1d的張量。在進行張量操弄時，若將兩者混淆，很可能會帶來錯誤的計算結果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之資料類型\n",
    "張量的資料類型，可透過 `.dtype` 方法獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"data type of tensor is\", a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若是要調整資料類型的話，則可透過 `tf.cast()`此函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(a.type(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`tensorflow` 內建多種資料類型，包含整數類型（如 `tf.int32` 與 `tf.int64`）與浮點數類型（如 `tf.float32` 與 `tf.float64`），完整的資料類型請見 [tf.dtypes.DType](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)。\n",
    "\n",
    "在進行張量的數學運算時，請務必確認張量間的資料類型都是一致的，而 `tensorflow` 常用之資料類型為 `tf.float32` 與 `tf.float64`，前者所需的記憶體較小，但運算結果的數值誤差較大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 張量之操弄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量之切片\n",
    "\n",
    "若要擷取一張量特定的行（row）或列（column）的話，則可透過切片（slicing）的功能獲得。`tensorflow` 張量的切片方式，與 `numpy` 類似，皆使用中括號 `[]`，再搭配所欲擷取資料行列的索引（index）獲得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"extract 1st row: \\n\",\n",
    "      a[0, :].numpy())\n",
    "print(\"extract 1st and 2nd rows: \\n\",\n",
    "      a[:2, :].numpy())\n",
    "print(\"extract 2nd column: \\n\",\n",
    "      a[:, 1].numpy())\n",
    "print(\"extract 2nd and 3rd columns: \\n\",\n",
    "      a[:, 1:3].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "進行切片時，有幾項重點需要注意。\n",
    "\n",
    "+ 各面向之索引從0開始。\n",
    "+ 負號表示從結尾數回來，如 `-1` 表示最後一個位置。\n",
    "+ `:`表示該面向所有元素皆挑選。\n",
    "+ `start:stop` 表示從 `start` 開始挑選到 `stop-1`。\n",
    "+ `start:stop:step` 表示從 `start` 開始到 `stop-1`，間隔 `step` 挑選。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量之串接\n",
    "多個張量在維度可對應之前提下，可透過 `tf.concat` 串接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"vertical concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 0).numpy())\n",
    "print(\"horizontal concatenation \\n\",\n",
    "      torch.cat([a, a], dim = 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量之運算\n",
    "考慮以下 `a` 與 `b` 兩張量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(data = [[1, 2], [3, 4], [5, 6]],\n",
    "                dtype = torch.float64)\n",
    "b = torch.tensor(data = [[1, 2], [1, 2], [1, 2]],\n",
    "                dtype = torch.float64)\n",
    "print(\"tensor a is \\n\", a.numpy())\n",
    "print(\"tensor b is \\n\", b.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們將使用 `a` 與 `b` 來展示如何使用 `tensorflow` 進行張量間的計算。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 張量元素對元素之運算\n",
    "透過 `tensorflow` 的數學函數，可進行張量元素對元素的四則運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"element-wise add \\n\",\n",
    "      torch.add(a, b))\n",
    "print(\"element-wise subtract \\n\",\n",
    "      torch.sub(a, b))\n",
    "print(\"element-wise multiply \\n\",\n",
    "      torch.mul(a, b))\n",
    "print(\"element-wise divide \\n\",\n",
    "      torch.div(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "前述採用的函數，皆可取代為其所對應之運算子計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"element-wise add \\n\", a + b)\n",
    "print(\"element-wise subtract \\n\", a - b)\n",
    "print(\"element-wise multiply \\n\", a * b)\n",
    "print(\"element-wise divide \\n\", a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若需要進行絕對值、對數、指數等較為進階之數學運算，可以至 [tf.math](https://www.tensorflow.org/api_docs/python/tf/math) 此模組中尋找對應的數學函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 張量線性代數之運算\n",
    "除了簡單的四則運算外，當張量的 `ndim` 為2時，`tensorflow` 提供了進行線性代數（linear algebra）相關的函數，如\n",
    "\n",
    "+ 矩陣轉置（matrix transpose）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a_t = torch.transpose(input=a, dim0=0, dim1=1)\n",
    "print(\"transpose of a is \\n\",\n",
    "      a_t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ 矩陣乘法（matrix multiplication）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c = a_t @ a\n",
    "print(\"c = a_t @ a is \\n\",\n",
    "      c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 反矩陣（matrix inverse）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_inv = torch.inverse(input = c)\n",
    "print(\"inverse of c is \\n\",\n",
    "      c_inv.numpy()) # c @ c_inv should be identity matrix\n",
    "print(\"check for inverse (left) \\n\",\n",
    "      (c_inv @ c).numpy())\n",
    "print(\"check for inverse (right) \\n\",\n",
    "      (c @ c_inv).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "+ Cholesky 拆解（Cholesky decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_chol = torch.cholesky(input = c)\n",
    "print(\"Cholesky factor of c is \\n\",\n",
    "      c_chol.numpy())\n",
    "print(\"check for Cholesky decomposition \\n\",\n",
    "      (c_chol @ torch.transpose(c_chol, 0 , 1)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 特徵拆解（eigen-decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "e, v = torch.symeig(input = c, eigenvectors=True)\n",
    "print(\"eigenvalue of c is \\n\",\n",
    "      e.numpy())\n",
    "print(\"eigenvector of c is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for eigen-decomposition \\n\",\n",
    "      (v @ torch.diag(e) @\n",
    "       torch.transpose(v, 0 , 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 奇異值拆解（singular value decomposition）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "u, s, v = torch.svd(input = a)\n",
    "print(\"singular value of a is \\n\",\n",
    "      s.numpy())\n",
    "print(\"left singular vector of a is \\n\",\n",
    "      u.numpy())\n",
    "print(\"right singular vector of a is \\n\",\n",
    "      v.numpy())\n",
    "print(\"check for singular value decomposition \\n\",\n",
    "      (u @ torch.diag(s) @\n",
    "       torch.transpose(v, 0, 1)).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 對張量之數值進行摘要\n",
    "`tf.math` 提供了一些化約（reduce）的函數，對張量內的數值進行摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"calculate mean \\n\",\n",
    "      torch.mean(input = a).numpy())\n",
    "print(\"calculate standard deviation \\n\",\n",
    "      torch.std(input = a).numpy())\n",
    "print(\"calculate max \\n\",\n",
    "      torch.max(input = a).numpy())\n",
    "print(\"calculate min \\n\",\n",
    "      torch.min(input = a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我們亦可對張量的各面向，進行前述的摘要。以平均數為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"calculate mean for each column \\n\",\n",
    "      torch.mean(input = a, dim=0).numpy())\n",
    "print(\"calculate mean for each row \\n\",\n",
    "      torch.mean(input = a, dim=1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 實徵範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生線性迴歸資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 計算模型參數"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}